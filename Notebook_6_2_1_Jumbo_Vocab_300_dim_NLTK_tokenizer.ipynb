{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: CNN\n",
    "\n",
    "## Overview: \n",
    "\n",
    "1. Begin by importing and getting the embeddings and word to index mappings we created in [Notebook 1: Embed Words](Notebook_1_Embed_Words.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "embedding_num_steps = 1000001\n",
    "\n",
    "unknown_word_token = \"<UNK/>\"\n",
    "embedding_batch_size = 20\n",
    "embedding_size = 300 # Dimension of the embedding vector.\n",
    "skip_window = 10       # How many words to consider left and right.\n",
    "num_skips = 20         # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(np.arange(valid_window), valid_size))\n",
    "num_sampled = 64    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "sentence_padding_token = \"<PAD/>\"\n",
    "sentence_padding = 80\n",
    "\n",
    "filter_sizes =  \"3,4,5\" #\"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "num_filters = 50  #\"Number of filters per filter size (default: 128)\")\n",
    "dropout_keep_prob = 0.5 #\"Dropout keep probability (default: 0.5)\")\n",
    "l2_reg_lambda = 3.0 #\"L2 regularizaion lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "text_cnn_batch_size = 64 # \"Batch Size (default: 64)\")\n",
    "num_epochs = 100 #\"Number of training epochs (default: 200)\")\n",
    "evaluate_every = 1000  #\"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "checkpoint_every = 100000 # \"Save model after this many steps (default: 100)\")\n",
    "\n",
    "# Evaluation Parameters\n",
    "num_folds = 10 # number of cross validation folds \n",
    "\n",
    "# Misc Parameters\n",
    "allow_soft_placement = True # \"Allow device soft device placement\")\n",
    "log_device_placement = False  #\"Log placement of ops on devices\")\n",
    "display_train_steps = False # toggles output of training step results\n",
    "\n",
    "run_name = \"jumbo-vocab-300-dim-plus-nltk-tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn.rnn_cell import BasicLSTMCell, LSTMCell \n",
    "import itertools\n",
    "from collections import Counter\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import string\n",
    "import pyprind\n",
    "import collections\n",
    "import math\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "urlFinder = re.compile('\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*')\n",
    "atNameFinder = re.compile(r'@([A-Za-z0-9_]+)')\n",
    "\n",
    "exclude_punc = set([\n",
    "        \"!\",\n",
    "        \"?\",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \":\",\n",
    "        \";\",\n",
    "        \"'\",\n",
    "        \"\\\"\",\n",
    "        \"'\",\n",
    "        \"-\",\n",
    "        \"(\",\n",
    "        \")\"\n",
    "])\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def clean(string):\n",
    "    global atNameFinder\n",
    "    global urlFinder\n",
    "    global tknzr\n",
    "\n",
    "    words = []\n",
    "    \n",
    "    string = string \\\n",
    "        .replace(\"&amp;\", \"\") \\\n",
    "        .replace(\"&gt;\",\"\") \\\n",
    "        .replace(\"&lt;\", \"\") \\\n",
    "        .lower()\n",
    "        \n",
    "    tokens = tknzr.tokenize(string)\n",
    "\n",
    "    for word in tokens:\n",
    "        if urlFinder.match(word):\n",
    "            words.append(\"<URL/>\")\n",
    "        elif atNameFinder.search(word):\n",
    "            words.append(\"<AT_NAME/>\")\n",
    "        else:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "def pad(sentence):\n",
    "    global sentence_padding\n",
    "    global sentence_padding_token\n",
    "    if(sentence_padding < len(sentence)):\n",
    "        raise Exception(\"Increase sentence_padding, \\\n",
    "            found sentence that is %s words long. sentence_padding must be \\\n",
    "            greater than or equal to the number of words in the longest sentence\" % len(sentence))\n",
    "    else:\n",
    "        for x in range(sentence_padding - len(sentence)):\n",
    "            sentence.append(sentence_padding_token)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##########################    ] | ETA: 00:01:13"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 156420\n"
     ]
    }
   ],
   "source": [
    "wordSet = set()\n",
    "vocabGrowth = 0\n",
    "vocabulary = {}\n",
    "vocabulary_inv = []\n",
    "\n",
    "# Build Vocab\n",
    "with open('vocab.csv', 'rb') as f:\n",
    "    \n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    numline = len([row for row in reader])\n",
    "    bar = pyprind.ProgBar(numline, monitor=True)\n",
    "    f.seek(0)\n",
    "    \n",
    "    for row in reader:\n",
    "        if len(row) > 0:\n",
    "            words = clean(row[0])\n",
    "            for word in words:\n",
    "                word = word.encode('ascii', 'replace')\n",
    "                if(word not in wordSet):\n",
    "                    vocabulary_inv.append(word)\n",
    "                    vocabulary[word] = vocabulary_inv.index(word)\n",
    "                    wordSet.add(word)\n",
    "        bar.update()\n",
    "                \n",
    "vocabulary_inv.append(sentence_padding_token)\n",
    "vocabulary[sentence_padding_token] = vocabulary_inv.index(word)\n",
    "wordSet.add(sentence_padding_token)\n",
    "\n",
    "vocabulary_inv.append(unknown_word_token)\n",
    "vocabulary[unknown_word_token] = vocabulary_inv.index(word)\n",
    "wordSet.add(unknown_word_token)\n",
    "\n",
    "vocabulary_size = len(wordSet)\n",
    "print(\"Vocabulary Size: %s\" % vocabulary_size)\n",
    "\n",
    "embeddings = None\n",
    "data_index = 0\n",
    "data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data [38357, 132008, 139451, 51423, 78351, 25242, 6520, 140438, 109603, 18326]\n",
      "81319 -> 75512\n",
      "brotherhoodworking -> brainwasher\n",
      "81319 -> 35810\n",
      "brotherhoodworking -> brainwashes\n",
      "81319 -> 15896\n",
      "brotherhoodworking -> zohanian\n",
      "81319 -> 1963\n",
      "brotherhoodworking -> brainwashed\n",
      "81319 -> 92528\n",
      "brotherhoodworking -> driventhey\n",
      "81319 -> 75424\n",
      "brotherhoodworking -> #cannonball\n",
      "81319 -> 29157\n",
      "brotherhoodworking -> affiliated\n",
      "81319 -> 13943\n",
      "brotherhoodworking -> affiliates\n",
      "81319 -> 13144\n",
      "brotherhoodworking -> ratings=plummet\n",
      "81319 -> 12393\n",
      "brotherhoodworking -> smartits\n",
      "Initialized"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[                              ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 0: 358.388092041\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 2000: 251.938658096"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#                             ] | ETA: 00:18:40"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 4000: 216.416538292\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 6000: 199.285164185"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##                            ] | ETA: 00:19:51"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 8000: 187.977703247"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[###                           ] | ETA: 00:19:49"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 10000: 178.520104385\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 12000: 170.679919083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[####                          ] | ETA: 00:19:16"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 14000: 164.581143963\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 16000: 153.788136196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#####                         ] | ETA: 00:18:33"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 18000: 141.346190031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[######                        ] | ETA: 00:17:44"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 20000: 130.259507465\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 22000: 117.678217488"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#######                       ] | ETA: 00:16:57"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 24000: 95.0610606565\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 26000: 76.2779784145"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########                      ] | ETA: 00:16:12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 28000: 62.0629873714"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#########                     ] | ETA: 00:15:27"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 30000: 50.9534995104\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 32000: 43.0472277302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##########                    ] | ETA: 00:14:42"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 34000: 37.2648266549\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 36000: 32.2609576305"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[###########                   ] | ETA: 00:13:57"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 38000: 28.3744694604"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[############                  ] | ETA: 00:13:12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 40000: 25.5547262659\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 42000: 22.7769108741"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#############                 ] | ETA: 00:12:26"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 44000: 21.2493893721\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 46000: 19.2828556228"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##############                ] | ETA: 00:11:41"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 48000: 17.4112573459"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[###############               ] | ETA: 00:10:56"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 50000: 16.4498500848\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 52000: 14.755635367"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[################              ] | ETA: 00:10:11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 54000: 14.1238320556\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 56000: 13.3925768398"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#################             ] | ETA: 00:09:26"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 58000: 12.5156530386"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##################            ] | ETA: 00:08:42"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 60000: 11.6254830676\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 62000: 10.6717665066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[###################           ] | ETA: 00:07:57"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 64000: 10.3623154776\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 66000: 9.75572463953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[####################          ] | ETA: 00:07:14"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 68000: 9.08545457864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#####################         ] | ETA: 00:06:30"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 70000: 8.76947354949\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 72000: 8.28430061209"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[######################        ] | ETA: 00:05:46"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 74000: 8.11413312542\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 76000: 7.7321424607"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#######################       ] | ETA: 00:05:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 78000: 7.32035826898"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################      ] | ETA: 00:04:19"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 80000: 7.17102608848\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 82000: 6.67401673341"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#########################     ] | ETA: 00:03:35"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 84000: 6.48646385527\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 86000: 6.32188090289"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##########################    ] | ETA: 00:02:52"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 88000: 6.23707947791"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[###########################   ] | ETA: 00:02:09"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 90000: 6.03272382748\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 92000: 5.83866734684"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[############################  ] | ETA: 00:01:26"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 94000: 5.78273441476\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes\n",
      "Average loss at step 96000: 5.49747481704"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[############################# ] | ETA: 00:00:43"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 98000: 5.50759158003"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##############################] | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 100000: 5.35481545472\n",
      "Nearest to who: #switzerland warstrust monuments networkmuslim ~u helping recurring thatswhatmakes"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##############################] | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:21:31\n"
     ]
    }
   ],
   "source": [
    "data = [ idx for word, idx in vocabulary.iteritems() ]\n",
    "\n",
    "print('Sample data %s' % data[:10])\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    global data\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "        while target in targets_to_avoid:\n",
    "            target = random.randint(0, span - 1)\n",
    "        targets_to_avoid.append(target)\n",
    "        batch[i * num_skips + j] = buffer[skip_window]\n",
    "        labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=10, num_skips=10, skip_window=5)\n",
    "\n",
    "for i in range(10):\n",
    "    print('%s -> %s' % (batch[i], labels[i, 0]))\n",
    "    print('%s -> %s' % (vocabulary_inv[batch[i]], vocabulary_inv[labels[i, 0]]))\n",
    "\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input da 4ta.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[embedding_batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[embedding_batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        # Construct the variables for the NCE loss\n",
    "        with tf.name_scope(\"nce_weights\") as scope:\n",
    "            nce_weights = tf.Variable(\n",
    "                tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                    stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        nce_biases_hist = tf.histogram_summary(\"nce_biases\", nce_biases)\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
    "                         num_sampled, vocabulary_size))\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope(\"train\") as scope:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.25).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "    valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "\n",
    "    # Step 5: Begin training.\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # We must initialize all variables before we use them.\n",
    "        merged = tf.merge_all_summaries()\n",
    "        writer = tf.train.SummaryWriter(\"/tmp/tensor_logs/expiriment_1\", session.graph_def)\n",
    "\n",
    "        #Adds an op to initialize all variables\n",
    "        init_op = tf.initialize_all_variables()\n",
    "\n",
    "        # Begins running the init opp\n",
    "        init_op.run()\n",
    "\n",
    "        print(\"Initialized\")\n",
    "        average_loss = 0\n",
    "        bar = pyprind.ProgBar(embedding_num_steps, monitor=True)\n",
    "        for step in xrange(embedding_num_steps):\n",
    "            batch_inputs, batch_labels = generate_batch(\n",
    "                embedding_batch_size, num_skips, skip_window)\n",
    "            feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n",
    "            # We perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run()\n",
    "            summary_str, _, loss_val = session.run([merged, optimizer, loss], feed_dict=feed_dict)\n",
    "            writer.add_summary(summary_str, step)\n",
    "            average_loss += loss_val\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print(\"Average loss at step %s: %s\" % (step, average_loss))\n",
    "                average_loss = 0\n",
    "            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 5000 == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in xrange(valid_size):\n",
    "                    valid_word = vocabulary_inv[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log_str = \"Nearest to %s:\" % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = vocabulary_inv[nearest[k]]\n",
    "                    log_str = \"%s %s\" % (log_str, close_word)\n",
    "                print(log_str)\n",
    "            bar.update()\n",
    "\n",
    "        # eval embedding tensor\n",
    "        embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TextCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "_y = []\n",
    "with open('data.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, delimiter=',')   \n",
    "    \n",
    "    for row in reader:\n",
    "        sentences.append(clean(row[1]))\n",
    "        labels.append(([0, 1] if row[0] == \"example\" else [1, 0]))\n",
    "        _y.append(1 if row[0] == \"example\" else 0)\n",
    "\n",
    "\n",
    "sequence_length = max(len(i) for i in sentences)\n",
    "padded_sentences = [ pad(sentence) for sentence in sentences]\n",
    "    \n",
    " \n",
    "word_counts = Counter(itertools.chain(*padded_sentences))\n",
    "\n",
    "# Mapping from index to word\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "# Mapping from word to index\n",
    "vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "\n",
    "x = np.array([[vocabulary[word] for word in sentence] for sentence in padded_sentences])\n",
    "y = np.array(labels)\n",
    "_y = np.array(_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, embedding_tensor, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(embedding_tensor,\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_data = data[shuffle_indices]\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV_Progress\n",
      "0%      100%\n",
      "[          ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/10f-cv-unique-name-replacement/1459880330\n",
      "\n",
      "Steps: 1222500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold_0\n",
      "0%                          100%\n",
      "[                              ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-1887b87666d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m                 \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcheckpoint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-1887b87666d3>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x_batch, y_batch)\u001b[0m\n\u001b[0;32m    137\u001b[0m                 _, step, summaries, loss, accuracy = sess.run(\n\u001b[0;32m    138\u001b[0m                     \u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                     feed_dict)\n\u001b[0m\u001b[0;32m    140\u001b[0m                 \u001b[0mtime_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisplay_train_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;33m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 511\u001b[1;33m                            feed_dict_string)\n\u001b[0m\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 564\u001b[1;33m                            target_list)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    569\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m       \u001b[0me_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_traceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data Preparatopn\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "sys.stdout.flush()\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "_y_shuffled = _y[shuffle_indices]\n",
    "\n",
    "skf = StratifiedKFold(_y_shuffled, n_folds=num_folds)\n",
    "\n",
    "fold_accuracies = []\n",
    "fold_specificities = []\n",
    "fold_sensitivities = []\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "#x_train, x_dev = x_shuffled[:len(x_shuffled)-1], x_shuffled[-len(x_shuffled)-1:]\n",
    "#y_train, y_dev = y_shuffled[:len(y_shuffled)-1], y_shuffled[-len(y_shuffled)-1:]\n",
    "#print(\"Vocabulary Size: {:d}\".format(len(vocabulary)))\n",
    "#print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "#print(\"Train Pos/Dev Pos Split {:d}/{:d}\"\n",
    "#      .format(\n",
    "#        len(\n",
    "#            [y for y in y_train if y[0] == 0 and y[1] == 1]\n",
    "#        ), len(\n",
    "#            [y for y in y_dev if y[0] == 0 and y[1] == 1]\n",
    "#        )))\n",
    "#print(\"Train Neg/Dev Neg Split {:d}/{:d}\"\n",
    "#      .format(\n",
    "#        len(\n",
    "#            [y for y in y_train if y[0] == 1 and y[1] == 0]\n",
    "#        ), len(\n",
    "#            [y for y in y_dev if y[0] == 1 and y[1] == 0]\n",
    "#        )))\n",
    "\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "foldBar = pyprind.ProgBar(num_folds, title='CV_Progress')\n",
    "for idx, fold in zip(skf, range(num_folds)):\n",
    "    x_train = x[idx[0]]\n",
    "    y_train = y[idx[0]]\n",
    "    \n",
    "    x_dev = x[idx[1]]\n",
    "    y_dev = y[idx[1]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=allow_soft_placement,\n",
    "          log_device_placement=log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=2,\n",
    "                vocab_size=len(vocabulary),\n",
    "                embedding_size=embedding_size,\n",
    "                embedding_tensor=embeddings,\n",
    "                filter_sizes=map(int, filter_sizes.split(\",\")),\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", run_name, timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\", str(fold))\n",
    "            train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\", str(fold))\n",
    "            dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph_def)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: dropout_keep_prob\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                if(display_train_steps):\n",
    "                    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                \n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "                return accuracy\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                zip(x_train, y_train), text_cnn_batch_size, num_epochs)\n",
    "            # Training loop. For each batch...\n",
    "            it = ((int(len(data)/text_cnn_batch_size)+1)*num_epochs)\n",
    "            print(\"Steps: {0}\".format(it))\n",
    "            bar = pyprind.ProgBar(it, title='fold_{0}'.format(fold), monitor=True)\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                if current_step % evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                \n",
    "                bar.update()\n",
    "                sys.stderr.flush()\n",
    "            \n",
    "            acc = dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "            fold_accuracies.append(acc)\n",
    "            \n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            tn = 0\n",
    "            fn = 0\n",
    "            \n",
    "            for _y, _x in zip(y_dev, x_dev):\n",
    "                a= dev_step([_x], [_y])\n",
    "                expected = \"example\" if _y[0] == 0 and _y[1] == 1  else \"nonexample\"\n",
    "                actual = None\n",
    "                if(_y[0] == 0 and _y[1] == 1):\n",
    "                    # correct label is example\n",
    "                    if(a == 1.0):\n",
    "                        actual = \"example\"\n",
    "                    else:\n",
    "                        actual = \"nonexample\"\n",
    "                elif(_y[0] == 1 and _y[1] == 0):\n",
    "                    if(a == 1.0):\n",
    "                        actual = \"nonexample\"\n",
    "                    else:\n",
    "                        actual = \"example\"\n",
    "\n",
    "                if(expected == \"example\" and actual == \"example\"):\n",
    "                    tp += 1\n",
    "                elif(expected == \"example\" and actual == \"nonexample\"):\n",
    "                    fn += 1\n",
    "                elif(expected == \"nonexample\" and actual ==\"exaple\"):\n",
    "                    fp += 1\n",
    "                elif(expected == \"nonexample\" and actual == \"nonexample\"):\n",
    "                    tn +=1 \n",
    "\n",
    "            sensitivity = (tp/(tp+float(fn)))\n",
    "            fold_sensitivities.append(sensitivity)\n",
    "\n",
    "            specificity = (tn/(tn+float(fp)))\n",
    "            fold_specificities.append(specificity)\n",
    "            foldBar.update()\n",
    "            \n",
    "            sys.stderr.flush()\n",
    "\n",
    "            \n",
    "  \n",
    "        \n",
    "        print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold final accuracy: 0.854582905769\n",
      "10-fold final specificity: 1.0\n",
      "10-fold final sensitivity: 0.0\n"
     ]
    }
   ],
   "source": [
    "final_accuracy = sum(fold_accuracies) / float(len(fold_accuracies))\n",
    "print(\"10-fold final accuracy: %s\" % final_accuracy)\n",
    "final_specificity = sum(fold_specificities) / float(len(fold_specificities))\n",
    "print(\"10-fold final specificity: %s\" % final_specificity)\n",
    "final_sensitivities = sum(fold_sensitivities) / float(len(fold_sensitivities))\n",
    "print(\"10-fold final sensitivity: %s\" % final_sensitivities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
