{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: CNN\n",
    "\n",
    "## Overview: \n",
    "\n",
    "1. Begin by importing and getting the embeddings and word to index mappings we created in [Notebook 1: Embed Words](Notebook_1_Embed_Words.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "from tensorflow.models.rnn.rnn_cell import BasicLSTMCell, LSTMCell \n",
    "import itertools\n",
    "from collections import Counter\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embeddings = None\n",
    "mappings = None\n",
    "rows = None\n",
    "\n",
    "with open(\"word_embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "with open(\"word_mappings.pkl\", \"rb\") as f:\n",
    "    mappings = pickle.load(f)\n",
    "    \n",
    "\n",
    "urlFinder = re.compile('\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*')\n",
    "atNameFinder = re.compile(r'@([A-Za-z0-9_]+)')\n",
    "atNameCounter = 0\n",
    "\n",
    "exclude_punc = set([\n",
    "        \"!\",\n",
    "        \"?\",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \":\",\n",
    "        \";\",\n",
    "        \"'\",\n",
    "        \"\\\"\",\n",
    "        \"“\",\n",
    "        \"’\",\n",
    "        \"-\"\n",
    "])\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "x = []\n",
    "y = []\n",
    "_y = []\n",
    "\n",
    "with open('data.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, delimiter=',')   \n",
    "    \n",
    "    for row in reader:\n",
    "        words = []\n",
    "        \n",
    "        for word in row[1] \\\n",
    "            .strip() \\\n",
    "            .replace(\"&amp;\", \"\") \\\n",
    "            .replace(\"&gt;\",\"\") \\\n",
    "            .replace(\"&lt;\", \"\") \\\n",
    "            .lower().split():\n",
    "            \n",
    "            if urlFinder.match(word):\n",
    "                words.append(\"<URL/>\")\n",
    "            elif atNameFinder.search(word):\n",
    "                words.append(\"<AT_NAME_%s/>\" % atNameCounter)\n",
    "                atNameCounter +=1\n",
    "            else:\n",
    "                word = ''.join(ch for ch in word if ch not in exclude_punc)\n",
    "                words.append(word)\n",
    "        sentences.append(words)\n",
    "        labels.append(([0, 1] if row[0] == \"example\" else [1, 0]))\n",
    "        _y.append(1 if row[0] == \"example\" else 0)\n",
    "\n",
    "\n",
    "sequence_length = max(len(i) for i in sentences)\n",
    "padded_sentences = []\n",
    "for i in range(len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    num_padding = sequence_length - len(sentence)\n",
    "    new_sentence = sentence + [\"<PAD/>\"] * num_padding\n",
    "    padded_sentences.append(new_sentence)\n",
    "    \n",
    " \n",
    "word_counts = Counter(itertools.chain(*padded_sentences))\n",
    "\n",
    "# Mapping from index to word\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "# Mapping from word to index\n",
    "vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "\n",
    "x = np.array([[vocabulary[word] for word in sentence] for sentence in padded_sentences])\n",
    "y = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_data = data[shuffle_indices]\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Writing to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:16:41.964000: step 100, loss 0.753468, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:17:07.382411: step 200, loss 0.746928, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:17:31.904576: step 300, loss 0.786555, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:17:56.791441: step 400, loss 0.72553, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:18:22.044064: step 500, loss 0.72033, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:18:46.305458: step 600, loss 0.685184, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:19:10.648559: step 700, loss 0.665811, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:19:36.391267: step 800, loss 0.680268, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:20:00.288040: step 900, loss 0.68693, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:20:24.499552: step 1000, loss 0.67181, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-1000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:20:50.103903: step 1100, loss 0.665351, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-1100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:21:14.623449: step 1200, loss 0.675402, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-1200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:21:39.085073: step 1300, loss 0.682398, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-1300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:22:04.637721: step 1400, loss 0.6593, acc 0.888158\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-1400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:22:28.727681: step 1500, loss 0.683659, acc 0.888664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-1500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:22:53.101432: step 1600, loss 0.689509, acc 0.888158\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-1600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:23:18.616612: step 1700, loss 0.681888, acc 0.887146\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-1700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:23:43.480711: step 1800, loss 0.703077, acc 0.887146\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-1800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:24:07.662440: step 1900, loss 0.694563, acc 0.887146\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-1900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:24:33.321735: step 2000, loss 0.681034, acc 0.887146\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-2000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:24:58.208350: step 2100, loss 0.714489, acc 0.887146\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-2100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:25:22.666131: step 2200, loss 0.723142, acc 0.887146\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-2200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:25:47.552526: step 2300, loss 0.679729, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-2300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:26:12.132799: step 2400, loss 0.686039, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-2400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:26:36.209344: step 2500, loss 0.730313, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-2500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:27:00.638919: step 2600, loss 0.697138, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-2600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:27:26.183728: step 2700, loss 0.721765, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-2700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:27:46.604523: step 2800, loss 0.721428, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-2800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:28:04.822086: step 2900, loss 0.738716, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-2900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:28:23.489086: step 3000, loss 0.745837, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-3000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T00:28:41.827599: step 3100, loss 0.710581, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457396177/checkpoints/model-3100\n",
      "\n",
      "\n",
      "Final Evaluations:\n",
      "2016-03-08T00:28:43.100201: step 3100, loss 0.710581, acc 0.88664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 128  #Dimensionality of character embedding (default: 128)\n",
    "filter_sizes =  \"3,4,5\" #\"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "num_filters = 128  #\"Number of filters per filter size (default: 128)\")\n",
    "dropout_keep_prob = 0.5 #\"Dropout keep probability (default: 0.5)\")\n",
    "l2_reg_lambda = 0.0 #\"L2 regularizaion lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64 # \"Batch Size (default: 64)\")\n",
    "num_epochs = 100 #\"Number of training epochs (default: 200)\")\n",
    "evaluate_every = 100  #\"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "checkpoint_every = 100 # \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "allow_soft_placement = True # \"Allow device soft device placement\")\n",
    "log_device_placement = False  #\"Log placement of ops on devices\")\n",
    "display_train_steps = False # toggles output of training step results\n",
    "\n",
    "\n",
    "\n",
    "# Data Preparatopn\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "# Randomly shuffle data\n",
    "sss = StratifiedShuffleSplit(_y, 1, test_size=0.5, random_state=0)\n",
    "for train, test in sss:\n",
    "    x_train = np.random.permutation(x[train])\n",
    "    y_train = np.random.permutation(y[train])\n",
    "\n",
    "    x_dev = np.random.permutation(x[test])\n",
    "    y_dev = np.random.permutation(y[test])\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=len(vocabulary),\n",
    "            embedding_size=embedding_dim,\n",
    "            filter_sizes=map(int, filter_sizes.split(\",\")),\n",
    "            num_filters=num_filters,\n",
    "            l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph_def)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if(display_train_steps):\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return accuracy\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            zip(x_train, y_train), batch_size, num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "        \n",
    "        print(\"\\nFinal Evaluations:\")\n",
    "        dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "  \n",
    "        \n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438, 438)\n",
      "Balanced Evaluation:\n",
      "2016-03-08T00:28:43.316714: step 3100, loss 3.1576, acc 0.497717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4977169"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        pos_x_dev = []\n",
    "        pos_y_dev = []\n",
    "\n",
    "        neg_x_dev = []\n",
    "        neg_y_dev = []\n",
    "\n",
    "        for y, x in zip(y_dev, x_dev):\n",
    "            if(y[0] == 0 and y[1] == 1):\n",
    "                pos_x_dev.append(x)\n",
    "                pos_y_dev.append(y)\n",
    "            else:\n",
    "                neg_x_dev.append(x)\n",
    "                neg_y_dev.append(y)\n",
    "        even_x_dev = np.array(pos_x_dev + neg_x_dev[:len(pos_x_dev)])\n",
    "        even_y_dev = np.array(pos_y_dev + neg_y_dev[:len(pos_y_dev)])\n",
    "        print(len(even_y_dev), len(even_x_dev))\n",
    "\n",
    "        print(\"Balanced Evaluation:\")\n",
    "        dev_step(even_x_dev, even_y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AT_NAME_445/> time to come up with a new script #notready <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.403657: step 3100, loss 4.75294, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1982/> not believing you <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.406836: step 3100, loss 3.39905, acc 0\n",
      "\n",
      "\n",
      "ending systemic racism requires contributions from all of us—especially those of us who haven’t experienced it ourselves —hillary <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.410682: step 3100, loss 7.10241, acc 0\n",
      "\n",
      "\n",
      "we will take on the drug and insurance industry and make it clear that health care must be a right for all i wont leave 29 million behind <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.414037: step 3100, loss 8.20295, acc 0\n",
      "\n",
      "\n",
      "the days for the clintons in public housing are over #gopdebate #christie2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.417757: step 3100, loss 5.6396, acc 0\n",
      "\n",
      "\n",
      "i love new hampshire  will be an exciting evening <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.422240: step 3100, loss 6.4202, acc 0\n",
      "\n",
      "\n",
      "to raise incomes we need to support unions and a living wage i stand with working minnesotans today <URL/> h <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.425510: step 3100, loss 5.71835, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1409/> dont forget tomorrow is #bellletstalk day <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.429191: step 3100, loss 4.8447, acc 0\n",
      "\n",
      "\n",
      "theyre counting on you new hampshire go vote <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.432199: step 3100, loss 8.23909, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1954/> but dt 2+2=1+3215/48 * 46it all makes perfect sensenot 2 mention the liberal revision of history lessons making us bad <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.436559: step 3100, loss 8.29276, acc 0\n",
      "\n",
      "\n",
      "history made #iowacaucus <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.439797: step 3100, loss 6.08577, acc 0\n",
      "\n",
      "\n",
      "icymi were liveblogging the #gopdebate go check it out <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.443108: step 3100, loss 6.56803, acc 0\n",
      "\n",
      "\n",
      "this kind of deception is deplorable and nothing like it would be tolerated by this campaign <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.447948: step 3100, loss 6.83189, acc 0\n",
      "\n",
      "\n",
      "happy birthday <AT_NAME_587/> thank you for your service to our great country you are an american hero <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.451612: step 3100, loss 3.55999, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1952/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.456784: step 3100, loss 3.67712, acc 0\n",
      "\n",
      "\n",
      "i vetoed $2 billion in spending cut taxes by $19 billion created 13 million new jobs balanced 8 budgets that’s a conservative record <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.459770: step 3100, loss 4.92206, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1436/> #tedcruz he is just getting started <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.462849: step 3100, loss 7.14186, acc 0\n",
      "\n",
      "\n",
      "make sure you get on the trump line and are not mislead by the cruz people they are bad be careful <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.465876: step 3100, loss 10.0047, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_239/> encourages iowans to get out and #caucusforcruz today <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.469043: step 3100, loss 5.07245, acc 0\n",
      "\n",
      "\n",
      "#caucusforcruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.472586: step 3100, loss 4.37348, acc 0\n",
      "\n",
      "\n",
      "you arent going to accomplish what we need for working families as long as big money interests control the us congress #demdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.475691: step 3100, loss 7.05156, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2553/> the loser of losers #trump <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.478903: step 3100, loss 2.24949, acc 0\n",
      "\n",
      "\n",
      "we are a nation of immigrants but we are also a sovereign nation of laws our sovereignty demands that we protect our borders <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.481846: step 3100, loss 8.20266, acc 0\n",
      "\n",
      "\n",
      "executives for coal and gas companies have blocked every attempt to act on climate change and have bought the loyalty of elected officials <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.484952: step 3100, loss 6.07174, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1391/> <AT_NAME_1392/> <AT_NAME_1393/> shell be voting for second place this electionhahaha <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.487893: step 3100, loss 3.8966, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_334/> donald trump loves america he loves americans he loves our vets #trumpsupporters <AT_NAME_335/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.491285: step 3100, loss 3.49331, acc 0\n",
      "\n",
      "\n",
      "racial inequality isn’t just a symptom of economic inequality <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.494182: step 3100, loss 4.345, acc 0\n",
      "\n",
      "\n",
      "it is not a radical idea to state that no worker in this country working 40 hours a week should live in poverty #caucusforbernie <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.497199: step 3100, loss 7.68806, acc 0\n",
      "\n",
      "\n",
      "it’s time for us to recognize that america needs someone who can lead — someone with a proven record <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.500337: step 3100, loss 7.00923, acc 0\n",
      "\n",
      "\n",
      "south carolina has a critical choice to make not just the presidency but also the supreme court hang in the balance #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.503620: step 3100, loss 8.68256, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1002/> <AT_NAME_1003/> <AT_NAME_1004/> <AT_NAME_1005/> he won almost all polls and mark halperin gave him an a <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.508464: step 3100, loss 5.77604, acc 0\n",
      "\n",
      "\n",
      "“i have seen many incredible thingsi look forward to seeing a woman president <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.511615: step 3100, loss 3.82202, acc 0\n",
      "\n",
      "\n",
      "feeling burned by politicians who say one thing on the trail but do another once elected <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.515107: step 3100, loss 7.92873, acc 0\n",
      "\n",
      "\n",
      "if i am elected president we will not be drafting our daughters into combat #goptownhall <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.518716: step 3100, loss 4.50258, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_993/> is a man who for forty years has given money to liberal democrats who support liberal judicial activists #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.522143: step 3100, loss 4.48472, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1263/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.525572: step 3100, loss 2.57695, acc 0\n",
      "\n",
      "\n",
      "we also need to break barriers of racism sexism discrimination against lgbt americans immigrants people with disabilities —hillary <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.529087: step 3100, loss 5.27964, acc 0\n",
      "\n",
      "\n",
      "nyt agrees <AT_NAME_985/> should release her wall street transcripts day 20 since she said i will look into it <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.532289: step 3100, loss 7.37215, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1563/> give me strength today for i am taking finals <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.535522: step 3100, loss 6.51885, acc 0\n",
      "\n",
      "\n",
      "in a few hours nevadans will head out to caucus rt if youre standing with hillary <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.538999: step 3100, loss 7.68378, acc 0\n",
      "\n",
      "\n",
      "join me at clemson university on wednesday february 10th #makeamericagreatagain <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.542525: step 3100, loss 6.08739, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1947/> with cruz you lose #trump2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.547316: step 3100, loss 7.45316, acc 0\n",
      "\n",
      "\n",
      "just as the united states opposed the spread of communism it must now oppose the spread of terrorism in all corners of the world <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.550961: step 3100, loss 6.1818, acc 0\n",
      "\n",
      "\n",
      "instead of focusing on any one method of securing the border i will tackle all threats that arise read my plan <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.553883: step 3100, loss 6.73302, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2219/> every morning i wonder why hillary and huma and the rest of hills underlings are not in jail <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.556793: step 3100, loss 5.2932, acc 0\n",
      "\n",
      "\n",
      "#christie2016 #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.561176: step 3100, loss 3.74139, acc 0\n",
      "\n",
      "\n",
      "ill be on <AT_NAME_203/> with <AT_NAME_204/> tonight at 9pm tune in <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.564926: step 3100, loss 6.22006, acc 0\n",
      "\n",
      "\n",
      "we do not represent the interests of the billionaire class wall street or corporate america we dont want their money #debatewithbernie <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.567952: step 3100, loss 8.88589, acc 0\n",
      "\n",
      "\n",
      "wow was ted cruz disloyal to his very capable director of communication he used him as a scape goatfired like a dog ted panicked <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.571146: step 3100, loss 4.94448, acc 0\n",
      "\n",
      "\n",
      "looking forward to our town hall in spartanburg sc w/ <AT_NAME_481/> <AT_NAME_482/> at 12pm rsvp here <URL/> #scformarco <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.574234: step 3100, loss 2.57231, acc 0\n",
      "\n",
      "\n",
      "today i want you to remember that mother and father <URL/> #iacaucus <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.576901: step 3100, loss 3.95861, acc 0\n",
      "\n",
      "\n",
      "we are going to have a big event at the verizon wireless arena in manchester new hampshire 5k+ join us tomorrow <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.580460: step 3100, loss 8.76653, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2492/> where can i get a trump sign for my yard #makeamericagreatagain #votetrump <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.584088: step 3100, loss 8.54021, acc 0\n",
      "\n",
      "\n",
      "we need to end prisons for profit which result in an overincentive to arrest jail and detain in order to keep prison beds full <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.587080: step 3100, loss 5.93843, acc 0\n",
      "\n",
      "\n",
      "mandate <AT_NAME_133/> supports cap and trade <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.590092: step 3100, loss 5.29509, acc 0\n",
      "\n",
      "\n",
      "i will end illegal immigration and protect our borders we need to make america safe great again #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.593875: step 3100, loss 7.59381, acc 0\n",
      "\n",
      "\n",
      "one of the most important questions for 2016 is who is prepared and committed to fight for principled #scotus justices #goptownhall <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.597166: step 3100, loss 7.2609, acc 0\n",
      "\n",
      "\n",
      "thank you <AT_NAME_178/> really appreciate it <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.599895: step 3100, loss 3.05832, acc 0\n",
      "\n",
      "\n",
      "i look forward to signing the #auditthefed bill into law as president #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.603003: step 3100, loss 6.48641, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1991/> today in 1998 president clinton denies an affair with monica lewinsky <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.606113: step 3100, loss 8.36268, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1860/> <AT_NAME_1861/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.609211: step 3100, loss 4.96957, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2408/> the court has ruled that plan parenthood never sold body parts broke no laws you defy the courts you lie <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.614842: step 3100, loss 8.62381, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2291/> you will win trust and believe that edom kathy is over and the real daughter of zion is rightfully air and it aint jinx jyn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.618434: step 3100, loss 5.61796, acc 0\n",
      "\n",
      "\n",
      "jebs response to <AT_NAME_138/> summed up in one picture #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.621876: step 3100, loss 5.14656, acc 0\n",
      "\n",
      "\n",
      "as president ill fight for a #fullrepeal of obamacare #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.625210: step 3100, loss 3.17907, acc 0\n",
      "\n",
      "\n",
      "proper approach is a potus that defends this country goes after terrorists rather than showing weakness #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.628615: step 3100, loss 7.47367, acc 0\n",
      "\n",
      "\n",
      "when we stand together we win thank you new hampshire <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.631996: step 3100, loss 8.58738, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1786/> <AT_NAME_1787/> <AT_NAME_1788/> so the fate of constitutional govt rests solely in the hands of one individual who decided that <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.635223: step 3100, loss 6.8046, acc 0\n",
      "\n",
      "\n",
      "ted cruz is a cheater he holds the bible high and then lies and misrepresents the facts <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.638251: step 3100, loss 6.57985, acc 0\n",
      "\n",
      "\n",
      "super pacs funded by billionaires buy elections ordinary people don’t vote we have an economic and political crisis in this country <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.641548: step 3100, loss 7.71175, acc 0\n",
      "\n",
      "\n",
      "my thoughts and prayers are with justice scalias family and his colleagues on the court who mourn his passing <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.644694: step 3100, loss 5.06646, acc 0\n",
      "\n",
      "\n",
      "from fixing the criminal justice system to creating jobs clintons ambitions match our own <AT_NAME_507/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.647845: step 3100, loss 8.54474, acc 0\n",
      "\n",
      "\n",
      "really dumb <AT_NAME_341/> begged my people for a job turned her down twice and she went hostile major loser zero credibility <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.650782: step 3100, loss 6.24696, acc 0\n",
      "\n",
      "\n",
      "remember it was the republican party with the help of conservatives that made so many promises to their base but didnt keep them hi dt <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.654012: step 3100, loss 7.21686, acc 0\n",
      "\n",
      "\n",
      "i’ll be joining the <AT_NAME_546/> #faithfamilyforum in just a few minutes watch live here <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.657129: step 3100, loss 8.06822, acc 0\n",
      "\n",
      "\n",
      "join congressman raúl labrador in supporting our campaign <URL/> <URL/> #nvgopcaucus <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.659967: step 3100, loss 4.79561, acc 0\n",
      "\n",
      "\n",
      "thank you to my nh family <URL/> #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.663546: step 3100, loss 5.53715, acc 0\n",
      "\n",
      "\n",
      "your next president chris christie  <AT_NAME_359/> watch <URL/> #fitn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.667886: step 3100, loss 5.74743, acc 0\n",
      "\n",
      "\n",
      "if our next president is even half the president ronald reagan was america will be greater than its ever been <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.672423: step 3100, loss 9.2608, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2465/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.675834: step 3100, loss 4.03107, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1044/> is the religious guy when hes the first to have strip clubs in his casinos that is found in 2 corinthians <AT_NAME_1045/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.679078: step 3100, loss 9.76985, acc 0\n",
      "\n",
      "\n",
      "heading to south carolina really big crowd will be back in new hampshire tomorrow #makeamericagreatagain <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.682990: step 3100, loss 8.528, acc 0\n",
      "\n",
      "\n",
      "every american needs to say 2 simple words to every vet they meet thank you john wayne walding <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.685977: step 3100, loss 8.10572, acc 0\n",
      "\n",
      "\n",
      "your vote is a choice for the best person to be commander in chief <URL/> #fitn #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.690388: step 3100, loss 8.70709, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2502/> #dumptrump #againsttrump no way in hell well you get our votes ass clown <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.693322: step 3100, loss 5.58613, acc 0\n",
      "\n",
      "\n",
      "grateful mom could join me on this journey <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.696436: step 3100, loss 5.1645, acc 0\n",
      "\n",
      "\n",
      "i ask you to join me in this campaign to build a future that works for all of us and not just the few on top <URL/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.699623: step 3100, loss 7.68541, acc 0\n",
      "\n",
      "\n",
      "thats not profiling thats law enforcement #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.702914: step 3100, loss 5.50798, acc 0\n",
      "\n",
      "\n",
      "just landed in baton rouge louisiana reports are out that lines are three quarters of a mile to get in wow #makeamericagreatagain <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.706012: step 3100, loss 4.13632, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1652/> common core is just about the money and not education you are right <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.709159: step 3100, loss 4.72159, acc 0\n",
      "\n",
      "\n",
      "black families are denied mortgages nearly 3x as often as white families we need to tear down barriers of systemic racism that still exist <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.712489: step 3100, loss 5.30321, acc 0\n",
      "\n",
      "\n",
      "i believe that we need a political revolution the same old same old just isnt going to do it we have to be bold #bernieinmn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.716135: step 3100, loss 7.93177, acc 0\n",
      "\n",
      "\n",
      "tonight join me for a rally in north las vegas ill be joined by a group of very special guests rsvp <URL/> #teammarco <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.721350: step 3100, loss 8.23964, acc 0\n",
      "\n",
      "\n",
      "america can not stand idly by while the islamic state continues to persecute societies and perpetrate terrorist acts against the free world <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.724753: step 3100, loss 9.28128, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1850/> ccore is a disaster and its criminal considering its effect on our children and their future <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.728086: step 3100, loss 3.63104, acc 0\n",
      "\n",
      "\n",
      "we don’t use children to “send a message <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.731291: step 3100, loss 3.56977, acc 0\n",
      "\n",
      "\n",
      "civil rights activist fannie lou hamer dedicated her life to securing voting rights ending racial injustice #bhm <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.734595: step 3100, loss 4.11402, acc 0\n",
      "\n",
      "\n",
      "banking should help our communities not be an island unto itself benefiting only the wealthy <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.738126: step 3100, loss 7.6993, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2530/> talks tough when its immigrants and isis but cowardly when it comes to facing <AT_NAME_2531/> as a moderator #coward <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.741524: step 3100, loss 6.70645, acc 0\n",
      "\n",
      "\n",
      "live free or die a motto for the whole country to follow #newhampshire #fitn #votetrumpnh <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.744829: step 3100, loss 7.72973, acc 0\n",
      "\n",
      "\n",
      "behind the vote totals and the delegate counts are the voters themselves their stories <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.747868: step 3100, loss 6.46337, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1530/> <AT_NAME_1531/> <AT_NAME_1532/> <AT_NAME_1533/> <AT_NAME_1534/>  yup <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.752213: step 3100, loss 2.43737, acc 0\n",
      "\n",
      "\n",
      "way to go #cruzcrew <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.755413: step 3100, loss 4.32937, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_948/> <AT_NAME_949/> this texan will be voting trump march 1st cruz is a fake texan <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.759051: step 3100, loss 6.92493, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2471/> <AT_NAME_2472/> <AT_NAME_2473/> trump loves 2nd amendment <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.762202: step 3100, loss 2.82785, acc 0\n",
      "\n",
      "\n",
      "obamacare saved this woman when she needed emergency surgery lets build on its progress not start over <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.765989: step 3100, loss 9.4115, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2164/> <AT_NAME_2165/> <AT_NAME_2166/> you coward i hope her husband punches you in the nose for calling her a bimbo on twitter <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.769112: step 3100, loss 6.85493, acc 0\n",
      "\n",
      "\n",
      "failed presidential candidate <AT_NAME_997/> was made to look like a fool by senator harry reid didnt release his tax returns until 9/21/12 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.773395: step 3100, loss 6.98753, acc 0\n",
      "\n",
      "\n",
      "anyone asking for your vote has to grapple with the reality of inequality in america—and offer real plans to fix it <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.777218: step 3100, loss 7.64336, acc 0\n",
      "\n",
      "\n",
      "nasty ted cruz is at it again same dirty tricks he used w/ <AT_NAME_592/> saying i may not be on ballot i hold liberal positions lies <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.780623: step 3100, loss 6.30858, acc 0\n",
      "\n",
      "\n",
      "winning in defense of the cross ten commandments #2a pledge of allegiance #choosecruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.784056: step 3100, loss 8.97239, acc 0\n",
      "\n",
      "\n",
      "want to show your support for marco right now chip in $5 here <URL/> #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.787635: step 3100, loss 4.891, acc 0\n",
      "\n",
      "\n",
      "when i say i will secure the border and stop illegal immigration you can trust me #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.790765: step 3100, loss 5.63434, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2511/> do you consider yourself religious as well <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.793914: step 3100, loss 3.11883, acc 0\n",
      "\n",
      "\n",
      "tune in live to bernie speaking at the henderson rally before tomorrows caucus in nevada <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.797251: step 3100, loss 5.7217, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1114/> <AT_NAME_1115/> <AT_NAME_1116/> trump all the way <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.800774: step 3100, loss 5.87101, acc 0\n",
      "\n",
      "\n",
      "the time to get out and caucus is approaching #iacaucus #caucusfortrump #icaucsed #ivoted <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.805125: step 3100, loss 5.76633, acc 0\n",
      "\n",
      "\n",
      "support a proven conservative for president #caucusforcruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.808575: step 3100, loss 5.00457, acc 0\n",
      "\n",
      "\n",
      "we must increase the min wage to $15 which will increase the wages of nearly 60 percent of latinos #votetogether <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.811733: step 3100, loss 8.33947, acc 0\n",
      "\n",
      "\n",
      "its not enough for a president to stop bad things from happeningi think its important to try to start good things —hillary in nh <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.814910: step 3100, loss 7.22041, acc 0\n",
      "\n",
      "\n",
      "thank you <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.818326: step 3100, loss 6.10419, acc 0\n",
      "\n",
      "\n",
      "voting rights marriage equality campaign finance—all are at stake in the nomination of our next scotus justice <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.821352: step 3100, loss 4.58475, acc 0\n",
      "\n",
      "\n",
      "candy is making phone calls for tonights #iacaucus from our iowa hq #bc2dc16 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.826385: step 3100, loss 8.41234, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1606/> correction it is unbearable to watch her show #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.829620: step 3100, loss 3.33826, acc 0\n",
      "\n",
      "\n",
      "5 days to #iacaucus here are a few ways your five bucks help hillary win in iowa <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.832323: step 3100, loss 8.04404, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1625/> says common law abroad supreme to constitutional and positive law of america accordingly nativeborn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.835427: step 3100, loss 7.7416, acc 0\n",
      "\n",
      "\n",
      "americans must know that their government is there to serve them — not itself i will restore integrity so government is held accountable <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.838569: step 3100, loss 6.20695, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2354/> thats not peace thats compromise and sacraficing and a becomming to an unacceptable offering <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.841612: step 3100, loss 5.70949, acc 0\n",
      "\n",
      "\n",
      "the polls show that i picked up many jeb bush supporters that is how i got to 46% when others drop out i will pick up more sad but true <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.846618: step 3100, loss 4.58219, acc 0\n",
      "\n",
      "\n",
      "the government is meant to serve the people our founding fathers did not intend for america to be beholden to an overreaching bureaucracy <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.849511: step 3100, loss 7.45, acc 0\n",
      "\n",
      "\n",
      "get ready to #caucusforcruz tomorrow <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.852577: step 3100, loss 3.76166, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_167/> may be satisfied with 07% growth but with the right leadership i know we can achieve 4% economic growth <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.855541: step 3100, loss 6.12636, acc 0\n",
      "\n",
      "\n",
      "on this holocaust remembrance day we vow never to forget the lessons of that tragedy and to stand up to all forms of hate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.860112: step 3100, loss 7.3505, acc 0\n",
      "\n",
      "\n",
      "proud to have my good friends <AT_NAME_316/> and <AT_NAME_317/> on the team <URL/> #christie2016 #fitn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.863241: step 3100, loss 8.34858, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_281/> <AT_NAME_282/> you must win the presidency in 2016  <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.866316: step 3100, loss 7.92732, acc 0\n",
      "\n",
      "\n",
      "show marco youre on his team and donate now <URL/> #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.869310: step 3100, loss 7.12216, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1041/> i dont know why youre not capitalising more on the fact that you could be the first female president <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.872271: step 3100, loss 9.1922, acc 0\n",
      "\n",
      "\n",
      "we need a president with a servant’s heart — someone who can fix the va protect and defend our veterans <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.877514: step 3100, loss 5.91728, acc 0\n",
      "\n",
      "\n",
      "its no accident a machetewielding terrorist attacked a restaurant named nazareth and owned by an israeli immigrant <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.880957: step 3100, loss 3.93987, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_233/> thanks sally appreciate the support <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.885256: step 3100, loss 2.21965, acc 0\n",
      "\n",
      "\n",
      "school choice leads to healthy competition among schools to provide the best educational atmosphere for all students to learn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.888857: step 3100, loss 7.41217, acc 0\n",
      "\n",
      "\n",
      "thanks for your support tonight in the #gopdebate now commit to vote or caucus in your state <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.892300: step 3100, loss 7.28429, acc 0\n",
      "\n",
      "\n",
      "tonight iowans will officially start the process of choosing our next president—and may even make history while theyre at it <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.895614: step 3100, loss 8.04449, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1499/> <AT_NAME_1500/> <AT_NAME_1501/> based on your bias and contradiction u should not b allowed to run for president <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.898796: step 3100, loss 5.66058, acc 0\n",
      "\n",
      "\n",
      "new south carolina poll from ppp thank you #votetrumpsc <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.902390: step 3100, loss 5.97103, acc 0\n",
      "\n",
      "\n",
      "we live in a world with serious foreign policy challenges we need serious leadership #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.905580: step 3100, loss 4.97796, acc 0\n",
      "\n",
      "\n",
      "thanks <AT_NAME_179/> helps when you know what you stand for cc <AT_NAME_180/> <AT_NAME_181/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.908779: step 3100, loss 6.83339, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1661/> <AT_NAME_1662/> <AT_NAME_1663/> <AT_NAME_1664/> <AT_NAME_1665/> 👍🇺🇸 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.912252: step 3100, loss 3.54083, acc 0\n",
      "\n",
      "\n",
      "the nation doesn’t need more laws restricting the rights of gun owners we must protect defend the 2nd amendment <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.915727: step 3100, loss 6.4858, acc 0\n",
      "\n",
      "\n",
      "proud of the fl house of representatives for giving more freedom to innovative companies like <AT_NAME_116/> to create jobs in the sunshine state <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.919195: step 3100, loss 6.10312, acc 0\n",
      "\n",
      "\n",
      "trump couple turns to carson for ‘intelligence noncombativeness <URL/> via <AT_NAME_231/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.922655: step 3100, loss 3.63283, acc 0\n",
      "\n",
      "\n",
      "i’m a progressive who likes to get things done and i’ve learned that you have to be both a dreamer and a doer —hillary in nh <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.925839: step 3100, loss 6.49684, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2118/> is bought and paid for voting for you is like voting for a republican sorry time warner and wall st cant have my vote <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.931817: step 3100, loss 5.36717, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2451/> u say that  u will never divide us as if that is the only problem facing us u are a player after the fame without the skills <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.935492: step 3100, loss 8.95036, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1879/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.939004: step 3100, loss 4.01115, acc 0\n",
      "\n",
      "\n",
      "we cant just talk about combating inequality we need real plans to lift up communities that are being held back <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.942310: step 3100, loss 8.12138, acc 0\n",
      "\n",
      "\n",
      "thank you for your endorsement <AT_NAME_863/> #bikersfortrump #votetrumpnv video <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.945673: step 3100, loss 7.88814, acc 0\n",
      "\n",
      "\n",
      "listen here #cruzcrew <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.948873: step 3100, loss 2.26184, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1397/> you are held accountable for your lack of actions and deleting emails classified emails your bullying sex scandals pathet <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.953505: step 3100, loss 7.53787, acc 0\n",
      "\n",
      "\n",
      "a $1 trillion investment in infrastructure could create 13 million decentpaying jobs <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.956418: step 3100, loss 6.51144, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_921/> poll and <AT_NAME_922/> say that i beat both hillary and bernie and i havnt even started on them yet <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.960890: step 3100, loss 7.58338, acc 0\n",
      "\n",
      "\n",
      "if i’m president will wall street like me no will they begin to play by the rules if i’m president you better believe it <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.964012: step 3100, loss 9.42099, acc 0\n",
      "\n",
      "\n",
      "i know hillary will fight for the issues most important to me and to so many women and families <AT_NAME_298/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.967085: step 3100, loss 3.90081, acc 0\n",
      "\n",
      "\n",
      "if we stand together if we build a political revolution there is nothing we cant accomplish <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.970236: step 3100, loss 6.82114, acc 0\n",
      "\n",
      "\n",
      "shot some pool with <AT_NAME_9/> in iowa earlier this week not going to say who won but #candidcarson <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.973177: step 3100, loss 8.2754, acc 0\n",
      "\n",
      "\n",
      "candy had a great time talking to the students at riverside middle school in pendleton south carolina <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.976459: step 3100, loss 4.32888, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2120/> <AT_NAME_2121/> fake politicians need to be insulted its why this democrat is voting trump <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.981076: step 3100, loss 7.70774, acc 0\n",
      "\n",
      "\n",
      "russian pm said yesterday he believes theres a new cold war between russia and the west <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.985487: step 3100, loss 9.95153, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_815/> mrtrumpboth cruz and rubio are ineligible to be potus its a slam dunk case check it <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.988442: step 3100, loss 4.75087, acc 0\n",
      "\n",
      "\n",
      "a message to the great people of iowa on this historic day find your caucus site here <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.991650: step 3100, loss 7.27808, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2381/> <AT_NAME_2382/> new hamphire rocks it for marco rubio <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.995135: step 3100, loss 4.71786, acc 0\n",
      "\n",
      "\n",
      "it is so important to audit the federal reserve and yet ted cruz missed the vote on the bill that would allow this to be done <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:43.998837: step 3100, loss 6.94587, acc 0\n",
      "\n",
      "\n",
      "wow every poll said i won the debate last night great honor <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.002258: step 3100, loss 5.2254, acc 0\n",
      "\n",
      "\n",
      "honored to have my brother joining me on the trail this week join us monday in charleston <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.005781: step 3100, loss 7.53133, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_283/> <AT_NAME_284/> if you can do that well in iowa then i see you acing it in all other significant states momentum is growing <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.009050: step 3100, loss 5.68836, acc 0\n",
      "\n",
      "\n",
      "lets break down barriers—not build new walls #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.012192: step 3100, loss 4.55152, acc 0\n",
      "\n",
      "\n",
      "it’s time to close this dangerous loophole in our gun laws <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.015179: step 3100, loss 4.68562, acc 0\n",
      "\n",
      "\n",
      "thank you iowa <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.018269: step 3100, loss 4.46351, acc 0\n",
      "\n",
      "\n",
      "we are the party of diversity not the democratic party #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.022555: step 3100, loss 8.10018, acc 0\n",
      "\n",
      "\n",
      "#trump2016 #iacaucus finder <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.025677: step 3100, loss 8.23781, acc 0\n",
      "\n",
      "\n",
      "iowas own <AT_NAME_200/> gets a standing ovation at our town hall in wilton tonight #caucusforcruz #iacaucus <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.029255: step 3100, loss 5.17429, acc 0\n",
      "\n",
      "\n",
      "we need to be looking for solutions  its not about any candidate its about the american people #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.032617: step 3100, loss 9.96177, acc 0\n",
      "\n",
      "\n",
      "thank you to the 2500+ in north augusta south carolina lines down the block dont forget to vote on saturday <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.038040: step 3100, loss 10.1906, acc 0\n",
      "\n",
      "\n",
      "we need to knock down the barriers of racism sexism and discrimination holding americans back #demdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.041402: step 3100, loss 6.84467, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_618/> <AT_NAME_619/> you crushed the debate thank you <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.044537: step 3100, loss 4.9873, acc 0\n",
      "\n",
      "\n",
      "incredibly grateful to all of our staff and volunteers that have given their time and energy and done such wonderful work in south carolina <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.047545: step 3100, loss 6.0424, acc 0\n",
      "\n",
      "\n",
      "if anyone needed a reminder of how important it is to take back the senate hold onto the white house—look at the supreme court —hillary <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.051305: step 3100, loss 7.43542, acc 0\n",
      "\n",
      "\n",
      "greeting voters some a few years shy of voting today in sc find your polling place here <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.054540: step 3100, loss 5.81194, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2454/> why are jeff sessions people helping trump i thought ted and jeff were tight <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.058302: step 3100, loss 7.07635, acc 0\n",
      "\n",
      "\n",
      "a carson administration will take every reasonable measure to avoid using force  instead focussing on deterrence through strength <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.061615: step 3100, loss 7.40511, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1484/> 8 minutes ago thousands of cuban refugees crossing the border <URL/> … <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.065096: step 3100, loss 9.57325, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1014/> <AT_NAME_1015/> <AT_NAME_1016/> <AT_NAME_1017/> yup 200 polish immigrants were hired by his contractor not trump <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.069814: step 3100, loss 6.75878, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2430/> now do john rocker <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.072773: step 3100, loss 6.86532, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_15/> is a man of incredible principle and faith i am honored to have his blessing and endorsement for 2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.076164: step 3100, loss 7.86347, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2319/> <AT_NAME_2320/> youre supposed to be the bad candidate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.079339: step 3100, loss 3.73683, acc 0\n",
      "\n",
      "\n",
      "read more about my plan to rebuild our military <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.084047: step 3100, loss 5.22246, acc 0\n",
      "\n",
      "\n",
      "this campaign is about a political revolution — millions of people standing up and saying enough is enough #americatogether <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.088721: step 3100, loss 3.51705, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_795/> <AT_NAME_796/> i are coming back to pawleys island tonight rsvp <URL/> #scprimary <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.091855: step 3100, loss 6.2513, acc 0\n",
      "\n",
      "\n",
      "tomorrow #choosecruz  <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.095786: step 3100, loss 2.54013, acc 0\n",
      "\n",
      "\n",
      "if donald trump builds his wall the way he built trump tower he’ll be using illegal immigrant labor to do it #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.098565: step 3100, loss 8.31699, acc 0\n",
      "\n",
      "\n",
      "if we solved our problems with wall street tomorrow wed still have discrimination holding too many americans back <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.102174: step 3100, loss 6.22181, acc 0\n",
      "\n",
      "\n",
      "join us in sparks nevada today #nevadacaucus #votetrumpnv <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.105440: step 3100, loss 5.21895, acc 0\n",
      "\n",
      "\n",
      "thank you lynn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.108473: step 3100, loss 4.37145, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1747/> <AT_NAME_1748/> <AT_NAME_1749/> <AT_NAME_1750/> <AT_NAME_1751/> we are all proud of you donaldoh yes we are <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.111611: step 3100, loss 4.40063, acc 0\n",
      "\n",
      "\n",
      "woah big turnout for our #gotvforbernie rally at <AT_NAME_452/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.115205: step 3100, loss 6.16104, acc 0\n",
      "\n",
      "\n",
      "our economy works for wall street because its rigged by wall street and thats the problem #demdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.118626: step 3100, loss 5.94883, acc 0\n",
      "\n",
      "\n",
      "when i was leading the fight against amnesty <AT_NAME_1020/> was firing <AT_NAME_1021/> on tv #rhetoricvsrecord <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.121685: step 3100, loss 9.05245, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1555/> u almost won me back but its disappointing u cant admit error in judgment/that u did anything wrong re emails #demtownhall <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.125372: step 3100, loss 7.8618, acc 0\n",
      "\n",
      "\n",
      "im ben and im jerry im a person and im a person ben jerrys not a person go out and vote today <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.128408: step 3100, loss 6.06425, acc 0\n",
      "\n",
      "\n",
      "i will be interviewed by anderson cooper at 8pm on <AT_NAME_300/> from new hampshire should be very interesting <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.131780: step 3100, loss 5.41851, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2414/> what are you doing to better the mental health and addiction communities read our 6point strategy <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.135268: step 3100, loss 7.74347, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_823/> <AT_NAME_824/> <AT_NAME_825/> <AT_NAME_826/> this veteran voted for trump in tx early voting <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.139915: step 3100, loss 6.79702, acc 0\n",
      "\n",
      "\n",
      "when mitt romney asked me for my endorsement last time around he was so awkward and goofy that we all should have known he could not win <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.143503: step 3100, loss 8.42098, acc 0\n",
      "\n",
      "\n",
      "the bottom line i am the conservative who democrats fear most <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.147880: step 3100, loss 7.77402, acc 0\n",
      "\n",
      "\n",
      "i was referring to a backstop for preexisting conditions i will eliminate the law in its entirety replace it w/ something much better <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.151253: step 3100, loss 7.35765, acc 0\n",
      "\n",
      "\n",
      "you can have a different point of view on immigration or anything else but we cannot be attacking people because of their religion <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.154660: step 3100, loss 7.26237, acc 0\n",
      "\n",
      "\n",
      "hillary is handing over her snapchat account to <AT_NAME_171/> today don’t miss it <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.157989: step 3100, loss 8.02245, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_870/> if ever cancer picked the wrong person to mess with its you stay strong and take care of yourself im in your corner h <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.161838: step 3100, loss 8.19784, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2567/>  thank you for being real <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T00:28:44.164905: step 3100, loss 5.53419, acc 0\n",
      "\n",
      "\n",
      "big thanks to all those who joined me in nashua yesterday rsvp to our portsmouth town hall <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.168285: step 3100, loss 0.00460825, acc 1\n",
      "\n",
      "\n",
      "i have a radical idea instead of having unfettered free trade we should have fair trade protecting workers in the us and abroad <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.171347: step 3100, loss 4.30336e-05, acc 1\n",
      "\n",
      "\n",
      "ted cruz only talks tough on immigration now because he did so badly in sc he is in favor of amnesty and weak on illegal immigration <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.174617: step 3100, loss 0.00135068, acc 1\n",
      "\n",
      "\n",
      "#gopdebate makes it clear theres only one thing standing between republicans repeal of the affordable care act a democratic president <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.178086: step 3100, loss 0.000328725, acc 1\n",
      "\n",
      "\n",
      "nevada doors don’t close until noon find your caucus location here <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.180780: step 3100, loss 0.0019304, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_471/> <AT_NAME_472/> is willing to fight on behalf of the american people #choosecruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.184014: step 3100, loss 0.000567275, acc 1\n",
      "\n",
      "\n",
      "just one day until south carolina votes join the team before the #scprimary <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.187245: step 3100, loss 0.000173077, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1301/> <AT_NAME_1302/> <AT_NAME_1303/> <AT_NAME_1304/> youre such a complainer trump quit your whining and bashing do some real work <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.192631: step 3100, loss 5.35235e-05, acc 1\n",
      "\n",
      "\n",
      "just got back from tampa it was an amazing evening with an even more amazing crowd  fantastic people will be in south carolina tomorrow <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.195672: step 3100, loss 0.000209667, acc 1\n",
      "\n",
      "\n",
      "we need a leader on womens health—not just somebody who votes right but somebody who leads the effort #demdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.199257: step 3100, loss 5.09011e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_74/> i respected cruz ppl until this lie were just tired of it from your side 1949z <AT_NAME_75/> <URL/> no <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.203070: step 3100, loss 0.00137794, acc 1\n",
      "\n",
      "\n",
      "the us needs to lead the international community in fighting climate change to maintain our economic strength and global security <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.207032: step 3100, loss 0.00114423, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1412/> you will make america commit sewer side <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.211354: step 3100, loss 0.0020897, acc 1\n",
      "\n",
      "\n",
      "marco has accomplished a lot in the senate read about it here <URL/> #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.214562: step 3100, loss 0.00332398, acc 1\n",
      "\n",
      "\n",
      "watch this clip from last nights <AT_NAME_762/> #goptownhall on why i believe the 2nd amendment should be fiercely defended <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.218877: step 3100, loss 0.00110803, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1719/> what about the refugees😢 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.222095: step 3100, loss 0.000841859, acc 1\n",
      "\n",
      "\n",
      "new hampshire i am asking for your vote this is my closing argument <URL/> #fitn #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.225199: step 3100, loss 5.57884e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1083/> #thetruthisoutthere #disclosure #therockefellerinitiative <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.229251: step 3100, loss 0.0867216, acc 1\n",
      "\n",
      "\n",
      "new video <AT_NAME_443/> and i served as governors but when you match our records there’s no comparison <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.233829: step 3100, loss 0.000342906, acc 1\n",
      "\n",
      "\n",
      "a moment backstage in manchester nh im taking care of my mom she has alzheimers <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.236899: step 3100, loss 0.00111982, acc 1\n",
      "\n",
      "\n",
      "thank you for coming miri <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.240404: step 3100, loss 0.000926066, acc 1\n",
      "\n",
      "\n",
      "best of luck to the broncos and the panthers in the #superbowl tonight looking forward to a great game <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.247564: step 3100, loss 0.000646859, acc 1\n",
      "\n",
      "\n",
      "time is running out to join us in sc tomorrow were stopping in mt pleasant aiken chapin rsvp <URL/> #scformarco <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.250826: step 3100, loss 0.0316475, acc 1\n",
      "\n",
      "\n",
      "honored to earn the endorsement of congressman <AT_NAME_98/> <URL/> join the #cruzcrew <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.253949: step 3100, loss 0.000804935, acc 1\n",
      "\n",
      "\n",
      "its 2016 a womans place iswherever she wants it to be <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.257366: step 3100, loss 0.0617233, acc 1\n",
      "\n",
      "\n",
      "if we face the reality of systemic racism we can begin to reform our broken criminal justice immigration systems <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.260510: step 3100, loss 0.000145544, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_210/> is the only network that does not even mention my very successful event last night $6000000 raised in one hour for our vets <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.263959: step 3100, loss 0.0055476, acc 1\n",
      "\n",
      "\n",
      "#cruztovictory in new hampshire <URL/> #fitn #nhpolitics <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.267418: step 3100, loss 0.000676046, acc 1\n",
      "\n",
      "\n",
      "bring your family and friends with you when you caucus tonight in iowa click here to find your caucus location <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.270525: step 3100, loss 0.000142087, acc 1\n",
      "\n",
      "\n",
      "the single gravest national security threat facing this country is a nuclear iran #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.273406: step 3100, loss 0.0199687, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1818/> donald i think if we want our military strong we need to hire more idealists and inventors in star wars attack of the <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.276905: step 3100, loss 0.00130259, acc 1\n",
      "\n",
      "\n",
      "our service men and women deserve the best resources available as president ill make sure that they receive the treatment they deserve <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.281459: step 3100, loss 0.000117891, acc 1\n",
      "\n",
      "\n",
      "us debt hits $19 trillion nearly half under <AT_NAME_288/> failed leadership my plan for sustained 4% economic growth → <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.284147: step 3100, loss 0.00170503, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1539/> #scaredoftoughquestions im no <AT_NAME_1540/> fan but it sounds like your scared of her <AT_NAME_1541/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.287915: step 3100, loss 0.000506988, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2383/> has anybody even ever heard of <AT_NAME_2384/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.291326: step 3100, loss 0.00121674, acc 1\n",
      "\n",
      "\n",
      "when im president americas light will shine again <URL/> #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.295786: step 3100, loss 0.00327194, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1407/> megan kelly no trump you are liberal con you should not be pres of a republic #iowacaucus #trump2016 #ccot #tcot <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.298803: step 3100, loss 0.0112759, acc 1\n",
      "\n",
      "\n",
      "“to all my supporters out there—some may have doubted us but we never doubted each other this one’s for you” —hillary in nv <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.302223: step 3100, loss 0.000598847, acc 1\n",
      "\n",
      "\n",
      "as president my top domestic priorities will be to stop illegal immigration we must secure our border to ensure the safety our people <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.305785: step 3100, loss 0.000731439, acc 1\n",
      "\n",
      "\n",
      "im sick and tired of trump attacking my family this isnt about my family or his family its about your family #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.310165: step 3100, loss 3.99343e-05, acc 1\n",
      "\n",
      "\n",
      "i will end common core its a disaster <URL/> #makeamericagreatagain #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.313253: step 3100, loss 0.000552263, acc 1\n",
      "\n",
      "\n",
      "democracy is essentially a proactive citizenry demanding public servants represent just causes <AT_NAME_759/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.316416: step 3100, loss 0.00219462, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1185/> <AT_NAME_1186/> <AT_NAME_1187/> trump 2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.319372: step 3100, loss 0.00226955, acc 1\n",
      "\n",
      "\n",
      "the middle class has been left behind in the last seven years of the obama economy we have to get people back to work #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.322825: step 3100, loss 0.000531294, acc 1\n",
      "\n",
      "\n",
      "there is a reason it has been longstanding policy that we don’t negotiate with terrorists or pay ransoms #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.326291: step 3100, loss 0.00191077, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_856/> no matter what liar sleazeball <AT_NAME_857/> says about <AT_NAME_858/> in tennessee we have early voting and <AT_NAME_859/> has won <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.329908: step 3100, loss 0.000278077, acc 1\n",
      "\n",
      "\n",
      "we had an awesome crowd at our rally in minneapolis yesterday see for yourself <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.333136: step 3100, loss 0.000709163, acc 1\n",
      "\n",
      "\n",
      "inequality stalks our education system <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.338045: step 3100, loss 0.0808556, acc 1\n",
      "\n",
      "\n",
      "before social security was signed into law nearly half of our senior citizens lived in poverty <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.341905: step 3100, loss 0.000120275, acc 1\n",
      "\n",
      "\n",
      "our health care plan is the only plan that would provide care for all americans regardless of income <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.345334: step 3100, loss 0.00100074, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2108/> thank you sir <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.349743: step 3100, loss 0.000807555, acc 1\n",
      "\n",
      "\n",
      "thank you las vegas nevada #nevadacaucus #votetrumpnv <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.353059: step 3100, loss 0.0270708, acc 1\n",
      "\n",
      "\n",
      "read more about donald trump’s record here <URL/> #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.356602: step 3100, loss 0.163731, acc 1\n",
      "\n",
      "\n",
      "tomorrow were starting in rock hill then on to florence lexington for town halls rsvp to attend <URL/> #scformarco <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.359449: step 3100, loss 0.0402863, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1175/> <AT_NAME_1176/> <AT_NAME_1177/> gooooooo trump all the way  <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.362803: step 3100, loss 0.000756097, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2521/> really i was going to vote for you until i read this falwell is an ass clown <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.365983: step 3100, loss 0.00287643, acc 1\n",
      "\n",
      "\n",
      "here we go at the cbs #gopdebate tune in #cruzcrew <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.369139: step 3100, loss 0.0106602, acc 1\n",
      "\n",
      "\n",
      "for the first time in 8 years my brother hit the campaign trail watch what he has to say about our next president <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.372024: step 3100, loss 0.000195007, acc 1\n",
      "\n",
      "\n",
      "lifting the cap on social security would increase costoflivingadjustments and increase minimum benefits paid to lowincome seniors <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.375303: step 3100, loss 0.0085273, acc 1\n",
      "\n",
      "\n",
      "im sorry <AT_NAME_392/> being senator is a fine job but <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.378376: step 3100, loss 0.00590663, acc 1\n",
      "\n",
      "\n",
      "watch live from manchester nh <URL/> #fitn #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.382564: step 3100, loss 0.00122817, acc 1\n",
      "\n",
      "\n",
      "we’re bringing the message of liberty to the live free or die state <URL/> #fitn #nhpolitics <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.386074: step 3100, loss 0.00460944, acc 1\n",
      "\n",
      "\n",
      "we have to send a clear message just because your child gets across the border that doesnt mean the child gets to stay  hrc 2014 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.389465: step 3100, loss 0.000413452, acc 1\n",
      "\n",
      "\n",
      "how we will end childhood poverty in america today by standing together as one nation as a united nation <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.392870: step 3100, loss 0.000187022, acc 1\n",
      "\n",
      "\n",
      "we must significantly increase the role that all people play in the political process change happens when we stand up and fight back <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.396307: step 3100, loss 0.000156748, acc 1\n",
      "\n",
      "\n",
      "caucus day is here iowa text caucus to 47246 to find your caucus location and make a plan to get there by 630 pm shes counting on you <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.400422: step 3100, loss 5.84108e-05, acc 1\n",
      "\n",
      "\n",
      "the reality is there is one major country on earth that does not guarantee health care to all people the united states #demdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.404046: step 3100, loss 0.00291518, acc 1\n",
      "\n",
      "\n",
      "thank you illinois great news #votetrumpil on 3/15 trump 28% cruz 15% rubio 14% kasich 13% bush 8% carson 6% simon poll/siu <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.407779: step 3100, loss 0.00118031, acc 1\n",
      "\n",
      "\n",
      "thank you trump2016 #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.411445: step 3100, loss 0.0159641, acc 1\n",
      "\n",
      "\n",
      "great turnout at our meet greet at the grille on maine in newberry everyone was excited to hear our policies <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.414921: step 3100, loss 0.0202969, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_223/> thank you for joining me tonight and being a good friend ill see you tomorrow morning #iacaucus <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.418702: step 3100, loss 0.000529268, acc 1\n",
      "\n",
      "\n",
      "they say you dont care they say you wont caucus they say we cant win prove them wrong #caucusforbernie <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.421914: step 3100, loss 0.000262822, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1727/> we have to wonder what ppl have been doing whilst attending classes for years cannot read speak coherently or spell <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.425139: step 3100, loss 0.000331585, acc 1\n",
      "\n",
      "\n",
      "missouris voter registration deadline is today and its an open primary visit <URL/> for details <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.428144: step 3100, loss 0.0158893, acc 1\n",
      "\n",
      "\n",
      "even though i beat him in the first six debates especially the last one ted cruz wants to debate me again can we do it in canada <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.431442: step 3100, loss 0.000784328, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1057/> <AT_NAME_1058/> <AT_NAME_1059/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.434519: step 3100, loss 0.0194589, acc 1\n",
      "\n",
      "\n",
      "number 76 #fitn #christie2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.437644: step 3100, loss 0.00559016, acc 1\n",
      "\n",
      "\n",
      "get your jeb bumper sticker here → <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.440920: step 3100, loss 0.0131463, acc 1\n",
      "\n",
      "\n",
      "join rapper and activist <AT_NAME_984/> at our national day of action phone bank happening right now in orangeburg <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.445481: step 3100, loss 0.00479963, acc 1\n",
      "\n",
      "\n",
      "corporations cut millions of retirement dreams by ending benefit pension plans but social security was right there with full benefits <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.448828: step 3100, loss 0.00103801, acc 1\n",
      "\n",
      "\n",
      "the latest revelation about the public health crisis in flint is horrifying <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.453318: step 3100, loss 8.01054e-05, acc 1\n",
      "\n",
      "\n",
      "unlike <AT_NAME_638/> i fought against obamacare expansion every step of the way the next president must do the same <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.457088: step 3100, loss 0.000189525, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1080/> #benghazi is an issue because your lack of conscience caused numerous unpardonable american deaths <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.460537: step 3100, loss 0.00186258, acc 1\n",
      "\n",
      "\n",
      "such great support in new hampshire so many people are working so hard to #makeamericagreatagain <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.463742: step 3100, loss 0.00805783, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2514/> id rather be endorsed by the spawn of satan himself oh wait same thing <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.466650: step 3100, loss 0.00179523, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2001/> cnn polls are just too far right biased nice try shithead👎🇺🇸 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.471212: step 3100, loss 0.00494009, acc 1\n",
      "\n",
      "\n",
      "our campaign had absolutely nothing to do with this fraudulent facebook post <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.474384: step 3100, loss 0.00366314, acc 1\n",
      "\n",
      "\n",
      "regardless of their income we need to give our children a fair shot at attending college college education tuition should be free for all <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.477470: step 3100, loss 0.0046194, acc 1\n",
      "\n",
      "\n",
      "yet more details about how <AT_NAME_251/> put her political security ahead of national security <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.480496: step 3100, loss 0.000142326, acc 1\n",
      "\n",
      "\n",
      "hillary and dr glenda glover of <AT_NAME_886/> hbcus are fundamental to the basic bargain of america <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.483969: step 3100, loss 0.00788623, acc 1\n",
      "\n",
      "\n",
      "these are not only issues of economic inequality—they are also issues of racial inequality <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.487534: step 3100, loss 0.0025313, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1581/> wants to talk to you about love and kindness via <AT_NAME_1582/> <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.490286: step 3100, loss 0.00884023, acc 1\n",
      "\n",
      "\n",
      "today in 2016 in many respects a college degree is the equivalent of what a high school degree was 50 years ago #demtownhall <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.493500: step 3100, loss 0.000135174, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2377/> i get the hand in hand thing <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.497063: step 3100, loss 0.00190303, acc 1\n",
      "\n",
      "\n",
      "we can reignite the promise of america and with your help we will join the #cruzcrew <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.502082: step 3100, loss 0.00123937, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_386/> state legislator for president <URL/> #gopdebate #fitn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.506670: step 3100, loss 0.0101657, acc 1\n",
      "\n",
      "\n",
      "i am honored to receive the endorsement of a great public servant <AT_NAME_756/> (mi11) click here to read more <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.510000: step 3100, loss 0.000845194, acc 1\n",
      "\n",
      "\n",
      "america is about standing together as one people not allowing ourselves to be divided up by religion or nationality <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.513201: step 3100, loss 0.000992878, acc 1\n",
      "\n",
      "\n",
      "big storm in new hampshire moved my event to monday will be there next four days <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.516863: step 3100, loss 0.000948694, acc 1\n",
      "\n",
      "\n",
      "im most concerned about the kids who are left out and left behind #demtownhall <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.519864: step 3100, loss 0.000402131, acc 1\n",
      "\n",
      "\n",
      "this week hillary spoke about breaking down barriers for african americans heres how she did it in new york <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.522936: step 3100, loss 9.70317e-05, acc 1\n",
      "\n",
      "\n",
      "#makeamericagreatagain #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.525787: step 3100, loss 0.00211932, acc 1\n",
      "\n",
      "\n",
      "#thechristierecord #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.529006: step 3100, loss 0.00932091, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2224/> <AT_NAME_2225/> <AT_NAME_2226/> trump supporters really have to get off of the hate train i like him i like cruzwellsee <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.532491: step 3100, loss 0.000657342, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2056/> the fact that we cant get adequate healthcare to our veterans is also a national disgrace <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.535595: step 3100, loss 0.0161909, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_278/> <AT_NAME_279/> on <AT_NAME_280/> he finished 2nd but he made the turn successfully like a pro <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.538892: step 3100, loss 0.000822325, acc 1\n",
      "\n",
      "\n",
      "tune in tonight at 8 pm for the #goptownhall on <AT_NAME_727/> looking forward to presenting my solutions to the important issues facing our nation <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.542289: step 3100, loss 0.00426324, acc 1\n",
      "\n",
      "\n",
      "thank you for your support great to meet you <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.545287: step 3100, loss 0.0936835, acc 1\n",
      "\n",
      "\n",
      "millions of americans lost their jobs homes and life savings because of a handful of people on wall street #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.548867: step 3100, loss 0.00448127, acc 1\n",
      "\n",
      "\n",
      "the line waiting outside to get in to our henderson nv rally with <AT_NAME_850/> #nvgopcaucus <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.553359: step 3100, loss 0.00537522, acc 1\n",
      "\n",
      "\n",
      "if youre with hillary now is the time to stand up and say so if you can pitch in to show her you’re with her <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.557769: step 3100, loss 0.00488457, acc 1\n",
      "\n",
      "\n",
      "a physicist explains #marcomentum <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.562609: step 3100, loss 0.00446228, acc 1\n",
      "\n",
      "\n",
      "rip barry coates a brave veteran who should still be with us today best way to honor him is to fix a broken va <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.565941: step 3100, loss 0.0007773, acc 1\n",
      "\n",
      "\n",
      "im excited to join <AT_NAME_32/> on #siriusxmurbanview 126 in just a few minutes be sure to tune in <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.570541: step 3100, loss 0.0014127, acc 1\n",
      "\n",
      "\n",
      "new video last night we saw the difference between the talkers the leaders <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.573810: step 3100, loss 9.03566e-05, acc 1\n",
      "\n",
      "\n",
      "i want to call on <AT_NAME_320/> and <AT_NAME_321/> to include <AT_NAME_322/> on the debate stage <URL/> #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.576942: step 3100, loss 0.00395077, acc 1\n",
      "\n",
      "\n",
      "americans are right to be angry but we’re also hungry for real solutions <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.580133: step 3100, loss 0.000450152, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1399/> shut up <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.584516: step 3100, loss 0.0188789, acc 1\n",
      "\n",
      "\n",
      "today we remember the greatest atrocity in human history the horror of the holocaust must never be forgotten <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.587914: step 3100, loss 9.46477e-05, acc 1\n",
      "\n",
      "\n",
      "ive met so many wonderful supporters already this morning at several voting locations in south carolina #bc2dc16 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.591394: step 3100, loss 0.00108362, acc 1\n",
      "\n",
      "\n",
      "standing together we will create the grassroots political revolution that this country needs #votetogether <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.594044: step 3100, loss 0.000255671, acc 1\n",
      "\n",
      "\n",
      "poverty is free enterprise not reaching people <URL/> #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.597637: step 3100, loss 0.0107893, acc 1\n",
      "\n",
      "\n",
      "as president i will repair our broken immigration system by securing the border and restoring the concept of the american melting pot <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.601828: step 3100, loss 0.000158893, acc 1\n",
      "\n",
      "\n",
      "#cruzcrew tune in to cbs at 9 pm et <URL/> #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.605266: step 3100, loss 0.504843, acc 1\n",
      "\n",
      "\n",
      "proud to speak at the #iowacaucus in urbandale <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.610040: step 3100, loss 0.0109861, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2550/> if you are president how many federal holidays will you create <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.613234: step 3100, loss 0.139084, acc 1\n",
      "\n",
      "\n",
      "sad to hear another #borinqueneer francisco torregrosa died before receiving congressional gold medal <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.616502: step 3100, loss 0.00241346, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1714/> their goal is to create the condition of the global crash (global chaotic situation) in order to favor the emergence of <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.620955: step 3100, loss 0.000796239, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1135/> <AT_NAME_1136/> <AT_NAME_1137/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.623719: step 3100, loss 0.00257625, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1580/> well this is why your name is trump because you have the trump card <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.626864: step 3100, loss 0.00129806, acc 1\n",
      "\n",
      "\n",
      "the senate’s duty is to advise and consent you know what the senate is advising right now <URL/> #scotus <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.629906: step 3100, loss 0.00308086, acc 1\n",
      "\n",
      "\n",
      "will be interviewed on <AT_NAME_455/> at 800 am enjoy <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.633097: step 3100, loss 0.000509609, acc 1\n",
      "\n",
      "\n",
      "dont miss your chance to stand with hillary before the #iacaucus say #imwithher with your custom commit card <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.636603: step 3100, loss 0.000185472, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2563/> hes a nutcase no wonder he likes you <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.640089: step 3100, loss 0.00188959, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1152/> <AT_NAME_1153/> <AT_NAME_1154/> <AT_NAME_1155/> <AT_NAME_1156/> #feelthebern <AT_NAME_1157/> <AT_NAME_1158/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.643799: step 3100, loss 0.000159013, acc 1\n",
      "\n",
      "\n",
      "this is an emergency —hillary calling for action on the flint water crisis #demdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.646717: step 3100, loss 0.000112646, acc 1\n",
      "\n",
      "\n",
      "looking forward to a great day of meeting with #nvcaucus goers and supporters together we can return our nation to we the people #bc2dc16 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.649952: step 3100, loss 0.00111542, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2386/> only donald j trump make america win again #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.653256: step 3100, loss 1.56163e-05, acc 1\n",
      "\n",
      "\n",
      "yeah <AT_NAME_521/> got a *bit* of experience <URL/> #demdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.656392: step 3100, loss 0.00106755, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2227/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.660389: step 3100, loss 0.0139795, acc 1\n",
      "\n",
      "\n",
      "i guess firstterm senators stick together cc <AT_NAME_394/> <URL/> #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.663934: step 3100, loss 0.0144485, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2058/> is way off on this one dont defend a tyrant <AT_NAME_2059/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.667484: step 3100, loss 0.000668064, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_981/> honored to have your support together well take a stand for safety respect and full equality for trans americans <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.670546: step 3100, loss 0.00278989, acc 1\n",
      "\n",
      "\n",
      "tonight 830 pm tune in to cnn for the #gopdebate <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.673068: step 3100, loss 0.00904274, acc 1\n",
      "\n",
      "\n",
      "rsvp now for our rally in oklahoma city on friday dont miss your chance to join us before march 1st <URL/> #okformarco <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.676325: step 3100, loss 0.000752285, acc 1\n",
      "\n",
      "\n",
      "#choosecruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.679442: step 3100, loss 0.0154494, acc 1\n",
      "\n",
      "\n",
      "we need a proven leader with a record of solving problems  someone who doesnt cut run #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.682682: step 3100, loss 0.0287364, acc 1\n",
      "\n",
      "\n",
      "the likelihood is that wall street gets away with a lot more illegal behavior than we know of #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.685662: step 3100, loss 2.87168, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2512/> some of us dont care about who endorses you but rather what your stance on issues are along with your character <AT_NAME_2513/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.688713: step 3100, loss 0.000126235, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_310/> <AT_NAME_311/> <AT_NAME_312/> i agree great businessman i like your tone tonight keep it up youll have this democrats vote <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.691581: step 3100, loss 0.000345647, acc 1\n",
      "\n",
      "\n",
      "one of five people in this country that get a prescription from a doctor cannot afford to fill that prescription that’s wrong #bernieinmn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.694998: step 3100, loss 0.00714639, acc 1\n",
      "\n",
      "\n",
      "grt 2 have ias <AT_NAME_164/> support us looks like we made it to final rose ceremony <AT_NAME_165/> #thebachelor <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.698035: step 3100, loss 5.34043e-05, acc 1\n",
      "\n",
      "\n",
      "when it comes to my conservative record of accomplishments the numbers speak for themselves <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.702713: step 3100, loss 0.00523304, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_422/> trump just talked on msnbc like a man who knows he had a very good night <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.705873: step 3100, loss 0.0177432, acc 1\n",
      "\n",
      "\n",
      "this shouldnt be a debate about labels—it should be about making progress share this if you agree #demdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.709046: step 3100, loss 6.07949e-05, acc 1\n",
      "\n",
      "\n",
      "the fact that we’re seeing a launch from a nuclear north korea is a direct effect of a failure of the clinton administration #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.713387: step 3100, loss 0.00104349, acc 1\n",
      "\n",
      "\n",
      "this is what is so great about this country <URL/> #fitn #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.716825: step 3100, loss 0.0125429, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1145/> <AT_NAME_1146/> <AT_NAME_1147/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.720491: step 3100, loss 0.0105427, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_175/> thanks for stopping by appreciate you taking the time this morning <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.723764: step 3100, loss 0.00953489, acc 1\n",
      "\n",
      "\n",
      "real compassion is providing people a ladder of opportunity to climb up from a state of dependence #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.726900: step 3100, loss 0.000794334, acc 1\n",
      "\n",
      "\n",
      "there is only one person who is ready to be commander in chief on stage at the #demdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.730093: step 3100, loss 0.000249593, acc 1\n",
      "\n",
      "\n",
      "statement on tonights #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.733523: step 3100, loss 0.0326189, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1053/> <AT_NAME_1054/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.736544: step 3100, loss 0.0221269, acc 1\n",
      "\n",
      "\n",
      "listen to caucusgoers from all over iowa share why theyre on #teammarco <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.739723: step 3100, loss 0.00127818, acc 1\n",
      "\n",
      "\n",
      "help reignite the promise of america join the #cruzcrew <URL/> #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.743593: step 3100, loss 0.00166885, acc 1\n",
      "\n",
      "\n",
      "wishing a very happy birthday to my friend and a tremendous voice for liberty <AT_NAME_862/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.748117: step 3100, loss 0.0025577, acc 1\n",
      "\n",
      "\n",
      "during last night’s #gopdebate i called out <AT_NAME_641/> he is not a true conservative watch <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.751163: step 3100, loss 0.0133389, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1144/> 75% of the abortions are black babies i guess you an hillary are racist over 99 thousand black babies last year aborted <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.754276: step 3100, loss 0.00117102, acc 1\n",
      "\n",
      "\n",
      "awesome <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.757633: step 3100, loss 0.0129114, acc 1\n",
      "\n",
      "\n",
      "yesterday <AT_NAME_782/> and i talked with <AT_NAME_783/> on <AT_NAME_784/> about the #scprimary watch <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.760886: step 3100, loss 0.00161851, acc 1\n",
      "\n",
      "\n",
      "on with <AT_NAME_160/> right now listen live <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.764687: step 3100, loss 0.00119293, acc 1\n",
      "\n",
      "\n",
      "proud to welcome <AT_NAME_652/> to our growing team <URL/> #teammarco <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.768321: step 3100, loss 0.0353525, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1567/> <AT_NAME_1568/> read this) <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.771683: step 3100, loss 0.029372, acc 1\n",
      "\n",
      "\n",
      "thank you iowa this is our time join us <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.775071: step 3100, loss 0.0122257, acc 1\n",
      "\n",
      "\n",
      "most polls close at 7pm but if you live in any of these places your polls are open until 8 get out vote #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.779796: step 3100, loss 0.000345886, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2114/> if you truly believe in socialism/collectivism you are free to live it theres no call to impose it on the rest of us <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.782946: step 3100, loss 0.00641181, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1043/> whats your involvement in #therockefellerinitiative #thetruthisoutthere #disclosure <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.786170: step 3100, loss 0.0229818, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_964/> <AT_NAME_965/> romney should have been a tough guy with obama he cowered and lost badly hes not relevant <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.789319: step 3100, loss 0.000546664, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2002/> <AT_NAME_2003/> <AT_NAME_2004/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.792708: step 3100, loss 0.00401524, acc 1\n",
      "\n",
      "\n",
      "looking forward to being on #kellyfile with <AT_NAME_768/> tonight at 9 pm (est) looking forward to presenting real solutions tune in <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.795755: step 3100, loss 0.000675093, acc 1\n",
      "\n",
      "\n",
      "after a great night in south carolina were taking our momentum into nevada join us <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.799430: step 3100, loss 0.0127883, acc 1\n",
      "\n",
      "\n",
      "donald trump thinks planned parenthood is wonderful i think we ought to investigate and prosecute planned parenthood #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.802852: step 3100, loss 0.00554464, acc 1\n",
      "\n",
      "\n",
      "#votetrumpnh #nhprimary #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.806198: step 3100, loss 0.000795048, acc 1\n",
      "\n",
      "\n",
      "#cruzcrew ill be on <AT_NAME_624/> and <AT_NAME_625/> <AT_NAME_626/> this morning hope youll tune in <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.809178: step 3100, loss 0.00225837, acc 1\n",
      "\n",
      "\n",
      "the #carsonflattax is a simple transparent fair tax that will empower we the people and energize our economy <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.811953: step 3100, loss 0.00163802, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2199/> not a chance way to busy supporting #berniesanders <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.817552: step 3100, loss 0.026146, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2388/> congratulations and thank you jerry <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.821083: step 3100, loss 0.0944015, acc 1\n",
      "\n",
      "\n",
      "3 days until nh heads to the polls here’s one thing you can do right now to help hillary—no matter where you live <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.824492: step 3100, loss 0.000370434, acc 1\n",
      "\n",
      "\n",
      "its been less than a week since a mass shooting tore apart a community and now another this has to end praying for hesston ks h <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.828887: step 3100, loss 0.000329679, acc 1\n",
      "\n",
      "\n",
      "my statement of the sad lose of a great man justice scalia <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.831592: step 3100, loss 0.00120555, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2482/> yay its all coming together/go trump <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.834512: step 3100, loss 0.010123, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1473/> <AT_NAME_1474/> <AT_NAME_1475/> #dumpglennbeck he has lost it and will only drag you down <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.838371: step 3100, loss 0.00413314, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1898/> did you hear the one about another negative zero endorsing rubio it must be his cute little puppy growl charm <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.841291: step 3100, loss 0.000322409, acc 1\n",
      "\n",
      "\n",
      "you should never have to choose between your family your paycheck proud of all we accomplished with #fmla23—next paid leave for all h <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.844990: step 3100, loss 0.00409241, acc 1\n",
      "\n",
      "\n",
      "never let them see you sweat <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.848098: step 3100, loss 0.126813, acc 1\n",
      "\n",
      "\n",
      "after a great night in iowa yesterday looking forward to our rally in exeter nh tonight get details here <URL/> #fitn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.851914: step 3100, loss 0.000764435, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1912/> <AT_NAME_1913/> <AT_NAME_1914/> cruz wont get pass iowaafter that country moves to center right <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.855210: step 3100, loss 0.00476653, acc 1\n",
      "\n",
      "\n",
      "asked about his record <AT_NAME_387/> only gives general answers on how proud he was of his time as a public servant <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.858541: step 3100, loss 0.000190955, acc 1\n",
      "\n",
      "\n",
      "closing statement on tonights #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.861839: step 3100, loss 0.0492925, acc 1\n",
      "\n",
      "\n",
      "ted cruz does not have the right temperment to be president look at the way he totally panicked in firing his director of comm bad <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.864965: step 3100, loss 0.000623747, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1535/> well go get it and straighten this mess out <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.870231: step 3100, loss 0.0780907, acc 1\n",
      "\n",
      "\n",
      "as president i will tear down the epa’s blend wall #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.873536: step 3100, loss 0.000906057, acc 1\n",
      "\n",
      "\n",
      "thanks to george of fl his gift put us over the $140k line so he won a signed football <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.876900: step 3100, loss 9.8462e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_591/> thank you <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.880855: step 3100, loss 0.0123549, acc 1\n",
      "\n",
      "\n",
      "read our postdebate statement on tonights #gopdebate in greenville south carolina full statement here <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.885487: step 3100, loss 0.00363202, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2419/> theocracy <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.888618: step 3100, loss 0.00949321, acc 1\n",
      "\n",
      "\n",
      "thank you to all of the incredible volunteers behind the scenes in iowa #caucusfortrump <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.891723: step 3100, loss 0.000657581, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1587/> <AT_NAME_1588/> <AT_NAME_1589/>  and when did <AT_NAME_1590/> become a constitutional scholar ted cruz is an idiot for quoting beck <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.894752: step 3100, loss 0.00309738, acc 1\n",
      "\n",
      "\n",
      "i will be on <AT_NAME_872/> tonight from las vegas nevada at 10pme enjoy #hannity #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.897806: step 3100, loss 0.00460694, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2029/> <AT_NAME_2030/> <AT_NAME_2031/> <AT_NAME_2032/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.900762: step 3100, loss 0.0115871, acc 1\n",
      "\n",
      "\n",
      "couldnt agree more <AT_NAME_658/> its time to act on gun violence thank you for telling these stories today <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.904119: step 3100, loss 0.013906, acc 1\n",
      "\n",
      "\n",
      "watch our town hall from dover nh live here <URL/> #christie2016 #fitn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.907336: step 3100, loss 7.31918e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1356/> <AT_NAME_1357/> <AT_NAME_1358/> boo hoo hoo im a wealthy fart face from ny scared of <AT_NAME_1359/> cries tough man trump <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.910527: step 3100, loss 0.000185949, acc 1\n",
      "\n",
      "\n",
      "you guys ready for a radical idea this morning #gotvforbernie <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.913942: step 3100, loss 0.000957031, acc 1\n",
      "\n",
      "\n",
      "dropped by the red arrow in milford nh their fresh blueberry muffins look too good to pass up <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.917322: step 3100, loss 0.00231534, acc 1\n",
      "\n",
      "\n",
      "this shows what a complete total liar ted cruz is he said he wouldnt have nominated john roberts really <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.923310: step 3100, loss 0.000550476, acc 1\n",
      "\n",
      "\n",
      "today every day we have an obligation to confront hate advance human rights help those fleeing persecution #holocaustmemorialday h <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.926683: step 3100, loss 0.00240965, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1859/> werkin up a sweat <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.929227: step 3100, loss 0.0227677, acc 1\n",
      "\n",
      "\n",
      "nearly eight out of ten workers who can take time off under the family and medical leave act cannot do so because they can’t afford it <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T00:28:44.932739: step 3100, loss 0.00360019, acc 1\n",
      "\n",
      "\n",
      "2016-03-08T00:28:44.938466: step 3100, loss 4.75294, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:44.941498: step 3100, loss 0.00460825, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.944154: step 3100, loss 4.30336e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.947428: step 3100, loss 0.00135068, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.950642: step 3100, loss 0.000328725, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.954043: step 3100, loss 0.0019304, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.957091: step 3100, loss 0.000567275, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.960003: step 3100, loss 0.000173077, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.962781: step 3100, loss 5.35235e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.967420: step 3100, loss 0.000209667, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.970653: step 3100, loss 5.09011e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.975035: step 3100, loss 0.00137794, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.978214: step 3100, loss 0.00114423, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.981100: step 3100, loss 0.0020897, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.983778: step 3100, loss 0.00332398, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.987207: step 3100, loss 0.00110803, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.990838: step 3100, loss 0.000841859, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.995455: step 3100, loss 5.57884e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:44.998681: step 3100, loss 0.0867216, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.001867: step 3100, loss 0.000342906, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.004706: step 3100, loss 0.00111982, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.007864: step 3100, loss 0.000926066, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.010861: step 3100, loss 3.39905, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.014190: step 3100, loss 0.000646859, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.017191: step 3100, loss 0.0316475, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.020276: step 3100, loss 7.10241, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.023391: step 3100, loss 0.000804935, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.027865: step 3100, loss 0.0617233, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.031373: step 3100, loss 8.20295, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.035323: step 3100, loss 0.000145544, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.038615: step 3100, loss 0.0055476, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.041809: step 3100, loss 5.6396, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.045163: step 3100, loss 6.4202, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.048330: step 3100, loss 5.71835, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.051746: step 3100, loss 0.000676046, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.054575: step 3100, loss 0.000142087, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.058004: step 3100, loss 0.0199687, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.061835: step 3100, loss 0.00130259, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.065109: step 3100, loss 0.000117891, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.067815: step 3100, loss 0.00170503, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.070950: step 3100, loss 0.000506988, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.074576: step 3100, loss 0.00121674, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.079070: step 3100, loss 0.00327194, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.082546: step 3100, loss 0.0112759, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.085905: step 3100, loss 0.000598847, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.090548: step 3100, loss 0.000731439, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.094250: step 3100, loss 3.99343e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.097333: step 3100, loss 0.000552263, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.100408: step 3100, loss 0.00219462, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.103947: step 3100, loss 0.00226955, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.107011: step 3100, loss 0.000531294, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.110577: step 3100, loss 4.8447, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.113706: step 3100, loss 0.00191077, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.117356: step 3100, loss 0.000278077, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.120591: step 3100, loss 0.000709163, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.123620: step 3100, loss 0.0808556, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.126736: step 3100, loss 0.000120275, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.130715: step 3100, loss 0.00100074, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.134047: step 3100, loss 8.23909, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.137233: step 3100, loss 0.000807555, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.140875: step 3100, loss 0.0270708, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.143571: step 3100, loss 0.163731, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.148659: step 3100, loss 0.0402863, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.153556: step 3100, loss 0.000756097, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.156861: step 3100, loss 0.00287643, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.161797: step 3100, loss 0.0106602, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.165301: step 3100, loss 0.000195007, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.168443: step 3100, loss 0.0085273, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.172039: step 3100, loss 8.29276, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.175342: step 3100, loss 0.00590663, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.178679: step 3100, loss 6.08577, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.182344: step 3100, loss 0.00122817, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.185791: step 3100, loss 0.00460944, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.190415: step 3100, loss 0.000413452, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.193872: step 3100, loss 0.000187022, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.196964: step 3100, loss 0.000156748, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.200569: step 3100, loss 5.84108e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.203845: step 3100, loss 0.00291518, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.207026: step 3100, loss 0.00118031, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.210464: step 3100, loss 0.0159641, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.213255: step 3100, loss 0.0202969, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.216704: step 3100, loss 0.000529268, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.219644: step 3100, loss 6.56803, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.223436: step 3100, loss 0.000262822, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.226304: step 3100, loss 0.000331585, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.229677: step 3100, loss 0.0158893, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.233861: step 3100, loss 0.000784328, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.237628: step 3100, loss 0.0194589, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.241355: step 3100, loss 0.00559016, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.244710: step 3100, loss 0.0131463, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.247579: step 3100, loss 0.00479963, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.250820: step 3100, loss 0.00103801, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.254481: step 3100, loss 6.83189, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.257812: step 3100, loss 8.01054e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.261146: step 3100, loss 0.000189525, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.265361: step 3100, loss 0.00186258, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.270093: step 3100, loss 0.00805783, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.273235: step 3100, loss 0.00179523, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.276792: step 3100, loss 0.00494009, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.279537: step 3100, loss 0.00366314, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.282946: step 3100, loss 3.55999, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.287541: step 3100, loss 0.0046194, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.290880: step 3100, loss 0.000142326, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.294128: step 3100, loss 0.00788623, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.297354: step 3100, loss 3.67712, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.300471: step 3100, loss 0.0025313, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.302995: step 3100, loss 0.00884023, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.306326: step 3100, loss 0.000135174, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.309233: step 3100, loss 0.00190303, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.312408: step 3100, loss 0.00123937, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.315520: step 3100, loss 0.0101657, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.318891: step 3100, loss 0.000845194, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.322133: step 3100, loss 0.000992878, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.325800: step 3100, loss 4.92206, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.329301: step 3100, loss 0.000948694, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.332580: step 3100, loss 0.000402131, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.335833: step 3100, loss 9.70317e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.340182: step 3100, loss 0.00211932, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.343819: step 3100, loss 0.00932091, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.347030: step 3100, loss 7.14186, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.350479: step 3100, loss 10.0047, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.353705: step 3100, loss 0.000657342, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.356602: step 3100, loss 0.0161909, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.359703: step 3100, loss 0.000822325, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.362779: step 3100, loss 0.00426324, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.367610: step 3100, loss 0.0936835, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.371044: step 3100, loss 0.00448127, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.374667: step 3100, loss 0.00537522, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.377626: step 3100, loss 0.00488457, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.380632: step 3100, loss 0.00446228, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.383818: step 3100, loss 0.0007773, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.387381: step 3100, loss 0.0014127, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.391494: step 3100, loss 9.03566e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.394692: step 3100, loss 0.00395077, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.397600: step 3100, loss 0.000450152, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.401194: step 3100, loss 0.0188789, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.404869: step 3100, loss 9.46477e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.408088: step 3100, loss 0.00108362, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.411185: step 3100, loss 0.000255671, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.414275: step 3100, loss 0.0107893, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.417704: step 3100, loss 0.000158893, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.422274: step 3100, loss 0.504843, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.425532: step 3100, loss 0.0109861, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.428739: step 3100, loss 0.139084, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.433204: step 3100, loss 5.07245, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.436479: step 3100, loss 0.00241346, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.439532: step 3100, loss 0.000796239, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.443521: step 3100, loss 0.00257625, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.446958: step 3100, loss 0.00129806, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.450068: step 3100, loss 0.00308086, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.453026: step 3100, loss 0.000509609, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.456433: step 3100, loss 0.000185472, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.459799: step 3100, loss 0.00188959, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.462954: step 3100, loss 0.000159013, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.467378: step 3100, loss 0.000112646, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.470236: step 3100, loss 0.00111542, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.473245: step 3100, loss 1.56163e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.476331: step 3100, loss 0.00106755, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.480032: step 3100, loss 0.0139795, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.483239: step 3100, loss 0.0144485, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.487908: step 3100, loss 0.000668064, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.490744: step 3100, loss 0.00278989, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.494805: step 3100, loss 0.00904274, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.497589: step 3100, loss 4.37348, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.500956: step 3100, loss 0.000752285, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.504307: step 3100, loss 0.0154494, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.507692: step 3100, loss 0.0287364, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.510565: step 3100, loss 7.05156, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.513624: step 3100, loss 2.87168, acc 0\n",
      "2016-03-08T00:28:45.516782: step 3100, loss 0.000126235, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.520131: step 3100, loss 0.000345647, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.523522: step 3100, loss 0.00714639, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.526861: step 3100, loss 5.34043e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.529462: step 3100, loss 0.00523304, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.532954: step 3100, loss 2.24949, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.535647: step 3100, loss 0.0177432, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.539126: step 3100, loss 6.07949e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.542572: step 3100, loss 8.20266, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.546816: step 3100, loss 0.00104349, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.550326: step 3100, loss 0.0125429, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.553724: step 3100, loss 0.0105427, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.556812: step 3100, loss 0.00953489, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.560301: step 3100, loss 0.000794334, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.563869: step 3100, loss 0.000249593, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.567071: step 3100, loss 0.0326189, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.570215: step 3100, loss 0.0221269, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.573361: step 3100, loss 0.00127818, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.576838: step 3100, loss 0.00166885, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.579806: step 3100, loss 0.0025577, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.583861: step 3100, loss 6.07174, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.586624: step 3100, loss 0.0133389, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.589876: step 3100, loss 0.00117102, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.592789: step 3100, loss 0.0129114, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.595743: step 3100, loss 0.00161851, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.599834: step 3100, loss 0.00119293, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.602863: step 3100, loss 3.8966, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.606045: step 3100, loss 0.0353525, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.609540: step 3100, loss 0.029372, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.613910: step 3100, loss 3.49331, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.616627: step 3100, loss 4.345, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.620062: step 3100, loss 0.0122257, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.623543: step 3100, loss 7.68806, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.626961: step 3100, loss 0.000345886, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.629934: step 3100, loss 0.00641181, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.633280: step 3100, loss 0.0229818, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.636587: step 3100, loss 7.00923, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.639688: step 3100, loss 0.000546664, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.643186: step 3100, loss 0.00401524, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.646431: step 3100, loss 0.000675093, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.651865: step 3100, loss 0.0127883, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.654792: step 3100, loss 0.00554464, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.657833: step 3100, loss 0.000795048, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.661238: step 3100, loss 0.00225837, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.665061: step 3100, loss 0.00163802, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.668399: step 3100, loss 0.026146, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.671738: step 3100, loss 0.0944015, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.675072: step 3100, loss 0.000370434, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.678277: step 3100, loss 0.000329679, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.681110: step 3100, loss 0.00120555, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.684873: step 3100, loss 0.010123, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.687903: step 3100, loss 8.68256, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.691782: step 3100, loss 0.00413314, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.695370: step 3100, loss 0.000322409, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.698578: step 3100, loss 0.00409241, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.702557: step 3100, loss 0.126813, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.705561: step 3100, loss 0.000764435, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.708848: step 3100, loss 0.00476653, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.712575: step 3100, loss 0.000190955, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.716064: step 3100, loss 5.77604, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.719389: step 3100, loss 0.0492925, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.722502: step 3100, loss 0.000623747, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.725950: step 3100, loss 0.0780907, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.729070: step 3100, loss 0.000906057, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.732189: step 3100, loss 9.8462e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.735100: step 3100, loss 0.0123549, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.738216: step 3100, loss 0.00363202, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.741788: step 3100, loss 3.82202, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.744807: step 3100, loss 0.00949321, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.748656: step 3100, loss 0.000657581, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.752351: step 3100, loss 0.00309738, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.756507: step 3100, loss 0.00460694, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.759294: step 3100, loss 0.0115871, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.762435: step 3100, loss 0.013906, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.765563: step 3100, loss 7.31918e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.768580: step 3100, loss 0.000185949, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.771586: step 3100, loss 7.92873, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.775027: step 3100, loss 0.000957031, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.778639: step 3100, loss 0.00231534, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.782779: step 3100, loss 0.000550476, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.786025: step 3100, loss 0.00240965, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.789301: step 3100, loss 0.0227677, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.792678: step 3100, loss 0.00360019, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.795787: step 3100, loss 0.000660321, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.799027: step 3100, loss 0.00343721, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.802458: step 3100, loss 0.0598048, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.807139: step 3100, loss 0.000877472, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.812408: step 3100, loss 0.000977753, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.815609: step 3100, loss 0.00430039, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.818448: step 3100, loss 0.000279268, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.821590: step 3100, loss 0.00098871, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.824772: step 3100, loss 0.00879427, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.828234: step 3100, loss 0.0207255, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.831540: step 3100, loss 0.000548451, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.835306: step 3100, loss 0.00133437, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.838803: step 3100, loss 0.000285466, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.842181: step 3100, loss 0.00446312, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.845503: step 3100, loss 0.00131461, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.848444: step 3100, loss 0.00306862, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.852087: step 3100, loss 0.000507584, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.854969: step 3100, loss 0.000170216, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.858374: step 3100, loss 0.0052245, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.862520: step 3100, loss 0.0213153, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.865912: step 3100, loss 4.50258, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.869021: step 3100, loss 0.00137485, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.872519: step 3100, loss 0.00626564, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.876247: step 3100, loss 0.0209699, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.879344: step 3100, loss 0.00427832, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.882761: step 3100, loss 0.083935, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.885659: step 3100, loss 0.00462059, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.888882: step 3100, loss 0.000849244, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.892235: step 3100, loss 4.48472, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.895594: step 3100, loss 0.00042644, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.898693: step 3100, loss 0.000182374, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.901528: step 3100, loss 0.000574424, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.905144: step 3100, loss 0.0126876, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.908672: step 3100, loss 0.0175985, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.913322: step 3100, loss 0.0180975, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.918615: step 3100, loss 0.000550357, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.921698: step 3100, loss 0.00243879, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.925510: step 3100, loss 0.000921897, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.928759: step 3100, loss 0.000197034, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.931995: step 3100, loss 2.57695, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.935107: step 3100, loss 0.00781573, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.938656: step 3100, loss 0.0044344, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.941698: step 3100, loss 0.00523897, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.944716: step 3100, loss 0.00496903, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.948826: step 3100, loss 0.0456781, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.952362: step 3100, loss 0.000148524, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.955901: step 3100, loss 0.000132433, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.959214: step 3100, loss 0.0101906, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.962140: step 3100, loss 5.27964, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.965256: step 3100, loss 0.00505254, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.969214: step 3100, loss 0.00856677, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.972462: step 3100, loss 0.00200214, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.975599: step 3100, loss 0.0510935, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.978448: step 3100, loss 0.00132568, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.981744: step 3100, loss 0.011865, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.984887: step 3100, loss 0.00152388, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.988487: step 3100, loss 0.00125151, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.991749: step 3100, loss 0.00785299, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:45.994976: step 3100, loss 7.37215, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:45.998348: step 3100, loss 0.000102276, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.001587: step 3100, loss 6.51885, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.004869: step 3100, loss 0.00992695, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.007825: step 3100, loss 7.68378, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.011336: step 3100, loss 0.000998118, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.015767: step 3100, loss 0.000717263, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.018926: step 3100, loss 0.00446774, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.023366: step 3100, loss 6.13909e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.026715: step 3100, loss 0.00805735, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.029946: step 3100, loss 0.000104899, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.033255: step 3100, loss 0.001189, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.036369: step 3100, loss 0.0934394, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.039892: step 3100, loss 0.000217772, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.042987: step 3100, loss 0.00257982, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.045948: step 3100, loss 0.00474043, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.050475: step 3100, loss 0.00012242, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.054132: step 3100, loss 0.00257304, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.057477: step 3100, loss 1.51395e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.060527: step 3100, loss 0.00537747, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.063778: step 3100, loss 0.000888549, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.066998: step 3100, loss 0.000709044, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.071207: step 3100, loss 0.000231239, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.076517: step 3100, loss 0.00241001, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.080156: step 3100, loss 0.0385596, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.083171: step 3100, loss 0.00545964, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.086279: step 3100, loss 0.0293458, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.089503: step 3100, loss 0.0262603, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.092963: step 3100, loss 0.00601032, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.096755: step 3100, loss 0.000115984, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.099536: step 3100, loss 0.000536179, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.102719: step 3100, loss 0.00108588, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.107482: step 3100, loss 0.00910665, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.111936: step 3100, loss 0.00155971, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.115780: step 3100, loss 0.0004828, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.118818: step 3100, loss 0.00233473, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.122563: step 3100, loss 0.0033323, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.126773: step 3100, loss 0.000489234, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.130119: step 3100, loss 0.000297502, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.132952: step 3100, loss 0.000496264, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.136302: step 3100, loss 6.08739, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.139177: step 3100, loss 0.0510935, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.141949: step 3100, loss 0.0135942, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.145164: step 3100, loss 0.00290068, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.148287: step 3100, loss 7.45316, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.151758: step 3100, loss 6.1818, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.155141: step 3100, loss 0.00453337, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.158331: step 3100, loss 0.297739, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.161726: step 3100, loss 0.0167533, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.164420: step 3100, loss 9.26213e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.167934: step 3100, loss 6.73302, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.171325: step 3100, loss 0.00129806, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.174714: step 3100, loss 0.00184842, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.179465: step 3100, loss 5.2932, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.182628: step 3100, loss 0.005765, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.186355: step 3100, loss 0.0195472, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.189439: step 3100, loss 0.00150234, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.192612: step 3100, loss 0.00830352, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.195464: step 3100, loss 3.74139, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.198794: step 3100, loss 0.0524495, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.202357: step 3100, loss 0.000244468, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.205606: step 3100, loss 0.000293927, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.209056: step 3100, loss 0.00344089, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.213388: step 3100, loss 0.0562035, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.216714: step 3100, loss 0.000898315, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.219650: step 3100, loss 6.22006, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.222889: step 3100, loss 8.88589, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.225901: step 3100, loss 4.94448, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.230579: step 3100, loss 0.000480656, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.233898: step 3100, loss 0.0350502, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.237427: step 3100, loss 0.000230405, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.240431: step 3100, loss 0.00251228, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.243510: step 3100, loss 0.0582277, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.246544: step 3100, loss 0.0177557, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.250618: step 3100, loss 0.00185603, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.253931: step 3100, loss 0.0166217, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.256452: step 3100, loss 0.000562033, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.259673: step 3100, loss 0.0159101, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.262671: step 3100, loss 0.0148736, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.267273: step 3100, loss 2.57231, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.270968: step 3100, loss 0.000114434, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.274026: step 3100, loss 8.42774e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.277305: step 3100, loss 0.00274793, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.282279: step 3100, loss 0.000150431, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.285691: step 3100, loss 0.00112054, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.288748: step 3100, loss 0.054701, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.292117: step 3100, loss 0.0303347, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.295147: step 3100, loss 0.000167713, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.298323: step 3100, loss 0.00538269, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.301596: step 3100, loss 0.00447914, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.304901: step 3100, loss 0.00419807, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.308148: step 3100, loss 0.00519367, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.311285: step 3100, loss 0.00843155, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.314619: step 3100, loss 0.00266649, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.318746: step 3100, loss 0.0181306, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.322740: step 3100, loss 0.0875804, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.325582: step 3100, loss 3.95861, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.330057: step 3100, loss 0.0139803, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.335126: step 3100, loss 0.0449546, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.338819: step 3100, loss 0.00187567, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.342593: step 3100, loss 0.00197513, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.345806: step 3100, loss 0.00698588, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.350454: step 3100, loss 0.00208042, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.353947: step 3100, loss 0.00021503, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.357250: step 3100, loss 0.000144233, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.360287: step 3100, loss 8.76653, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.365032: step 3100, loss 0.000548808, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.368625: step 3100, loss 0.000132194, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.371855: step 3100, loss 0.000310254, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.375192: step 3100, loss 0.005694, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.379158: step 3100, loss 0.00498054, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.382324: step 3100, loss 0.00436022, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.387478: step 3100, loss 0.0006795, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.390568: step 3100, loss 0.000210978, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.394139: step 3100, loss 0.0614838, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.397898: step 3100, loss 0.000737753, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.400786: step 3100, loss 0.00406309, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.404015: step 3100, loss 0.000506035, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.407438: step 3100, loss 8.54021, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.411047: step 3100, loss 5.93843, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.415570: step 3100, loss 0.00991751, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.419031: step 3100, loss 0.000254003, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.422487: step 3100, loss 0.000367336, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.425802: step 3100, loss 0.000227902, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.428603: step 3100, loss 0.00122746, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.431992: step 3100, loss 3.09939e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.435553: step 3100, loss 0.0180363, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.439788: step 3100, loss 0.000121944, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.442725: step 3100, loss 0.000458493, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.446172: step 3100, loss 5.29509, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.449589: step 3100, loss 0.000343025, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.453006: step 3100, loss 0.00107445, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.456488: step 3100, loss 0.0033304, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.459783: step 3100, loss 0.000420124, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.462958: step 3100, loss 0.000237794, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.465659: step 3100, loss 0.000399033, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.469103: step 3100, loss 0.0337708, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.472106: step 3100, loss 7.59381, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.475696: step 3100, loss 0.00194503, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.478748: step 3100, loss 0.000364952, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.482277: step 3100, loss 0.00399268, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.485356: step 3100, loss 0.000711545, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.488745: step 3100, loss 0.0152297, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.492819: step 3100, loss 0.00441825, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.496118: step 3100, loss 0.000256029, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.499849: step 3100, loss 0.0153133, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.502875: step 3100, loss 0.000996808, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.505997: step 3100, loss 0.00159125, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.509345: step 3100, loss 0.00442241, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.512646: step 3100, loss 8.36814e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.515587: step 3100, loss 0.0007773, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.518867: step 3100, loss 0.000163423, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.522158: step 3100, loss 0.00371825, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.526607: step 3100, loss 0.000330513, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.530946: step 3100, loss 0.00201725, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.533958: step 3100, loss 0.0028592, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.538728: step 3100, loss 6.8543e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.542700: step 3100, loss 0.0075737, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.548150: step 3100, loss 0.000736561, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.550874: step 3100, loss 0.000484588, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.554127: step 3100, loss 0.000671161, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.557150: step 3100, loss 7.2609, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.560006: step 3100, loss 0.000326461, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.563299: step 3100, loss 0.00519438, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.566344: step 3100, loss 0.00137378, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.569414: step 3100, loss 0.00279928, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.572050: step 3100, loss 0.00250122, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.575383: step 3100, loss 0.00559763, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.578631: step 3100, loss 0.000591342, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.582119: step 3100, loss 0.000164257, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.585651: step 3100, loss 0.00502004, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.588868: step 3100, loss 3.05832, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.592198: step 3100, loss 0.00170872, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.595390: step 3100, loss 0.000763959, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.599454: step 3100, loss 0.0390722, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.603012: step 3100, loss 0.00551832, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.605832: step 3100, loss 0.00384948, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.609021: step 3100, loss 6.48641, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.612162: step 3100, loss 0.000401416, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.615212: step 3100, loss 0.00113744, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.619662: step 3100, loss 0.000876638, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.624001: step 3100, loss 0.000666158, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.627648: step 3100, loss 0.000427393, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.630980: step 3100, loss 0.00201416, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.634523: step 3100, loss 0.00170336, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.638946: step 3100, loss 0.00162945, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.642262: step 3100, loss 0.00139152, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.645639: step 3100, loss 0.000174865, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.648923: step 3100, loss 0.00811293, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.652906: step 3100, loss 0.00132759, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.656093: step 3100, loss 0.000199536, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.659267: step 3100, loss 0.000326103, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.662629: step 3100, loss 0.000344217, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.665836: step 3100, loss 0.000459208, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.669959: step 3100, loss 0.000909868, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.673431: step 3100, loss 0.000135889, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.677048: step 3100, loss 0.0155353, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.679990: step 3100, loss 0.117196, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.683091: step 3100, loss 0.00260942, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.687706: step 3100, loss 8.36268, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.690976: step 3100, loss 0.00390565, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.694517: step 3100, loss 0.00195051, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.697846: step 3100, loss 0.00172419, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.701451: step 3100, loss 0.000198106, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.705509: step 3100, loss 0.00115542, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.709857: step 3100, loss 0.0996013, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.713162: step 3100, loss 4.96957, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.716346: step 3100, loss 0.10538, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.719624: step 3100, loss 0.000185592, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.722767: step 3100, loss 0.000982041, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.725960: step 3100, loss 0.0325468, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.729214: step 3100, loss 9.72701e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.732269: step 3100, loss 0.0280984, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.735916: step 3100, loss 0.000297025, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.739000: step 3100, loss 0.0052602, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.741982: step 3100, loss 0.000460042, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.745341: step 3100, loss 0.000152099, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.748654: step 3100, loss 0.101094, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.751929: step 3100, loss 0.000304891, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.754905: step 3100, loss 0.0018345, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.758457: step 3100, loss 0.00092678, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.761397: step 3100, loss 0.00307908, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.764380: step 3100, loss 0.00358285, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.767588: step 3100, loss 0.0693331, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.771346: step 3100, loss 0.0102556, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.774523: step 3100, loss 0.00299375, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.778997: step 3100, loss 0.00372431, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.782130: step 3100, loss 0.00388487, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.785134: step 3100, loss 8.62381, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.787988: step 3100, loss 0.000211812, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.791446: step 3100, loss 0.000977634, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.794774: step 3100, loss 0.000113481, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.797947: step 3100, loss 0.000599324, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.802731: step 3100, loss 0.000134936, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.805984: step 3100, loss 0.000148047, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.809577: step 3100, loss 0.000307989, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.813150: step 3100, loss 5.61796, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.816521: step 3100, loss 0.00223601, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.819580: step 3100, loss 0.00186127, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.824271: step 3100, loss 5.14656, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.828798: step 3100, loss 3.05171e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.832069: step 3100, loss 0.0657761, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.835754: step 3100, loss 0.000155318, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.839289: step 3100, loss 0.0280365, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.842627: step 3100, loss 0.00344053, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.846277: step 3100, loss 0.000289637, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.849700: step 3100, loss 0.000128857, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.852749: step 3100, loss 0.000762291, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.856321: step 3100, loss 0.00347118, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.859567: step 3100, loss 0.000222181, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.864051: step 3100, loss 0.00102837, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.867474: step 3100, loss 3.17907, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.871117: step 3100, loss 0.000347792, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.874184: step 3100, loss 0.00152555, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.877446: step 3100, loss 0.0030212, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.882038: step 3100, loss 0.00106493, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.885327: step 3100, loss 0.000795882, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.888770: step 3100, loss 0.00052212, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.892121: step 3100, loss 1.57732, acc 0\n",
      "2016-03-08T00:28:46.895427: step 3100, loss 0.00105718, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.898590: step 3100, loss 7.47367, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.902116: step 3100, loss 0.00117578, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.905612: step 3100, loss 7.73638e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.908607: step 3100, loss 0.00152698, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.911924: step 3100, loss 8.58738, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:46.916167: step 3100, loss 0.00117281, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.920992: step 3100, loss 0.000352797, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.923868: step 3100, loss 0.000209548, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.927401: step 3100, loss 0.00603508, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.930924: step 3100, loss 0.000290352, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.933705: step 3100, loss 0.00080291, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.936650: step 3100, loss 0.00128068, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.939790: step 3100, loss 0.00132068, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.943066: step 3100, loss 0.0014658, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.947718: step 3100, loss 0.00144056, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.950913: step 3100, loss 0.00255544, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.954414: step 3100, loss 0.000545472, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.957809: step 3100, loss 0.00334038, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.960879: step 3100, loss 0.0178568, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.964174: step 3100, loss 0.014166, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.967812: step 3100, loss 0.00458914, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.970971: step 3100, loss 0.00037818, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.974651: step 3100, loss 0.000619577, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.977494: step 3100, loss 0.0015027, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.980317: step 3100, loss 0.00104921, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.983808: step 3100, loss 0.00295286, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.987053: step 3100, loss 0.0112967, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.989979: step 3100, loss 0.000394386, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.993364: step 3100, loss 0.00479038, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:46.996717: step 3100, loss 0.0585991, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.000148: step 3100, loss 0.000532247, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.003151: step 3100, loss 0.00166992, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.006683: step 3100, loss 0.000254956, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.009678: step 3100, loss 0.00263689, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.014071: step 3100, loss 0.000295715, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.017636: step 3100, loss 0.000644238, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.021272: step 3100, loss 0.0125062, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.024951: step 3100, loss 0.000292616, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.028515: step 3100, loss 0.000249355, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.031600: step 3100, loss 0.00670315, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.035122: step 3100, loss 6.8046, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.039047: step 3100, loss 0.000418337, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.042318: step 3100, loss 0.000710235, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.045446: step 3100, loss 0.00204069, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.048511: step 3100, loss 0.0611715, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.051848: step 3100, loss 6.57985, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.055315: step 3100, loss 0.00356099, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.058418: step 3100, loss 0.00391443, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.061997: step 3100, loss 0.00147698, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.065431: step 3100, loss 0.0105887, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.069146: step 3100, loss 4.89938e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.072953: step 3100, loss 0.000465046, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.076388: step 3100, loss 0.000286657, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.079724: step 3100, loss 0.0312888, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.083097: step 3100, loss 0.000594082, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.086748: step 3100, loss 0.000524145, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.089709: step 3100, loss 0.000262464, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.094639: step 3100, loss 0.0266637, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.098160: step 3100, loss 0.0936251, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.101046: step 3100, loss 9.0595e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.104163: step 3100, loss 0.00501221, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.107784: step 3100, loss 7.71175, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.111051: step 3100, loss 5.06646, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.114358: step 3100, loss 0.00123901, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.117551: step 3100, loss 0.010979, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.120984: step 3100, loss 0.00373868, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.125001: step 3100, loss 0.00109196, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.128255: step 3100, loss 0.000670328, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.131774: step 3100, loss 0.000945479, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.135341: step 3100, loss 0.00040368, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.138660: step 3100, loss 8.54474, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.142612: step 3100, loss 0.00294395, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.146152: step 3100, loss 6.24696, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.149618: step 3100, loss 0.0120675, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.152343: step 3100, loss 0.00187055, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.155246: step 3100, loss 0.012032, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.158263: step 3100, loss 0.00104039, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.161270: step 3100, loss 0.00134735, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.164323: step 3100, loss 7.21686, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.168441: step 3100, loss 0.000153291, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.171105: step 3100, loss 0.000543328, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.174622: step 3100, loss 0.00248267, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.178391: step 3100, loss 0.00778604, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.181222: step 3100, loss 0.000291901, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.184723: step 3100, loss 0.0668932, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.189336: step 3100, loss 0.00498718, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.192466: step 3100, loss 0.00707857, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.195830: step 3100, loss 0.0074923, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.199275: step 3100, loss 0.0102284, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.202727: step 3100, loss 0.0222314, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.205861: step 3100, loss 0.000835904, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.209145: step 3100, loss 2.53913e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.212514: step 3100, loss 0.016509, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.215986: step 3100, loss 0.00109589, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.219412: step 3100, loss 0.00111887, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.222553: step 3100, loss 0.000848529, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.225875: step 3100, loss 0.000222062, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.230278: step 3100, loss 0.000874018, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.233765: step 3100, loss 5.06627e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.236735: step 3100, loss 0.00047863, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.239781: step 3100, loss 0.00854053, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.244132: step 3100, loss 8.06822, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.247005: step 3100, loss 0.000687601, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.250328: step 3100, loss 0.0281517, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.253438: step 3100, loss 0.00291756, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.256725: step 3100, loss 4.79561, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.260442: step 3100, loss 0.00451486, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.263807: step 3100, loss 5.53715, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.267220: step 3100, loss 0.000112408, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.269968: step 3100, loss 8.5827e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.273419: step 3100, loss 0.000791356, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.276734: step 3100, loss 0.000595392, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.280870: step 3100, loss 0.000143637, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.284170: step 3100, loss 0.000114554, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.287544: step 3100, loss 0.00142508, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.291150: step 3100, loss 0.00266352, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.294320: step 3100, loss 0.0108591, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.297676: step 3100, loss 0.000295595, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.301010: step 3100, loss 0.000240535, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.304223: step 3100, loss 0.00045754, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.307473: step 3100, loss 0.00044622, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.312074: step 3100, loss 0.00870446, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.316465: step 3100, loss 0.000238033, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.319487: step 3100, loss 0.000441573, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.322332: step 3100, loss 0.00343661, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.325205: step 3100, loss 0.000632682, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.328835: step 3100, loss 0.000784566, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.332884: step 3100, loss 0.0248912, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.336288: step 3100, loss 0.000538085, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.339927: step 3100, loss 0.000669613, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.343470: step 3100, loss 0.0142834, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.346863: step 3100, loss 0.00334989, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.350279: step 3100, loss 0.00109886, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.353136: step 3100, loss 0.00211932, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.356368: step 3100, loss 0.0257955, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.359348: step 3100, loss 5.74743, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.362603: step 3100, loss 0.00106326, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.365943: step 3100, loss 9.2608, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.369257: step 3100, loss 0.000556195, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.372279: step 3100, loss 0.00171812, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.376175: step 3100, loss 0.00027474, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.379394: step 3100, loss 2.47952e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.382523: step 3100, loss 0.000107163, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.386831: step 3100, loss 0.00224469, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.390545: step 3100, loss 0.00535672, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.394356: step 3100, loss 4.03107, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.397710: step 3100, loss 0.00280238, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.400809: step 3100, loss 0.00699275, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.405060: step 3100, loss 0.000181778, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.408637: step 3100, loss 0.00108291, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.411990: step 3100, loss 0.00142139, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.415323: step 3100, loss 1.83581e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.418895: step 3100, loss 9.76985, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.422097: step 3100, loss 0.00198453, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.425390: step 3100, loss 6.60398e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.428498: step 3100, loss 0.0037458, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.431774: step 3100, loss 0.0111894, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.435114: step 3100, loss 1.63315e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.439332: step 3100, loss 0.0033481, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.442812: step 3100, loss 0.00216738, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.446535: step 3100, loss 0.00384983, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.449595: step 3100, loss 0.000151861, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.452586: step 3100, loss 0.00222721, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.457184: step 3100, loss 0.0480086, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.460133: step 3100, loss 0.000253764, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.463447: step 3100, loss 0.0196231, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.466839: step 3100, loss 0.000412617, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.470018: step 3100, loss 0.00120805, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.473191: step 3100, loss 0.012116, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.476290: step 3100, loss 0.000473626, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.479287: step 3100, loss 0.0001136, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.482810: step 3100, loss 0.000128976, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.486041: step 3100, loss 0.000400344, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.490107: step 3100, loss 0.00020633, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.493799: step 3100, loss 0.00692006, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.497072: step 3100, loss 0.00044062, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.499985: step 3100, loss 0.00533277, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.503122: step 3100, loss 0.000540587, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.506275: step 3100, loss 0.00803914, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.509245: step 3100, loss 0.000352321, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.512612: step 3100, loss 0.00644, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.515855: step 3100, loss 8.528, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.518815: step 3100, loss 7.91518e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.521897: step 3100, loss 0.0212303, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.525096: step 3100, loss 0.000829114, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.528298: step 3100, loss 0.0254096, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.531022: step 3100, loss 8.10572, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.534536: step 3100, loss 0.000336829, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.537492: step 3100, loss 8.70709, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.541917: step 3100, loss 0.0022077, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.544796: step 3100, loss 0.000648646, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.547972: step 3100, loss 5.58613, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.551104: step 3100, loss 0.000503294, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.554231: step 3100, loss 0.0431641, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.557609: step 3100, loss 0.000161754, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.560910: step 3100, loss 0.0221189, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.564382: step 3100, loss 0.0061756, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.567901: step 3100, loss 5.1645, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.570918: step 3100, loss 0.000394505, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.574074: step 3100, loss 0.000135413, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.577413: step 3100, loss 0.00545798, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.581239: step 3100, loss 0.00105123, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.584486: step 3100, loss 0.0106017, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.587688: step 3100, loss 0.00647423, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.590646: step 3100, loss 7.68541, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.594621: step 3100, loss 0.000659725, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.598080: step 3100, loss 0.00203093, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.601455: step 3100, loss 0.0213178, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.604481: step 3100, loss 0.000708448, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.607481: step 3100, loss 0.00087914, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.610745: step 3100, loss 0.00323024, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.613643: step 3100, loss 0.000284512, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.616568: step 3100, loss 0.0129114, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.619275: step 3100, loss 0.00108791, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.622554: step 3100, loss 5.50798, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.625791: step 3100, loss 0.00756909, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.629264: step 3100, loss 0.000324435, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.632430: step 3100, loss 0.010946, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.635583: step 3100, loss 0.00309168, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.638757: step 3100, loss 0.0815287, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.641556: step 3100, loss 0.000161396, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.645261: step 3100, loss 4.13632, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.648733: step 3100, loss 0.000232431, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.651566: step 3100, loss 0.0852949, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.655571: step 3100, loss 0.000547378, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.658726: step 3100, loss 0.00161148, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.661958: step 3100, loss 0.000764674, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.665215: step 3100, loss 5.84108e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.668747: step 3100, loss 0.00317688, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.673339: step 3100, loss 0.000404395, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.676361: step 3100, loss 0.000842336, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.679464: step 3100, loss 0.0051792, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.682739: step 3100, loss 0.00106195, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.685582: step 3100, loss 0.000237079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.688575: step 3100, loss 0.0108379, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.692412: step 3100, loss 0.00188316, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.696556: step 3100, loss 0.00171764, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.699641: step 3100, loss 0.0132998, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.705930: step 3100, loss 0.0449948, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.709349: step 3100, loss 0.00084841, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.712182: step 3100, loss 0.00181165, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.715820: step 3100, loss 0.000931663, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.718874: step 3100, loss 7.8675e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.721804: step 3100, loss 4.72159, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.725460: step 3100, loss 0.00223601, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.729082: step 3100, loss 5.30321, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.731964: step 3100, loss 1.52426, acc 0\n",
      "2016-03-08T00:28:47.734980: step 3100, loss 0.012057, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.738162: step 3100, loss 0.000193339, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.741510: step 3100, loss 0.000421674, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.744411: step 3100, loss 0.0105076, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.748882: step 3100, loss 0.00126056, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.752417: step 3100, loss 0.00945247, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.755850: step 3100, loss 0.000158655, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.759226: step 3100, loss 0.00140925, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.762570: step 3100, loss 7.93177, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.765873: step 3100, loss 8.23964, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.769338: step 3100, loss 0.0737293, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.772383: step 3100, loss 0.000366621, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.775211: step 3100, loss 0.0259468, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.778084: step 3100, loss 0.000725721, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.780909: step 3100, loss 0.00575718, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.783874: step 3100, loss 0.000161277, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.788658: step 3100, loss 0.0325175, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.791716: step 3100, loss 0.0004094, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.795194: step 3100, loss 0.00175394, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.799790: step 3100, loss 0.000124685, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.802762: step 3100, loss 0.00716853, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.806105: step 3100, loss 0.000772893, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.809184: step 3100, loss 0.000124804, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.812441: step 3100, loss 0.000590388, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.815765: step 3100, loss 0.000818394, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.818505: step 3100, loss 0.000680692, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.821522: step 3100, loss 9.28128, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.825182: step 3100, loss 0.00239895, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.828158: step 3100, loss 3.63104, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.831500: step 3100, loss 0.00144711, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.834891: step 3100, loss 0.00525984, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.837946: step 3100, loss 0.00395302, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.841305: step 3100, loss 0.00498742, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.844535: step 3100, loss 0.000337425, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.847898: step 3100, loss 0.000648407, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.852559: step 3100, loss 0.00154233, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.857332: step 3100, loss 0.00126401, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.861930: step 3100, loss 0.000905104, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.866065: step 3100, loss 3.56977, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.869080: step 3100, loss 0.0020119, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.873706: step 3100, loss 0.000319668, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.877917: step 3100, loss 0.00386551, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.880865: step 3100, loss 0.00164469, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.884203: step 3100, loss 0.00037103, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.888898: step 3100, loss 0.000212289, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.892068: step 3100, loss 0.00122246, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.895525: step 3100, loss 5.56692e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.898800: step 3100, loss 0.0250589, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.901964: step 3100, loss 0.0430721, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.905885: step 3100, loss 0.000764316, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.909562: step 3100, loss 0.00187864, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.913063: step 3100, loss 3.61198e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.916171: step 3100, loss 4.11402, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.919232: step 3100, loss 0.000607187, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.922307: step 3100, loss 0.000252811, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.925747: step 3100, loss 7.6993, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.929052: step 3100, loss 0.00649472, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.931867: step 3100, loss 0.00335583, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.935462: step 3100, loss 0.0243379, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.939059: step 3100, loss 0.000467549, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.942341: step 3100, loss 0.00124044, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.945368: step 3100, loss 9.10717e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.949531: step 3100, loss 0.00243629, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.952778: step 3100, loss 0.000482324, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.957140: step 3100, loss 0.000659487, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.961703: step 3100, loss 0.000309419, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.965321: step 3100, loss 0.0261561, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.968726: step 3100, loss 0.000320026, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.971762: step 3100, loss 0.0036287, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.975208: step 3100, loss 0.00528249, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.978068: step 3100, loss 0.000737276, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.981474: step 3100, loss 0.116549, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.984711: step 3100, loss 6.70645, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:47.987913: step 3100, loss 0.0118469, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.991052: step 3100, loss 0.000303818, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.994343: step 3100, loss 0.000143875, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:47.997278: step 3100, loss 0.0172897, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.000874: step 3100, loss 7.72973, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.005345: step 3100, loss 0.00123044, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.009885: step 3100, loss 0.000240535, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.013518: step 3100, loss 0.00802483, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.016922: step 3100, loss 0.00337424, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.020333: step 3100, loss 0.000655079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.023434: step 3100, loss 0.000204423, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.026472: step 3100, loss 6.46093e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.029934: step 3100, loss 0.00402153, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.033259: step 3100, loss 0.00250419, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.035731: step 3100, loss 0.0133118, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.039073: step 3100, loss 0.0316511, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.042448: step 3100, loss 0.00029929, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.045672: step 3100, loss 0.000253049, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.049070: step 3100, loss 0.00235625, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.052666: step 3100, loss 0.0169628, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.055817: step 3100, loss 0.0012121, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.059535: step 3100, loss 0.000108355, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.063527: step 3100, loss 0.00658201, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.066717: step 3100, loss 0.0015853, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.069891: step 3100, loss 0.0190786, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.072838: step 3100, loss 0.0177097, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.076132: step 3100, loss 0.000928686, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.079187: step 3100, loss 0.000167833, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.082630: step 3100, loss 0.0235306, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.086233: step 3100, loss 0.00569649, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.090012: step 3100, loss 0.00265912, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.093559: step 3100, loss 0.0122953, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.096736: step 3100, loss 0.0481219, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.100070: step 3100, loss 0.000265325, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.103030: step 3100, loss 0.00234579, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.105959: step 3100, loss 0.00527099, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.109346: step 3100, loss 0.00971365, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.112281: step 3100, loss 0.00277313, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.116565: step 3100, loss 0.000112408, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.119785: step 3100, loss 1.69213, acc 0\n",
      "2016-03-08T00:28:48.123374: step 3100, loss 0.00168253, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.126611: step 3100, loss 0.000102634, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.130096: step 3100, loss 0.0016166, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.132891: step 3100, loss 0.00511279, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.136044: step 3100, loss 3.86231e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.139754: step 3100, loss 0.0707023, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.144443: step 3100, loss 6.46337, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.148802: step 3100, loss 0.00144473, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.152013: step 3100, loss 0.00125889, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.156323: step 3100, loss 1.93149, acc 0\n",
      "2016-03-08T00:28:48.159064: step 3100, loss 0.0044986, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.162351: step 3100, loss 0.00223197, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.166018: step 3100, loss 0.00174775, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.170711: step 3100, loss 0.00240727, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.175519: step 3100, loss 0.000937142, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.179101: step 3100, loss 0.00139806, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.182315: step 3100, loss 0.00487307, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.186012: step 3100, loss 0.00887958, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.189524: step 3100, loss 0.00115042, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.192878: step 3100, loss 0.00861428, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.196304: step 3100, loss 0.000667111, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.199555: step 3100, loss 2.43737, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.203122: step 3100, loss 0.000912607, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.206512: step 3100, loss 0.00117281, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.209936: step 3100, loss 4.32937, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.213445: step 3100, loss 0.127447, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.216600: step 3100, loss 0.002943, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.219903: step 3100, loss 0.000915704, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.223943: step 3100, loss 0.00101741, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.227602: step 3100, loss 0.000622079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.231104: step 3100, loss 0.000841264, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.234654: step 3100, loss 0.000709163, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.237634: step 3100, loss 6.92493, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.241271: step 3100, loss 0.00028773, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.244474: step 3100, loss 0.0141719, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.247891: step 3100, loss 2.408e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.250884: step 3100, loss 0.0162312, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.254359: step 3100, loss 2.82785, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.257458: step 3100, loss 0.0174476, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.260617: step 3100, loss 0.00173811, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.263680: step 3100, loss 0.00113863, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.266969: step 3100, loss 0.00436045, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.270056: step 3100, loss 0.00628602, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.274659: step 3100, loss 6.30597e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.278284: step 3100, loss 0.0369148, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.282024: step 3100, loss 0.00223601, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.285609: step 3100, loss 0.00137294, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.288668: step 3100, loss 9.4115, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.291605: step 3100, loss 0.00460232, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.294320: step 3100, loss 0.000422388, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.298645: step 3100, loss 0.00177369, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.301625: step 3100, loss 0.000828757, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.305430: step 3100, loss 0.000582644, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.308608: step 3100, loss 0.00307397, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.311500: step 3100, loss 0.00303737, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.314420: step 3100, loss 0.000195603, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.317131: step 3100, loss 0.0494297, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.320281: step 3100, loss 5.19739e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.323405: step 3100, loss 0.0234665, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.326204: step 3100, loss 0.141614, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.330240: step 3100, loss 0.0173247, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.333142: step 3100, loss 6.85493, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.336619: step 3100, loss 0.00371884, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.339765: step 3100, loss 6.98753, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.343326: step 3100, loss 0.0145175, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.346608: step 3100, loss 0.000705232, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.350038: step 3100, loss 0.000637209, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.353155: step 3100, loss 0.000122182, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.356666: step 3100, loss 0.0963252, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.359953: step 3100, loss 0.0492243, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.362931: step 3100, loss 0.00153091, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.365978: step 3100, loss 0.000168429, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.369237: step 3100, loss 0.00110863, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.372294: step 3100, loss 0.000685695, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.375622: step 3100, loss 0.000303461, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.378958: step 3100, loss 7.40262e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.382912: step 3100, loss 7.64336, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.386106: step 3100, loss 0.001926, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.389378: step 3100, loss 6.30858, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.392600: step 3100, loss 0.000203112, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.395912: step 3100, loss 0.00485717, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.398966: step 3100, loss 0.00119972, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.402244: step 3100, loss 8.97239, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.405519: step 3100, loss 0.00347582, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.408801: step 3100, loss 0.00331187, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.412621: step 3100, loss 0.00136235, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.417266: step 3100, loss 0.00717551, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.420444: step 3100, loss 4.891, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.423884: step 3100, loss 5.63434, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.427156: step 3100, loss 0.00242511, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.430370: step 3100, loss 0.0197255, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.435379: step 3100, loss 0.000680334, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.438286: step 3100, loss 0.00621932, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.440904: step 3100, loss 0.00161708, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.444268: step 3100, loss 0.000641379, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.447381: step 3100, loss 0.0030918, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.450249: step 3100, loss 3.11883, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.453625: step 3100, loss 0.000637447, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.457105: step 3100, loss 0.00523648, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.460205: step 3100, loss 9.42901e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.463895: step 3100, loss 0.00256543, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.467505: step 3100, loss 0.0126876, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.470943: step 3100, loss 0.0563291, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.473818: step 3100, loss 0.00113935, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.476993: step 3100, loss 0.0109176, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.479993: step 3100, loss 0.00280249, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.483213: step 3100, loss 9.83428e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.487087: step 3100, loss 0.00351728, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.490234: step 3100, loss 0.0964811, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.493444: step 3100, loss 0.000338378, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.496648: step 3100, loss 0.0226163, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.499887: step 3100, loss 0.00262749, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.503002: step 3100, loss 0.00123091, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.506640: step 3100, loss 5.7217, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.509885: step 3100, loss 0.00189125, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.514393: step 3100, loss 5.87101, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.517496: step 3100, loss 0.00215608, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.520741: step 3100, loss 0.0742898, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.525738: step 3100, loss 0.000203112, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.528721: step 3100, loss 5.76633, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.531667: step 3100, loss 9.1191e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.534863: step 3100, loss 0.00243237, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.540172: step 3100, loss 0.000465285, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.543422: step 3100, loss 0.000477915, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.546356: step 3100, loss 0.00548797, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.549248: step 3100, loss 0.00501019, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.552328: step 3100, loss 0.000701062, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.555638: step 3100, loss 0.00015496, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.558557: step 3100, loss 0.0142073, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.561606: step 3100, loss 0.00029643, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.564429: step 3100, loss 0.0202276, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.567167: step 3100, loss 5.00457, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.570117: step 3100, loss 8.33947, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.573614: step 3100, loss 0.005829, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.577720: step 3100, loss 0.0019373, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.580913: step 3100, loss 0.0277702, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.584373: step 3100, loss 0.00512987, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.587423: step 3100, loss 0.00463507, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.591694: step 3100, loss 0.00618342, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.594987: step 3100, loss 0.00848002, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.598274: step 3100, loss 0.00167682, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.601477: step 3100, loss 2.56297e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.604515: step 3100, loss 7.22041, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.607645: step 3100, loss 0.00213169, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.611688: step 3100, loss 0.00269312, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.615289: step 3100, loss 0.00233877, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.618550: step 3100, loss 0.000437998, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.622060: step 3100, loss 0.00272748, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.624797: step 3100, loss 0.000131837, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.629233: step 3100, loss 0.000213123, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.632615: step 3100, loss 0.00171621, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.635767: step 3100, loss 0.00204128, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.638651: step 3100, loss 0.00198073, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.642948: step 3100, loss 0.00752448, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.645939: step 3100, loss 6.10419, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.649337: step 3100, loss 0.000275216, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.652703: step 3100, loss 0.000694153, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.656132: step 3100, loss 0.000244349, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.660181: step 3100, loss 0.0306927, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.663828: step 3100, loss 0.000220036, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.667169: step 3100, loss 0.000309896, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.670340: step 3100, loss 0.00205698, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.673464: step 3100, loss 5.63844e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.676671: step 3100, loss 0.00189161, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.679856: step 3100, loss 4.58475, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.683158: step 3100, loss 0.0234681, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.686307: step 3100, loss 0.000886524, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.689463: step 3100, loss 0.00246959, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.694636: step 3100, loss 0.00223601, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.697875: step 3100, loss 0.00725563, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.701661: step 3100, loss 0.000374009, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.704799: step 3100, loss 0.000117891, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.707925: step 3100, loss 0.00130044, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.712019: step 3100, loss 0.00805357, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.715284: step 3100, loss 0.0298219, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.718337: step 3100, loss 0.000515924, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.721303: step 3100, loss 0.0231344, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.724330: step 3100, loss 8.41234, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.727173: step 3100, loss 0.000518903, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.729965: step 3100, loss 0.0042503, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.732859: step 3100, loss 0.0126876, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.735906: step 3100, loss 0.00558649, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.739638: step 3100, loss 0.000798979, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.742772: step 3100, loss 0.0251765, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.746685: step 3100, loss 0.00023112, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.750292: step 3100, loss 3.33826, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.753584: step 3100, loss 0.0001254, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.756945: step 3100, loss 0.00157768, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.760196: step 3100, loss 8.04404, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.763423: step 3100, loss 0.0584176, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.766773: step 3100, loss 0.00295678, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.770148: step 3100, loss 8.38006e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.772977: step 3100, loss 0.000616718, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.776230: step 3100, loss 0.0193785, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.779103: step 3100, loss 0.00465061, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.781995: step 3100, loss 0.000215626, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.785758: step 3100, loss 0.01169, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.790099: step 3100, loss 0.00231166, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.793372: step 3100, loss 0.0720439, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.798773: step 3100, loss 0.000842216, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.802251: step 3100, loss 0.00472453, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.805584: step 3100, loss 0.00391954, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.808921: step 3100, loss 0.000185711, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.812169: step 3100, loss 0.000185472, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.816521: step 3100, loss 0.00219307, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.820089: step 3100, loss 0.00172847, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.823259: step 3100, loss 0.00101265, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.828023: step 3100, loss 0.0161502, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.832562: step 3100, loss 0.000141253, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.836580: step 3100, loss 7.53374e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.839790: step 3100, loss 0.00138473, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.843351: step 3100, loss 0.054024, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.847659: step 3100, loss 0.000890812, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.852119: step 3100, loss 0.000478392, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.856961: step 3100, loss 3.2186e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.860197: step 3100, loss 0.00147937, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.863402: step 3100, loss 0.00015651, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.866283: step 3100, loss 0.0073567, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.869549: step 3100, loss 0.000237079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.872230: step 3100, loss 0.00648347, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.876799: step 3100, loss 1.88349e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.879913: step 3100, loss 0.00126758, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.883380: step 3100, loss 0.0220803, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.886993: step 3100, loss 0.0178001, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.889847: step 3100, loss 7.7416, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.893039: step 3100, loss 0.00126246, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.895887: step 3100, loss 0.183635, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.900630: step 3100, loss 0.00296451, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.904048: step 3100, loss 0.00458416, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.909651: step 3100, loss 0.0186928, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.912926: step 3100, loss 0.00275732, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.916393: step 3100, loss 1.91925e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.919742: step 3100, loss 0.00147877, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.923603: step 3100, loss 0.0060448, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.926496: step 3100, loss 0.00148567, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.930053: step 3100, loss 0.000163899, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.933644: step 3100, loss 0.0111392, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.936723: step 3100, loss 0.000807317, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.940152: step 3100, loss 0.000191909, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.943382: step 3100, loss 0.000186664, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.946192: step 3100, loss 6.20695, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.949219: step 3100, loss 0.000216818, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.952328: step 3100, loss 0.000216461, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.955499: step 3100, loss 5.70949, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.959293: step 3100, loss 0.00245686, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.962858: step 3100, loss 0.00910866, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.966149: step 3100, loss 0.00112625, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.968912: step 3100, loss 4.58219, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.971854: step 3100, loss 7.45, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.975263: step 3100, loss 3.76166, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:48.978221: step 3100, loss 0.000879973, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.981424: step 3100, loss 0.000107521, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.984749: step 3100, loss 1.90733e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.988237: step 3100, loss 0.00311794, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.991615: step 3100, loss 0.000215507, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.994701: step 3100, loss 0.0111965, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:48.997606: step 3100, loss 6.12636, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.001133: step 3100, loss 0.0606013, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.005438: step 3100, loss 0.00043347, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.009106: step 3100, loss 7.3505, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.014735: step 3100, loss 0.00307112, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.018228: step 3100, loss 0.00118293, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.021219: step 3100, loss 0.000109667, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.024525: step 3100, loss 8.34858, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.027575: step 3100, loss 0.000359828, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.030651: step 3100, loss 0.0441683, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.033995: step 3100, loss 0.000335756, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.036949: step 3100, loss 7.92732, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.039809: step 3100, loss 0.00387548, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.042513: step 3100, loss 0.00445932, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.045775: step 3100, loss 0.000119083, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.049137: step 3100, loss 0.0079249, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.052192: step 3100, loss 0.0141818, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.055504: step 3100, loss 0.00709005, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.058658: step 3100, loss 0.0011796, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.061539: step 3100, loss 0.000391168, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.066027: step 3100, loss 0.00639547, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.069302: step 3100, loss 0.00326232, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.072616: step 3100, loss 0.000286181, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.076102: step 3100, loss 0.0875714, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.079733: step 3100, loss 0.00467126, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.083372: step 3100, loss 0.00188745, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.086385: step 3100, loss 7.12216, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.089657: step 3100, loss 9.1922, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.092853: step 3100, loss 5.91728, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.096576: step 3100, loss 0.0510935, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.099757: step 3100, loss 0.000557863, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.102739: step 3100, loss 0.000171527, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.106237: step 3100, loss 0.00436746, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.110994: step 3100, loss 0.170358, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.115417: step 3100, loss 0.000348031, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.120648: step 3100, loss 0.0121299, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.123961: step 3100, loss 0.000210382, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.127042: step 3100, loss 0.0031057, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.130243: step 3100, loss 0.000303103, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.133557: step 3100, loss 0.000430372, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.136907: step 3100, loss 0.0631255, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.140241: step 3100, loss 0.000543447, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.143168: step 3100, loss 0.000545353, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.146474: step 3100, loss 3.25436e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.149972: step 3100, loss 0.000973585, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.153010: step 3100, loss 0.00514374, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.156685: step 3100, loss 0.00228073, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.161651: step 3100, loss 0.0102162, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.164895: step 3100, loss 1.83581e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.167995: step 3100, loss 0.0190747, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.173045: step 3100, loss 0.00408482, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.176386: step 3100, loss 5.06627e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.179819: step 3100, loss 0.000586338, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.183209: step 3100, loss 0.00377965, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.188011: step 3100, loss 0.00188935, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.192687: step 3100, loss 0.00407686, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.195925: step 3100, loss 0.000965963, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.199134: step 3100, loss 0.00373606, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.202660: step 3100, loss 0.00549022, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.205529: step 3100, loss 0.000439547, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.208854: step 3100, loss 0.000296906, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.212471: step 3100, loss 0.00283566, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.216969: step 3100, loss 3.93987, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.220358: step 3100, loss 0.000493881, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.225564: step 3100, loss 2.21965, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.228604: step 3100, loss 0.0291725, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.232222: step 3100, loss 0.000893194, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.235913: step 3100, loss 0.00222423, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.239105: step 3100, loss 0.00225373, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.243276: step 3100, loss 0.00128139, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.246263: step 3100, loss 0.0032338, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.249344: step 3100, loss 0.00026461, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.252489: step 3100, loss 7.41217, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.255784: step 3100, loss 0.00168646, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.259100: step 3100, loss 5.47156e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.262450: step 3100, loss 7.28429, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.266262: step 3100, loss 0.000105256, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.269649: step 3100, loss 0.00583374, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.272798: step 3100, loss 0.052997, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.276706: step 3100, loss 0.00384948, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.280106: step 3100, loss 0.00695238, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.283154: step 3100, loss 0.000915585, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.286634: step 3100, loss 8.04449, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.291321: step 3100, loss 0.00357703, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.294371: step 3100, loss 0.000213958, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.297443: step 3100, loss 0.000865204, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.300063: step 3100, loss 0.000404991, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.302793: step 3100, loss 0.0055725, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.305967: step 3100, loss 0.000229928, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.309482: step 3100, loss 0.00183128, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.312582: step 3100, loss 0.000646621, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.316172: step 3100, loss 0.0100098, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.319207: step 3100, loss 0.000922255, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.322445: step 3100, loss 0.000356372, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.325905: step 3100, loss 0.000620054, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.329263: step 3100, loss 3.39741e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.332194: step 3100, loss 0.000661035, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.335782: step 3100, loss 0.000820062, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.339098: step 3100, loss 0.00230761, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.342413: step 3100, loss 0.000219083, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.346083: step 3100, loss 0.00579937, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.349412: step 3100, loss 0.00384485, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.352975: step 3100, loss 0.00227086, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.357773: step 3100, loss 0.0246227, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.360940: step 3100, loss 0.010344, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.364145: step 3100, loss 0.00508065, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.367339: step 3100, loss 0.0606505, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.370320: step 3100, loss 0.000581215, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.373291: step 3100, loss 5.66058, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.377839: step 3100, loss 0.000610523, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.381506: step 3100, loss 0.00120377, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.384856: step 3100, loss 0.000384853, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.387777: step 3100, loss 5.97103, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.390750: step 3100, loss 0.0116857, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.393583: step 3100, loss 0.000724292, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.396638: step 3100, loss 0.000983946, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.400180: step 3100, loss 0.000588244, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.403303: step 3100, loss 0.000668421, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.406650: step 3100, loss 0.00091094, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.409886: step 3100, loss 0.0162111, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.412626: step 3100, loss 4.97796, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.416085: step 3100, loss 0.0275539, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.419533: step 3100, loss 0.0543385, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.422993: step 3100, loss 0.000600277, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.425975: step 3100, loss 4.208e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.429289: step 3100, loss 0.00963643, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.432896: step 3100, loss 6.83339, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.436223: step 3100, loss 0.0138044, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.439294: step 3100, loss 3.54083, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.442285: step 3100, loss 0.0429129, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.446344: step 3100, loss 0.0006384, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.449203: step 3100, loss 6.4858, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.452423: step 3100, loss 6.10312, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.456106: step 3100, loss 6.22253e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.459367: step 3100, loss 0.0045507, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.462311: step 3100, loss 4.68482e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.465675: step 3100, loss 0.00103111, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.470049: step 3100, loss 3.63283, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.473240: step 3100, loss 0.000750379, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.476527: step 3100, loss 0.00252476, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.479532: step 3100, loss 0.0771398, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.482805: step 3100, loss 0.000924636, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.486519: step 3100, loss 0.0241267, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.490269: step 3100, loss 0.000641736, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.493776: step 3100, loss 0.0315368, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.496785: step 3100, loss 0.00271642, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.500023: step 3100, loss 0.00108767, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.503393: step 3100, loss 0.00410298, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.506204: step 3100, loss 0.0150039, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.509692: step 3100, loss 0.046446, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.513080: step 3100, loss 0.0043779, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.516507: step 3100, loss 0.00104563, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.519446: step 3100, loss 0.0512451, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.523090: step 3100, loss 6.49684, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.526184: step 3100, loss 0.00164957, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.529295: step 3100, loss 3.99343e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.533383: step 3100, loss 0.00345277, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.536589: step 3100, loss 0.001559, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.544665: step 3100, loss 0.00499204, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.548525: step 3100, loss 0.00684891, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.551843: step 3100, loss 0.000164614, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.555733: step 3100, loss 0.00101539, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.560068: step 3100, loss 0.00343079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.563705: step 3100, loss 0.0011503, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.566743: step 3100, loss 0.0112681, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.570043: step 3100, loss 0.00144746, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.573022: step 3100, loss 0.00196026, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.576558: step 3100, loss 0.00415878, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.581001: step 3100, loss 0.00787889, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.585504: step 3100, loss 0.0011796, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.587921: step 3100, loss 0.00100776, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.591643: step 3100, loss 0.0523867, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.594721: step 3100, loss 5.36717, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.598175: step 3100, loss 8.95036, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.601580: step 3100, loss 4.01115, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.604798: step 3100, loss 0.00457585, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.607677: step 3100, loss 0.000835308, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.610831: step 3100, loss 0.00019906, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.613774: step 3100, loss 0.00378321, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.618723: step 3100, loss 0.00408007, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.622092: step 3100, loss 0.00386278, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.625006: step 3100, loss 0.00052629, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.628082: step 3100, loss 8.12138, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.631098: step 3100, loss 7.85558e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.633858: step 3100, loss 7.88814, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.637068: step 3100, loss 0.00316749, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.640014: step 3100, loss 2.26184, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.643972: step 3100, loss 0.00111363, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.646833: step 3100, loss 7.53787, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.650189: step 3100, loss 0.00156828, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.653020: step 3100, loss 0.000300362, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.657625: step 3100, loss 0.00784317, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.660858: step 3100, loss 0.000233385, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.663876: step 3100, loss 0.0263976, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.668720: step 3100, loss 9.9058e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.671853: step 3100, loss 0.0213787, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.675622: step 3100, loss 0.00457134, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.679068: step 3100, loss 0.0012483, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.682261: step 3100, loss 0.000299528, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.685316: step 3100, loss 0.0771398, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.689073: step 3100, loss 0.00577353, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.692559: step 3100, loss 0.00023696, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.697153: step 3100, loss 0.00466698, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.700475: step 3100, loss 0.00610771, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.703697: step 3100, loss 0.00157006, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.707412: step 3100, loss 0.000769438, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.710559: step 3100, loss 0.000673425, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.714020: step 3100, loss 0.00236589, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.717679: step 3100, loss 0.000584193, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.721315: step 3100, loss 0.00223601, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.724418: step 3100, loss 0.00187388, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.727875: step 3100, loss 6.51144, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.731097: step 3100, loss 0.000286657, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.734783: step 3100, loss 0.000324197, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.738382: step 3100, loss 0.0481829, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.741733: step 3100, loss 0.0102045, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.744592: step 3100, loss 4.48217e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.748797: step 3100, loss 0.000315616, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.751788: step 3100, loss 6.44901e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.755371: step 3100, loss 0.0049096, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.760067: step 3100, loss 0.000454084, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.763094: step 3100, loss 0.00930296, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.766365: step 3100, loss 0.000703088, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.769323: step 3100, loss 0.000120513, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.772523: step 3100, loss 0.00019322, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.775668: step 3100, loss 0.0234349, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.778919: step 3100, loss 0.000119917, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.782085: step 3100, loss 0.00105242, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.786696: step 3100, loss 0.0062019, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.790142: step 3100, loss 0.000384614, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.794508: step 3100, loss 0.00205936, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.797398: step 3100, loss 0.0100958, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.802100: step 3100, loss 0.00173775, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.805822: step 3100, loss 7.58338, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.809156: step 3100, loss 0.000114673, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.812196: step 3100, loss 0.000746448, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.815090: step 3100, loss 0.000153172, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.818011: step 3100, loss 4.68482e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.821637: step 3100, loss 4.83978e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.825219: step 3100, loss 0.000175461, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.828376: step 3100, loss 9.42099, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.832009: step 3100, loss 0.00223375, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.835125: step 3100, loss 3.90081, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.838377: step 3100, loss 0.000603017, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.841536: step 3100, loss 0.000938452, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.844310: step 3100, loss 0.00501019, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.847546: step 3100, loss 0.00758991, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.850325: step 3100, loss 0.000218963, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.854326: step 3100, loss 0.00180356, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.857351: step 3100, loss 0.00487318, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.860660: step 3100, loss 0.00287441, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.863843: step 3100, loss 0.00139925, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.867380: step 3100, loss 0.187521, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.871940: step 3100, loss 2.8729e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.875778: step 3100, loss 0.10324, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.878485: step 3100, loss 0.000604328, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.881915: step 3100, loss 0.00360351, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.885841: step 3100, loss 0.0195967, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.888942: step 3100, loss 0.00774654, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.892005: step 3100, loss 6.56822e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.895363: step 3100, loss 0.000290113, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.898842: step 3100, loss 0.000328606, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.902576: step 3100, loss 6.82114, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.906259: step 3100, loss 0.000465046, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.909504: step 3100, loss 0.00239621, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.912670: step 3100, loss 0.000467906, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.915776: step 3100, loss 0.00157959, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.918906: step 3100, loss 3.25436e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.923004: step 3100, loss 0.0255635, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.926486: step 3100, loss 0.00464361, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.929622: step 3100, loss 0.000885333, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.932640: step 3100, loss 0.000589793, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.935812: step 3100, loss 0.00111208, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.939680: step 3100, loss 0.00130092, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.943196: step 3100, loss 0.000239463, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.946368: step 3100, loss 0.00683648, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.949601: step 3100, loss 0.00088438, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.952956: step 3100, loss 0.0138756, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.957444: step 3100, loss 0.00297841, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.961828: step 3100, loss 0.00239372, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.965456: step 3100, loss 0.00124687, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.969177: step 3100, loss 2.89674e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.973118: step 3100, loss 8.2754, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:49.976379: step 3100, loss 0.399012, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.979883: step 3100, loss 0.000502103, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.984282: step 3100, loss 0.00766207, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.988472: step 3100, loss 0.0367879, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.991694: step 3100, loss 0.000776585, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.994478: step 3100, loss 0.000467787, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:49.997667: step 3100, loss 0.000379252, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.000900: step 3100, loss 0.00449468, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.004302: step 3100, loss 0.00014614, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.007660: step 3100, loss 4.32888, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.011520: step 3100, loss 0.00171371, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.015534: step 3100, loss 0.000179394, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.019004: step 3100, loss 7.70774, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.022092: step 3100, loss 9.95153, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.024922: step 3100, loss 0.00316179, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.027930: step 3100, loss 0.00746485, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.031538: step 3100, loss 0.0104785, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.034890: step 3100, loss 0.00972179, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.038080: step 3100, loss 0.00100967, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.041198: step 3100, loss 0.00528202, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.044585: step 3100, loss 0.000147213, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.047445: step 3100, loss 0.0173728, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.050411: step 3100, loss 0.00453812, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.054345: step 3100, loss 0.000325627, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.057761: step 3100, loss 0.000352678, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.060501: step 3100, loss 0.00154245, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.063633: step 3100, loss 0.0644717, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.067319: step 3100, loss 0.0325048, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.070720: step 3100, loss 4.75087, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.073745: step 3100, loss 0.000131241, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.076870: step 3100, loss 0.000108594, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.079871: step 3100, loss 0.000191193, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.082994: step 3100, loss 0.00555246, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.086106: step 3100, loss 0.000236007, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.089127: step 3100, loss 0.138123, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.092373: step 3100, loss 9.28597e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.096125: step 3100, loss 0.00131509, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.099338: step 3100, loss 0.00192386, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.102420: step 3100, loss 0.0019166, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.106224: step 3100, loss 0.00682819, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.109627: step 3100, loss 0.000143279, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.112800: step 3100, loss 0.00105052, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.117297: step 3100, loss 0.0377368, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.121590: step 3100, loss 0.00131949, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.124769: step 3100, loss 0.000400821, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.128019: step 3100, loss 0.0150847, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.130894: step 3100, loss 0.00140282, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.134476: step 3100, loss 0.00059849, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.138097: step 3100, loss 0.0374686, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.142786: step 3100, loss 0.000408328, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.146574: step 3100, loss 0.00124008, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.150061: step 3100, loss 0.00184116, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.153483: step 3100, loss 8.29662e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.156857: step 3100, loss 7.27808, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.160034: step 3100, loss 0.00236042, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.163213: step 3100, loss 0.0002863, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.166172: step 3100, loss 0.548853, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.169512: step 3100, loss 0.00572174, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.173586: step 3100, loss 4.71786, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.176589: step 3100, loss 0.000237556, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.179903: step 3100, loss 6.94587, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.183107: step 3100, loss 5.2254, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.185983: step 3100, loss 0.031685, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.190284: step 3100, loss 0.000924517, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.193706: step 3100, loss 0.00242012, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.196760: step 3100, loss 0.00594953, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.201521: step 3100, loss 0.00793176, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.205032: step 3100, loss 0.00963773, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.208748: step 3100, loss 0.000343621, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.211876: step 3100, loss 0.000559769, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.215595: step 3100, loss 0.00144139, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.218703: step 3100, loss 7.53133, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.221886: step 3100, loss 0.00153948, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.225893: step 3100, loss 0.000266636, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.229307: step 3100, loss 0.00304426, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.234245: step 3100, loss 0.000392956, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.237491: step 3100, loss 0.000492928, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.240706: step 3100, loss 0.00511504, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.243668: step 3100, loss 0.000347792, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.246699: step 3100, loss 0.000261749, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.250207: step 3100, loss 0.000263656, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.253590: step 3100, loss 0.00498054, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.256674: step 3100, loss 0.0164506, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.259943: step 3100, loss 5.68836, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.263195: step 3100, loss 0.000154245, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.266237: step 3100, loss 0.0162816, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.269645: step 3100, loss 0.00795175, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.273240: step 3100, loss 0.000708091, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.277210: step 3100, loss 0.00289735, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.281220: step 3100, loss 4.55152, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.284447: step 3100, loss 0.0299298, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.288429: step 3100, loss 0.00976004, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.291471: step 3100, loss 0.000890216, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.294881: step 3100, loss 4.68562, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.298061: step 3100, loss 0.0033089, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.301493: step 3100, loss 0.00786257, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.304676: step 3100, loss 0.000280341, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.308189: step 3100, loss 0.00422312, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.311875: step 3100, loss 0.0510935, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.315127: step 3100, loss 0.0460077, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.318654: step 3100, loss 0.00283507, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.321869: step 3100, loss 0.00218213, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.325845: step 3100, loss 0.000514852, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.330017: step 3100, loss 0.00164076, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.333499: step 3100, loss 0.000267827, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.336364: step 3100, loss 0.012684, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.339990: step 3100, loss 7.35494e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.343489: step 3100, loss 0.0023685, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.346847: step 3100, loss 0.0805779, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.349854: step 3100, loss 4.46351, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.353101: step 3100, loss 0.000374247, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.356198: step 3100, loss 0.000278315, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.360991: step 3100, loss 0.000592295, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.364274: step 3100, loss 0.00202391, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.367407: step 3100, loss 0.00110625, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.370844: step 3100, loss 0.00592962, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.373977: step 3100, loss 0.00198013, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.377194: step 3100, loss 0.00378048, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.381310: step 3100, loss 8.10018, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.384877: step 3100, loss 0.0080259, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.389368: step 3100, loss 0.000847457, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.392572: step 3100, loss 8.23781, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.396062: step 3100, loss 0.00659456, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.398966: step 3100, loss 0.0111187, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.401965: step 3100, loss 0.0510935, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.405035: step 3100, loss 0.000195603, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.408266: step 3100, loss 0.00198084, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.411503: step 3100, loss 0.000496741, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.414705: step 3100, loss 0.00971671, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.418938: step 3100, loss 0.000560008, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.422050: step 3100, loss 0.000191074, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.425486: step 3100, loss 0.000824946, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.429076: step 3100, loss 0.00422775, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.433258: step 3100, loss 0.00082304, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.435991: step 3100, loss 0.000178679, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.439693: step 3100, loss 0.00132973, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.442994: step 3100, loss 0.000377941, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.446040: step 3100, loss 0.00176191, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.449046: step 3100, loss 0.000377822, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.452564: step 3100, loss 0.000574662, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.455875: step 3100, loss 5.17429, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.459351: step 3100, loss 0.000672829, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.462856: step 3100, loss 0.00628625, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.465732: step 3100, loss 0.000154483, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.469006: step 3100, loss 8.72574e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.472023: step 3100, loss 0.000128738, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.475405: step 3100, loss 0.0426687, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.478496: step 3100, loss 0.000256982, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.482221: step 3100, loss 0.000903913, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.486266: step 3100, loss 0.00529554, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.489689: step 3100, loss 0.0672418, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.492393: step 3100, loss 0.000677833, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.495869: step 3100, loss 9.96177, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.499245: step 3100, loss 0.00125151, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.502541: step 3100, loss 0.000203827, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.507138: step 3100, loss 0.00342616, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.509992: step 3100, loss 0.000264252, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.514386: step 3100, loss 0.000424176, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.517833: step 3100, loss 2.31263e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.521153: step 3100, loss 0.00161291, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.524304: step 3100, loss 0.000301435, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.527417: step 3100, loss 0.000862703, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.530383: step 3100, loss 0.00135235, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.533319: step 3100, loss 0.0197366, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.537578: step 3100, loss 10.1906, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.540674: step 3100, loss 0.0026156, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.545371: step 3100, loss 6.84467, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.549175: step 3100, loss 4.9873, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.552692: step 3100, loss 0.191818, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.556067: step 3100, loss 2.55105e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.559023: step 3100, loss 0.00463673, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.562559: step 3100, loss 0.00051652, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.566020: step 3100, loss 0.000732392, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.568876: step 3100, loss 0.000422984, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.571956: step 3100, loss 0.00139354, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.574964: step 3100, loss 0.000735489, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.578328: step 3100, loss 0.000435138, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.581417: step 3100, loss 0.0432795, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.584547: step 3100, loss 0.000964891, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.588743: step 3100, loss 0.0190786, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.591849: step 3100, loss 0.00196181, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.594973: step 3100, loss 0.00351466, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.598285: step 3100, loss 0.00311117, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.601100: step 3100, loss 0.000329321, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.603818: step 3100, loss 0.00112601, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.607456: step 3100, loss 0.0105221, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.610649: step 3100, loss 0.00224243, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.613427: step 3100, loss 0.0107624, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.616491: step 3100, loss 0.0156686, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.619510: step 3100, loss 0.000760147, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.622635: step 3100, loss 0.0278003, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.625705: step 3100, loss 0.00476511, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.629386: step 3100, loss 0.00100991, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.632663: step 3100, loss 0.013663, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.636138: step 3100, loss 7.59334e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.641438: step 3100, loss 0.00122758, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.645656: step 3100, loss 0.000439428, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.649232: step 3100, loss 0.000596226, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.653007: step 3100, loss 0.000568228, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.656054: step 3100, loss 6.0424, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.659197: step 3100, loss 0.00015639, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.663743: step 3100, loss 0.00272938, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.667189: step 3100, loss 0.00023982, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.670730: step 3100, loss 0.00293277, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.674114: step 3100, loss 0.000757526, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.677339: step 3100, loss 0.00376362, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.680501: step 3100, loss 0.00114887, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.683793: step 3100, loss 7.43542, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.687225: step 3100, loss 0.00195324, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.691610: step 3100, loss 0.000518664, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.694700: step 3100, loss 0.000381516, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.697640: step 3100, loss 5.81194, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.702134: step 3100, loss 0.00670718, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.705701: step 3100, loss 0.0154494, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.709172: step 3100, loss 2.70601e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.712997: step 3100, loss 0.0154178, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.716132: step 3100, loss 0.000243277, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.719409: step 3100, loss 0.000997046, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.722797: step 3100, loss 0.00016819, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.726907: step 3100, loss 0.000268066, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.730358: step 3100, loss 0.00497591, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.733194: step 3100, loss 0.0299963, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.736859: step 3100, loss 0.00358285, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.739902: step 3100, loss 0.00187507, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.744012: step 3100, loss 0.00938693, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.747319: step 3100, loss 0.00137461, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.750559: step 3100, loss 0.000773846, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.753796: step 3100, loss 0.00211932, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.756966: step 3100, loss 0.000365667, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.760265: step 3100, loss 7.07635, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.763729: step 3100, loss 0.000567871, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.768469: step 3100, loss 0.000158417, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.771780: step 3100, loss 0.00013434, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.774926: step 3100, loss 0.0110097, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.778111: step 3100, loss 0.000333254, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.780924: step 3100, loss 0.0332644, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.785568: step 3100, loss 0.000488162, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.789527: step 3100, loss 0.00089236, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.792702: step 3100, loss 0.00963218, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.796876: step 3100, loss 7.40511, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.800071: step 3100, loss 0.000159966, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.802767: step 3100, loss 0.00179095, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.807478: step 3100, loss 0.0258451, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.811012: step 3100, loss 0.00238741, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.814576: step 3100, loss 0.000995141, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.817687: step 3100, loss 0.000220394, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.821195: step 3100, loss 0.00342521, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.825660: step 3100, loss 0.000766222, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.828764: step 3100, loss 9.57325, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.832217: step 3100, loss 0.00265306, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.835408: step 3100, loss 6.75878, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.838643: step 3100, loss 0.00418893, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.842141: step 3100, loss 2.31263e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.845525: step 3100, loss 0.00042358, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.849681: step 3100, loss 0.00500841, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.853687: step 3100, loss 6.86532, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.856501: step 3100, loss 0.0289575, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.859675: step 3100, loss 0.000749784, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.863254: step 3100, loss 0.00223601, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.866604: step 3100, loss 7.86347, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.869733: step 3100, loss 0.0324349, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.872750: step 3100, loss 0.000121586, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.876055: step 3100, loss 0.0220679, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.879403: step 3100, loss 3.73683, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.882502: step 3100, loss 0.00638848, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.885787: step 3100, loss 0.0093959, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.890443: step 3100, loss 0.00026449, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.893853: step 3100, loss 0.000333373, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.896815: step 3100, loss 0.000837333, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.901263: step 3100, loss 0.000379014, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.904299: step 3100, loss 0.00201915, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.907866: step 3100, loss 0.00294264, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.912286: step 3100, loss 0.0089259, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.916674: step 3100, loss 0.0195104, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.919878: step 3100, loss 0.0118349, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.923270: step 3100, loss 0.00707573, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.926774: step 3100, loss 0.000563344, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.930020: step 3100, loss 0.0153688, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.933079: step 3100, loss 0.000859725, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.936307: step 3100, loss 5.22246, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.939278: step 3100, loss 0.00230416, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.942458: step 3100, loss 0.0195003, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.945967: step 3100, loss 0.00631397, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.949267: step 3100, loss 0.00274258, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.953597: step 3100, loss 0.00760884, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.957180: step 3100, loss 0.0015321, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.960535: step 3100, loss 5.99605e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.963765: step 3100, loss 3.51705, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.966801: step 3100, loss 0.00168908, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.969660: step 3100, loss 0.00140092, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.972948: step 3100, loss 6.2513, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.975905: step 3100, loss 0.00454559, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.979343: step 3100, loss 0.00429114, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.982476: step 3100, loss 0.0165003, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.986422: step 3100, loss 0.0087327, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.989768: step 3100, loss 0.00110351, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.992877: step 3100, loss 0.0017825, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:50.996249: step 3100, loss 2.54013, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:50.999497: step 3100, loss 0.00548548, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.002984: step 3100, loss 0.000673187, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.006787: step 3100, loss 0.000552025, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.009960: step 3100, loss 0.0280218, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.013100: step 3100, loss 0.00119257, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.015971: step 3100, loss 0.000784804, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.019265: step 3100, loss 0.000583955, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.022576: step 3100, loss 8.31699, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.026179: step 3100, loss 0.0438871, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.029700: step 3100, loss 0.000183446, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.034178: step 3100, loss 0.000482562, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.037511: step 3100, loss 0.000273905, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.040684: step 3100, loss 8.47542e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.045262: step 3100, loss 0.0013333, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.048443: step 3100, loss 0.00117495, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.051551: step 3100, loss 6.22181, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.054546: step 3100, loss 0.0111641, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.060384: step 3100, loss 0.00240109, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.063539: step 3100, loss 0.000525694, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.066936: step 3100, loss 0.000496979, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.070386: step 3100, loss 0.000908201, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.073800: step 3100, loss 9.87004e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.077528: step 3100, loss 0.000952982, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.080725: step 3100, loss 0.00338054, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.083693: step 3100, loss 5.21895, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.086529: step 3100, loss 0.00117162, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.089710: step 3100, loss 0.0001018, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.092913: step 3100, loss 4.37145, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.096169: step 3100, loss 0.00194456, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.099853: step 3100, loss 0.00167028, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.103036: step 3100, loss 4.40063, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.106680: step 3100, loss 0.000643523, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.111486: step 3100, loss 0.00149912, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.115071: step 3100, loss 0.000691413, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.118424: step 3100, loss 0.0119596, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.121847: step 3100, loss 4.00535e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.125031: step 3100, loss 0.000270211, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.128884: step 3100, loss 6.16104, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.132382: step 3100, loss 0.00226384, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.135568: step 3100, loss 0.00103611, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.138900: step 3100, loss 0.000167713, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.142316: step 3100, loss 0.00204533, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.145652: step 3100, loss 0.00336604, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.148873: step 3100, loss 0.000610761, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.151801: step 3100, loss 0.0218188, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.154917: step 3100, loss 0.000484349, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.158096: step 3100, loss 0.000989067, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.162523: step 3100, loss 0.004648, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.166456: step 3100, loss 0.00803772, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.169629: step 3100, loss 0.000358517, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.172754: step 3100, loss 0.0016886, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.175764: step 3100, loss 0.00253844, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.178452: step 3100, loss 0.0006384, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.182144: step 3100, loss 0.00478433, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.185845: step 3100, loss 5.94883, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.189108: step 3100, loss 0.000663775, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.192359: step 3100, loss 0.00259682, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.195778: step 3100, loss 0.00818553, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.199103: step 3100, loss 0.000278196, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.202580: step 3100, loss 0.00232533, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.205553: step 3100, loss 8.22541e-06, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.209083: step 3100, loss 0.000644, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.213910: step 3100, loss 0.000180347, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.217252: step 3100, loss 0.000626606, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.220565: step 3100, loss 0.000448365, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.223712: step 3100, loss 0.153483, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.228541: step 3100, loss 0.00158994, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.232151: step 3100, loss 9.05245, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.235589: step 3100, loss 0.00014912, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.239300: step 3100, loss 0.000161754, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.242192: step 3100, loss 0.0186022, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.246969: step 3100, loss 0.000237913, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.250033: step 3100, loss 7.8618, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.253465: step 3100, loss 0.00417172, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.257287: step 3100, loss 0.000217057, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.261998: step 3100, loss 0.000228856, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.266475: step 3100, loss 0.00339895, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.269904: step 3100, loss 0.00342271, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.273088: step 3100, loss 0.00270168, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.276422: step 3100, loss 0.00313993, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.280683: step 3100, loss 0.00284422, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.283976: step 3100, loss 0.000538919, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.287514: step 3100, loss 0.0234681, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.291097: step 3100, loss 0.00209755, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.294498: step 3100, loss 0.00453634, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.297602: step 3100, loss 0.000859725, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.300858: step 3100, loss 0.00223756, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.303994: step 3100, loss 0.00298816, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.307318: step 3100, loss 0.000256982, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.310317: step 3100, loss 0.0154494, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.313051: step 3100, loss 0.000164972, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.317445: step 3100, loss 0.00357441, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.320688: step 3100, loss 0.0207764, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.323658: step 3100, loss 0.000188929, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.326732: step 3100, loss 0.00142211, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.330173: step 3100, loss 9.28597e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.332937: step 3100, loss 0.000695821, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.336252: step 3100, loss 5.17355e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.339646: step 3100, loss 0.00308336, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.342992: step 3100, loss 0.000654483, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.346189: step 3100, loss 0.0126462, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.349626: step 3100, loss 0.00142806, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.354452: step 3100, loss 0.000196199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.357762: step 3100, loss 0.000591222, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.361530: step 3100, loss 0.000217414, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.365128: step 3100, loss 0.000994545, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.370634: step 3100, loss 0.000987995, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.373586: step 3100, loss 0.000328844, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.376770: step 3100, loss 6.06425, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.380156: step 3100, loss 0.00101979, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.384277: step 3100, loss 5.41851, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.387377: step 3100, loss 0.00154507, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.390616: step 3100, loss 0.00115042, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.394133: step 3100, loss 0.00413124, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.397322: step 3100, loss 0.00251668, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.400521: step 3100, loss 0.000418695, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.403895: step 3100, loss 7.74347, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.406819: step 3100, loss 0.00110196, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.410053: step 3100, loss 0.00313493, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.413237: step 3100, loss 0.0215759, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.416583: step 3100, loss 0.0028271, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.421042: step 3100, loss 0.000441215, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.424089: step 3100, loss 0.0126876, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.428709: step 3100, loss 6.79702, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.431590: step 3100, loss 0.000404991, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.434793: step 3100, loss 0.0022705, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.437698: step 3100, loss 0.00877701, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.441123: step 3100, loss 0.000313471, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.444090: step 3100, loss 0.00908208, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.448791: step 3100, loss 0.000215626, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.451770: step 3100, loss 0.000186784, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.454866: step 3100, loss 0.000682717, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.458296: step 3100, loss 0.000495311, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.461600: step 3100, loss 0.670379, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.464755: step 3100, loss 0.0381629, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.468338: step 3100, loss 0.00145032, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.472348: step 3100, loss 8.42098, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.475709: step 3100, loss 0.00916559, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.479222: step 3100, loss 0.000436092, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.482668: step 3100, loss 0.0273474, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.485967: step 3100, loss 0.00146508, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.489609: step 3100, loss 7.77402, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.493283: step 3100, loss 0.0510935, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.496633: step 3100, loss 0.000595988, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.499857: step 3100, loss 0.000472672, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.503670: step 3100, loss 0.00093869, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.506617: step 3100, loss 0.00504779, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.510045: step 3100, loss 0.000722267, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.513284: step 3100, loss 0.00108326, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.516264: step 3100, loss 0.00117507, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.519259: step 3100, loss 0.000233027, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.524613: step 3100, loss 0.003811, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.529325: step 3100, loss 0.00347463, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.532312: step 3100, loss 0.000128619, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.535413: step 3100, loss 0.000291186, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.538723: step 3100, loss 0.00354507, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.541943: step 3100, loss 0.0108901, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.545268: step 3100, loss 0.00012981, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.548361: step 3100, loss 0.0314857, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.551611: step 3100, loss 0.000611357, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.554688: step 3100, loss 0.000418456, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.558031: step 3100, loss 0.0163393, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.561082: step 3100, loss 0.00554725, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.564079: step 3100, loss 0.000111335, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.567059: step 3100, loss 0.0182548, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.570248: step 3100, loss 0.000364237, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.574570: step 3100, loss 0.132981, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.577753: step 3100, loss 0.00648146, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.581175: step 3100, loss 0.0852949, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.584673: step 3100, loss 0.00812712, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.588517: step 3100, loss 0.00099002, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.592137: step 3100, loss 0.0190284, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.597019: step 3100, loss 0.000141611, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.600197: step 3100, loss 0.0906182, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.603696: step 3100, loss 0.00311319, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.607309: step 3100, loss 0.000747282, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.610050: step 3100, loss 0.00820066, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.613163: step 3100, loss 7.35765, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.616232: step 3100, loss 0.0348811, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.619686: step 3100, loss 0.000154841, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.622889: step 3100, loss 0.00083245, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.628665: step 3100, loss 7.26237, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.631707: step 3100, loss 0.000118964, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.634713: step 3100, loss 0.00061088, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.637747: step 3100, loss 0.0108841, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.640970: step 3100, loss 0.0915044, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.644204: step 3100, loss 7.59334e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.646995: step 3100, loss 0.000613978, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.651441: step 3100, loss 0.00536882, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.654966: step 3100, loss 0.000650075, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.657714: step 3100, loss 0.00810749, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.662214: step 3100, loss 0.0474461, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.664958: step 3100, loss 8.02245, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.668454: step 3100, loss 8.19784, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.671687: step 3100, loss 5.53419, acc 0\n",
      "False Negative\n",
      "2016-03-08T00:28:51.675047: step 3100, loss 0.000280103, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.678897: step 3100, loss 8.4635e-05, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.682613: step 3100, loss 0.000174865, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.685648: step 3100, loss 0.00733149, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.689236: step 3100, loss 0.00939, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.692518: step 3100, loss 0.0139677, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.695655: step 3100, loss 0.000719765, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.698713: step 3100, loss 0.000346005, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.702481: step 3100, loss 0.215435, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.705498: step 3100, loss 0.000155198, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.709084: step 3100, loss 0.00143937, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.712480: step 3100, loss 0.00563746, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.716327: step 3100, loss 0.000792666, acc 1\n",
      "False Negateive\n",
      "2016-03-08T00:28:51.719679: step 3100, loss 0.000485541, acc 1\n",
      "False Negateive\n",
      "True Positives 0\n",
      "True Negatives 1752\n",
      "False Positives 0\n",
      "False Negatives 219\n",
      "Sensitivity: 0.0\n",
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "    accuracies = []\n",
    "    for y, x in zip(even_y_dev, even_x_dev):\n",
    "        sent = []\n",
    "        for word in x:\n",
    "            sent.append(vocabulary_inv[word])\n",
    "        print(' '.join(sent))\n",
    "        print(\"example\" if y[0] == 0 and y[1] == 1 else \"nonexample\")\n",
    "        dev_step([x], [y])\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "    \n",
    "    tp = 0  \n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for y, x in zip(y_dev, x_dev):\n",
    "        a= dev_step([x], [y])\n",
    "        expected = \"example\" if y[0] == 0 and y[1] == 1  else \"nonexample\"\n",
    "        actual = None\n",
    "        if(y[0] == 0 and y[1] == 1):\n",
    "            # correct label is example\n",
    "            if(a == 1.0):\n",
    "                actual = \"example\"\n",
    "            else:\n",
    "                actual = \"nonexample\"\n",
    "        elif(y[0] == 1 and y[1] == 0):\n",
    "            if(a == 1.0):\n",
    "                actual = \"nonexample\"\n",
    "            else:\n",
    "                actual = \"example\"\n",
    "            \n",
    "            \n",
    "        if(expected == \"example\" and actual == \"example\"):\n",
    "            tp += 1\n",
    "            print(\"True Positive\")\n",
    "        elif(expected == \"example\" and actual == \"nonexample\"):\n",
    "            fn += 1\n",
    "            print(\"False Negative\")\n",
    "        elif(expected == \"nonexample\" and actual ==\"exaple\"):\n",
    "            fp += 1\n",
    "            print(\"False Positive\")\n",
    "        elif(expected == \"nonexample\" and actual == \"nonexample\"):\n",
    "            tn +=1 \n",
    "            print(\"False Negateive\")\n",
    "            \n",
    "            \n",
    "    print(\"True Positives %s\" % tp)\n",
    "    print(\"True Negatives %s\" % tn)\n",
    "    print(\"False Positives %s\" % fp)\n",
    "    print(\"False Negatives %s\" % fn)\n",
    "    sensitivity = (tp/(tp+float(fn)))\n",
    "    print(\"Sensitivity: %s\" % sensitivity)\n",
    "    specificity = (tn/(tn+float(fp)))\n",
    "    print(\"Specificity: %s\" % specificity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
