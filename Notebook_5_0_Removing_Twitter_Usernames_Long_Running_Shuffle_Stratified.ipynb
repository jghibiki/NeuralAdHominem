{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: CNN\n",
    "\n",
    "## Overview: \n",
    "\n",
    "1. Begin by importing and getting the embeddings and word to index mappings we created in [Notebook 1: Embed Words](Notebook_1_Embed_Words.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "from tensorflow.models.rnn.rnn_cell import BasicLSTMCell, LSTMCell \n",
    "import itertools\n",
    "from collections import Counter\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embeddings = None\n",
    "mappings = None\n",
    "rows = None\n",
    "\n",
    "with open(\"word_embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "with open(\"word_mappings.pkl\", \"rb\") as f:\n",
    "    mappings = pickle.load(f)\n",
    "    \n",
    "\n",
    "urlFinder = re.compile('\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*')\n",
    "atNameFinder = re.compile(r'@([A-Za-z0-9_]+)')\n",
    "atNameCounter = 0\n",
    "\n",
    "exclude_punc = set([\n",
    "        \"!\",\n",
    "        \"?\",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \":\",\n",
    "        \";\",\n",
    "        \"'\",\n",
    "        \"\\\"\",\n",
    "        \"“\",\n",
    "        \"’\",\n",
    "        \"-\"\n",
    "])\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "x = []\n",
    "y = []\n",
    "_y = []\n",
    "\n",
    "with open('data.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, delimiter=',')   \n",
    "    \n",
    "    for row in reader:\n",
    "        words = []\n",
    "        \n",
    "        for word in row[1] \\\n",
    "            .strip() \\\n",
    "            .replace(\"&amp;\", \"\") \\\n",
    "            .replace(\"&gt;\",\"\") \\\n",
    "            .replace(\"&lt;\", \"\") \\\n",
    "            .lower().split():\n",
    "            \n",
    "            if urlFinder.match(word):\n",
    "                words.append(\"<URL/>\")\n",
    "            elif atNameFinder.search(word):\n",
    "                words.append(\"<AT_NAME_%s/>\" % atNameCounter)\n",
    "                atNameCounter +=1\n",
    "            else:\n",
    "                word = ''.join(ch for ch in word if ch not in exclude_punc)\n",
    "                words.append(word)\n",
    "        sentences.append(words)\n",
    "        labels.append(([0, 1] if row[0] == \"example\" else [1, 0]))\n",
    "        _y.append(1 if row[0] == \"example\" else 0)\n",
    "\n",
    "\n",
    "sequence_length = max(len(i) for i in sentences)\n",
    "padded_sentences = []\n",
    "for i in range(len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    num_padding = sequence_length - len(sentence)\n",
    "    new_sentence = sentence + [\"<PAD/>\"] * num_padding\n",
    "    padded_sentences.append(new_sentence)\n",
    "    \n",
    " \n",
    "word_counts = Counter(itertools.chain(*padded_sentences))\n",
    "\n",
    "# Mapping from index to word\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "# Mapping from word to index\n",
    "vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "\n",
    "x = np.array([[vocabulary[word] for word in sentence] for sentence in padded_sentences])\n",
    "y = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_data = data[shuffle_indices]\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Writing to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:20:38.667434: step 100, loss 0.747434, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:20:57.030623: step 200, loss 0.770087, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:21:15.170552: step 300, loss 0.758405, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:21:33.529428: step 400, loss 0.719626, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:21:51.696601: step 500, loss 0.725991, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:22:09.994135: step 600, loss 0.732688, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:22:28.376142: step 700, loss 0.691609, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:22:46.696749: step 800, loss 0.699547, acc 0.888664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:23:04.916783: step 900, loss 0.717417, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:23:23.229158: step 1000, loss 0.68473, acc 0.888664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-1000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:23:41.402564: step 1100, loss 0.647892, acc 0.888664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-1100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:23:59.613445: step 1200, loss 0.718062, acc 0.888664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-1200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:24:17.751747: step 1300, loss 0.725871, acc 0.888664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-1300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:24:36.074330: step 1400, loss 0.684208, acc 0.888664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-1400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:24:54.301739: step 1500, loss 0.655662, acc 0.887652\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-1500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:25:12.459533: step 1600, loss 0.703609, acc 0.888158\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-1600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:25:30.547715: step 1700, loss 0.685635, acc 0.887652\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-1700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:25:48.946456: step 1800, loss 0.725528, acc 0.887652\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-1800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:26:07.083005: step 1900, loss 0.730299, acc 0.887652\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-1900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:26:25.201021: step 2000, loss 0.687984, acc 0.887652\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-2000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:26:43.305458: step 2100, loss 0.685356, acc 0.887652\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-2100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:27:01.429294: step 2200, loss 0.739085, acc 0.887652\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-2200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:27:19.649777: step 2300, loss 0.741209, acc 0.887652\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-2300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:27:37.680826: step 2400, loss 0.725816, acc 0.887146\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-2400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:27:55.788270: step 2500, loss 0.76749, acc 0.887146\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-2500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:28:13.897848: step 2600, loss 0.751806, acc 0.887652\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-2600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:28:32.142186: step 2700, loss 0.755224, acc 0.887652\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-2700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:28:50.260469: step 2800, loss 0.765818, acc 0.887652\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-2800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:29:08.557507: step 2900, loss 0.773022, acc 0.887652\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-2900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:29:26.684120: step 3000, loss 0.735352, acc 0.886134\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-3000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:29:44.738620: step 3100, loss 0.770435, acc 0.887146\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-3100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:30:02.894302: step 3200, loss 0.780882, acc 0.887146\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-3200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:30:21.204657: step 3300, loss 0.762174, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-3300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:30:39.353943: step 3400, loss 0.786415, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-3400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:30:57.527462: step 3500, loss 0.76124, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-3500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:31:15.600393: step 3600, loss 0.817939, acc 0.887146\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-3600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:31:33.662125: step 3700, loss 0.783681, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-3700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:31:51.652426: step 3800, loss 0.802239, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-3800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:32:09.888544: step 3900, loss 0.812921, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-3900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:32:27.949359: step 4000, loss 0.799832, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-4000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:32:46.002709: step 4100, loss 0.808938, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-4100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:33:04.029137: step 4200, loss 0.81373, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-4200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:33:22.240834: step 4300, loss 0.845505, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-4300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:33:40.205104: step 4400, loss 0.835666, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-4400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:33:58.317164: step 4500, loss 0.828516, acc 0.886134\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-4500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:34:16.441241: step 4600, loss 0.824585, acc 0.886134\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-4600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:34:34.676245: step 4700, loss 0.836736, acc 0.886134\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-4700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:34:52.799823: step 4800, loss 0.843403, acc 0.886134\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-4800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:35:10.818366: step 4900, loss 0.864913, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-4900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:35:29.139663: step 5000, loss 0.851664, acc 0.886134\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-5000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:35:47.262476: step 5100, loss 0.827943, acc 0.885628\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-5100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:36:05.336034: step 5200, loss 0.862897, acc 0.886134\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-5200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:36:23.344247: step 5300, loss 0.854372, acc 0.886134\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-5300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:36:41.519763: step 5400, loss 0.844664, acc 0.885121\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-5400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:36:59.653563: step 5500, loss 0.902527, acc 0.886134\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-5500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:37:17.718052: step 5600, loss 0.851072, acc 0.886134\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-5600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:37:35.614612: step 5700, loss 0.866917, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-5700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:37:53.754245: step 5800, loss 0.844738, acc 0.885628\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-5800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:38:11.902927: step 5900, loss 0.915834, acc 0.886134\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-5900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:38:30.114394: step 6000, loss 0.890356, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-6000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:38:48.184816: step 6100, loss 0.878616, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-6100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-07T19:39:06.334799: step 6200, loss 0.93679, acc 0.88664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457378419/checkpoints/model-6200\n",
      "\n",
      "\n",
      "Final Evaluations:\n",
      "2016-03-07T19:39:07.603720: step 6200, loss 0.93679, acc 0.88664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 128  #Dimensionality of character embedding (default: 128)\n",
    "filter_sizes =  \"3,4,5\" #\"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "num_filters = 128  #\"Number of filters per filter size (default: 128)\")\n",
    "dropout_keep_prob = 0.5 #\"Dropout keep probability (default: 0.5)\")\n",
    "l2_reg_lambda = 0.0 #\"L2 regularizaion lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64 # \"Batch Size (default: 64)\")\n",
    "num_epochs = 200 #\"Number of training epochs (default: 200)\")\n",
    "evaluate_every = 100  #\"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "checkpoint_every = 100 # \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "allow_soft_placement = True # \"Allow device soft device placement\")\n",
    "log_device_placement = False  #\"Log placement of ops on devices\")\n",
    "display_train_steps = False # toggles output of training step results\n",
    "\n",
    "\n",
    "\n",
    "# Data Preparatopn\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "# Randomly shuffle data\n",
    "sss = StratifiedShuffleSplit(_y, 1, test_size=0.5, random_state=0)\n",
    "for train, test in sss:\n",
    "    x_train = np.random.permutation(x[train])\n",
    "    y_train = np.random.permutation(y[train])\n",
    "\n",
    "    x_dev = np.random.permutation(x[test])\n",
    "    y_dev = np.random.permutation(y[test])\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=len(vocabulary),\n",
    "            embedding_size=embedding_dim,\n",
    "            filter_sizes=map(int, filter_sizes.split(\",\")),\n",
    "            num_filters=num_filters,\n",
    "            l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph_def)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if(display_train_steps):\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return accuracy\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            zip(x_train, y_train), batch_size, num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "        \n",
    "        print(\"\\nFinal Evaluations:\")\n",
    "        dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "  \n",
    "        \n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438, 438)\n",
      "Balanced Evaluation:\n",
      "2016-03-07T19:39:07.800409: step 6200, loss 4.15957, acc 0.502283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5022831"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        pos_x_dev = []\n",
    "        pos_y_dev = []\n",
    "\n",
    "        neg_x_dev = []\n",
    "        neg_y_dev = []\n",
    "\n",
    "        for y, x in zip(y_dev, x_dev):\n",
    "            if(y[0] == 0 and y[1] == 1):\n",
    "                pos_x_dev.append(x)\n",
    "                pos_y_dev.append(y)\n",
    "            else:\n",
    "                neg_x_dev.append(x)\n",
    "                neg_y_dev.append(y)\n",
    "        even_x_dev = np.array(pos_x_dev + neg_x_dev[:len(pos_x_dev)])\n",
    "        even_y_dev = np.array(pos_y_dev + neg_y_dev[:len(pos_y_dev)])\n",
    "        print(len(even_y_dev), len(even_x_dev))\n",
    "\n",
    "        print(\"Balanced Evaluation:\")\n",
    "        dev_step(even_x_dev, even_y_dev, writer=dev_summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last night we welcomed <AT_NAME_353/> to our growing team hear from bobby himself why he is supporting our campaign <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.863223: step 6200, loss 13.5571, acc 0\n",
      "\n",
      "\n",
      "at a time when senior poverty is increasing we must expand social security benefits not cut them <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.866618: step 6200, loss 8.75266, acc 0\n",
      "\n",
      "\n",
      "questions or issues call our hotline #americatogether <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.871634: step 6200, loss 7.69328, acc 0\n",
      "\n",
      "\n",
      "its unacceptable that billionaire families can leave virtually all of their wealth to their families without paying a reasonable fair tax <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.875437: step 6200, loss 10.5568, acc 0\n",
      "\n",
      "\n",
      "why would the people of florida vote for marco rubio when he defrauded them by agreeing to represent them as their senator and then quit <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.879222: step 6200, loss 9.97718, acc 0\n",
      "\n",
      "\n",
      "bring your family and friends with you when you caucus tonight in iowa click here to find your caucus location <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.883524: step 6200, loss 10.8651, acc 0\n",
      "\n",
      "\n",
      "kids being sent into the criminal justice system instead of school is a civil rights issue and it needs to end <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.887274: step 6200, loss 10.2109, acc 0\n",
      "\n",
      "\n",
      "support a proven conservative for president #caucusforcruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.890607: step 6200, loss 8.66618, acc 0\n",
      "\n",
      "\n",
      "only a few hours left nh i am asking for your vote find your polling location here <URL/> #fitn #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.894018: step 6200, loss 7.09154, acc 0\n",
      "\n",
      "\n",
      "on this #holocaustremembrance day let us honor the resilience and courage of the jewish people <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.899119: step 6200, loss 9.77685, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_207/> looks more like a gym rat than a us senator how the hell did he ever get elected <AT_NAME_208/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.902667: step 6200, loss 8.55906, acc 0\n",
      "\n",
      "\n",
      "incredibly grateful to all of our volunteers and staff that did amazing work today and in the months leading up to the #nvcaucus thank you <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.906052: step 6200, loss 10.7951, acc 0\n",
      "\n",
      "\n",
      "good morning south carolina excited to get going will be on <AT_NAME_475/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.909365: step 6200, loss 7.403, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2563/> hes a nutcase no wonder he likes you <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.913737: step 6200, loss 7.49749, acc 0\n",
      "\n",
      "\n",
      "while <AT_NAME_599/> was building a reality tv show my brother was building a security apparatus to keep us safe #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.917346: step 6200, loss 2.69339, acc 0\n",
      "\n",
      "\n",
      "you know before it was called obamacare it was called hillarycare #demdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.920576: step 6200, loss 3.84603, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1668/> global value chains revolution and trade and prosperity age throughout macroeconomic and geopolitic instability their goal is to <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.924092: step 6200, loss 8.61643, acc 0\n",
      "\n",
      "\n",
      "nobody who is actually prolife could stand on a national stage and say planned parenthood does wonderful things #goptownhall <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.927960: step 6200, loss 10.3276, acc 0\n",
      "\n",
      "\n",
      "together we can send a message that will echo from wall street to washington from maine to california go caucus #nvforbernie <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.931937: step 6200, loss 8.71742, acc 0\n",
      "\n",
      "\n",
      "to raise incomes we need to support unions and a living wage i stand with working minnesotans today <URL/> h <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.935768: step 6200, loss 7.84717, acc 0\n",
      "\n",
      "\n",
      "from fixing the criminal justice system to creating jobs clintons ambitions match our own <AT_NAME_507/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.939456: step 6200, loss 8.73326, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2530/> talks tough when its immigrants and isis but cowardly when it comes to facing <AT_NAME_2531/> as a moderator #coward <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.943070: step 6200, loss 7.99022, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1951/> how could you get 8%  ugh fail <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.946877: step 6200, loss 9.38872, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1639/> youre not trying to jew us over are you bernie <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.950321: step 6200, loss 6.97135, acc 0\n",
      "\n",
      "\n",
      "this country owes senator harry reid a debt of gratitude #demtownhall <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.953536: step 6200, loss 8.65365, acc 0\n",
      "\n",
      "\n",
      "my experience in iowa was a great one i started out with all of the experts saying i couldnt do well there and ended up in 2nd place nice <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.956928: step 6200, loss 8.79612, acc 0\n",
      "\n",
      "\n",
      "honored to have gold star mother natalie healy and vfw state commander lew chipola on the team <URL/> #fitn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.960829: step 6200, loss 9.6077, acc 0\n",
      "\n",
      "\n",
      "thank you <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.965131: step 6200, loss 5.75146, acc 0\n",
      "\n",
      "\n",
      "wondering if youre eligible to caucus in nevada help us win nevada #americatogether <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.968515: step 6200, loss 9.52627, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1398/> die <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.972895: step 6200, loss 3.02599, acc 0\n",
      "\n",
      "\n",
      "things got exciting when chelsea got her first hit at softball hillary tells the story <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.976111: step 6200, loss 9.10314, acc 0\n",
      "\n",
      "\n",
      "americans dont care about who is establishment or who is grassroots #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.979383: step 6200, loss 0.882846, acc 0\n",
      "\n",
      "\n",
      "new video <AT_NAME_443/> and i served as governors but when you match our records there’s no comparison <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.982818: step 6200, loss 9.63927, acc 0\n",
      "\n",
      "\n",
      "8 i have news for republicans who would put politics over the constitution refusing to do your duty isn’t righteous its disgraceful h <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.986152: step 6200, loss 12.0021, acc 0\n",
      "\n",
      "\n",
      "i am in virginia <AT_NAME_924/> presidential forum with dr pat robertson beginning now watch here <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.990482: step 6200, loss 7.08053, acc 0\n",
      "\n",
      "\n",
      "#trump2016 #iacaucus finder <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.993752: step 6200, loss 3.57506, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1222/> the aca act is a law that provides affordable healthcare for all even with pre existing conditions embrace the law <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:07.997143: step 6200, loss 14.4332, acc 0\n",
      "\n",
      "\n",
      "every year the federal government makes billions of dollars in profits off of student loans we must end this most revolting practice <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.001034: step 6200, loss 11.5751, acc 0\n",
      "\n",
      "\n",
      "donald trump thinks planned parenthood is wonderful i think we ought to investigate and prosecute planned parenthood #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.004899: step 6200, loss 11.6112, acc 0\n",
      "\n",
      "\n",
      "new video once again i was the only candidate on the #gopdebate stage to take on <AT_NAME_615/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.009089: step 6200, loss 9.28011, acc 0\n",
      "\n",
      "\n",
      "today we remember and honor the legacy of the #challenger crew members who gave their lives for our nation <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.012591: step 6200, loss 6.97567, acc 0\n",
      "\n",
      "\n",
      "the president doesnt get to choose what issues come to her desk navigating foreign policy challenges is part of the job #demdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.017902: step 6200, loss 10.1898, acc 0\n",
      "\n",
      "\n",
      "ben carson picks up endorsement reveals plan to care for veterans  breitbart <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.022134: step 6200, loss 5.18189, acc 0\n",
      "\n",
      "\n",
      "hillary and <AT_NAME_465/> share a lot of big progressive goals—but they differ in how they plan to achieve them <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.025727: step 6200, loss 11.3733, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1410/> <AT_NAME_1411/> considering the polls seems to me the commercial money could be better spent like going after the leaders in the polls <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.028882: step 6200, loss 10.7713, acc 0\n",
      "\n",
      "\n",
      "it’s official the fbi confirms it is investigating hillary clinton’s private server <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.032336: step 6200, loss 7.78509, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2181/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.036087: step 6200, loss 3.73272, acc 0\n",
      "\n",
      "\n",
      "team jeb taking over follow along with us during the #gopdebate here → <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.039719: step 6200, loss 6.38422, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1789/> <AT_NAME_1790/> <AT_NAME_1791/> <AT_NAME_1792/> <AT_NAME_1793/> <AT_NAME_1794/> islam 9 yr old girl is legal <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.043550: step 6200, loss 9.33701, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2501/> but i thought you were satan <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.047486: step 6200, loss 3.16602, acc 0\n",
      "\n",
      "\n",
      "check out our new tv ad where rep <AT_NAME_688/> explains why hes supporting our campaign #scprimary <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.051720: step 6200, loss 10.3917, acc 0\n",
      "\n",
      "\n",
      "a quote was read from a parody account last night on msnbc re jeb <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.055506: step 6200, loss 10.3006, acc 0\n",
      "\n",
      "\n",
      "the us is less safe from <AT_NAME_504/> weak leadingfrombehind foreign policy i believe in peace through strength <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.059090: step 6200, loss 8.12772, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2411/> donald its time to quit the attacks on your competitors and start attacking hillary clinton we must defeat the democrats <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.063001: step 6200, loss 7.26693, acc 0\n",
      "\n",
      "\n",
      "thank you america #trump2016 <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.066915: step 6200, loss 11.4693, acc 0\n",
      "\n",
      "\n",
      "i would consult with top military minds to create proactive ways to seek out and destroy #isis read my plan here <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.071862: step 6200, loss 7.9805, acc 0\n",
      "\n",
      "\n",
      "as president my top domestic priority will be to stop illegal immigration we must secure our border to ensure the safety of our people <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.075276: step 6200, loss 12.6878, acc 0\n",
      "\n",
      "\n",
      "i will never hesitate to defend this nation but i will never send our sons and daughters to war under false pretense <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.078625: step 6200, loss 9.11737, acc 0\n",
      "\n",
      "\n",
      "just a reminder <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.083567: step 6200, loss 4.87348, acc 0\n",
      "\n",
      "\n",
      "south carolina im asking for your vote #scprimary <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.087481: step 6200, loss 9.53842, acc 0\n",
      "\n",
      "\n",
      "lets debate before the #iacaucus <AT_NAME_29/> <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.091189: step 6200, loss 7.58692, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1209/> <AT_NAME_1210/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.094103: step 6200, loss 2.8583, acc 0\n",
      "\n",
      "\n",
      "if this team wins the white house virginia is invited to come by and dance any time <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.098094: step 6200, loss 7.65212, acc 0\n",
      "\n",
      "\n",
      "a few moments with <AT_NAME_259/> behind the scenes in iowa <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.101040: step 6200, loss 6.74519, acc 0\n",
      "\n",
      "\n",
      "it makes no sense to me that the united states of america has more jails and prisons than colleges and universities <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.104598: step 6200, loss 10.8704, acc 0\n",
      "\n",
      "\n",
      "“jeb will be a strong and steady hand when confronted with the unexpected”  george w bush full speech here → <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.107905: step 6200, loss 10.9191, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1879/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.111268: step 6200, loss 3.24983, acc 0\n",
      "\n",
      "\n",
      "the responsibility of commander in chief deserves a serious sober and responsible person <URL/> #fitn #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.114820: step 6200, loss 8.216, acc 0\n",
      "\n",
      "\n",
      "and now weve crossed $150k thanks to chris of iowa he donated and won a signed football <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.118183: step 6200, loss 7.37622, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1433/> shatter it to the core <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.122697: step 6200, loss 6.20218, acc 0\n",
      "\n",
      "\n",
      "i will promote peace and prosperity through the power of we the people and restore america’s role as the world’s preeminent global leader <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.126390: step 6200, loss 8.81895, acc 0\n",
      "\n",
      "\n",
      "if you agree we should stop the washington bull donate $5 today <URL/> #gopdebate #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.129687: step 6200, loss 12.0998, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1666/> thats good with what will you replace it with <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.133056: step 6200, loss 4.61209, acc 0\n",
      "\n",
      "\n",
      "executives at exxon pioneered the research on climate change before anyone else did and lied about it to protect their bottom line <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.136251: step 6200, loss 7.36214, acc 0\n",
      "\n",
      "\n",
      "hbcus provide more than a quality education—they increase opportunity supporting them is critical <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.140207: step 6200, loss 2.80129, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1823/> thanks for shedding light on the plight of our 50000 homeless veterans on any given night it is a shame #wearebernie <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.143964: step 6200, loss 11.1798, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1437/> – the candidate for main street not wall street #feelthebern #bernie2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.147712: step 6200, loss 7.07396, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1248/> <AT_NAME_1249/> <AT_NAME_1250/> he is just trying to get out of debate because he knows he is not a good debater <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.151954: step 6200, loss 9.32136, acc 0\n",
      "\n",
      "\n",
      "jeanette i mourn the loss of justice scalia and our thoughts prayers are with his wife maureen his family <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.155789: step 6200, loss 10.4147, acc 0\n",
      "\n",
      "\n",
      "hillary is handing over her snapchat account to <AT_NAME_171/> today don’t miss it <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.159854: step 6200, loss 4.74737, acc 0\n",
      "\n",
      "\n",
      "i believe in the teddy roosevelt philosophy #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.163745: step 6200, loss 7.49767, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2591/>  go trump  we love u  <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.167220: step 6200, loss 8.27769, acc 0\n",
      "\n",
      "\n",
      "we must declare dreamers eligible for instate tuition if they meet state residency requirements <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.170726: step 6200, loss 13.2056, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_218/> <AT_NAME_219/> <AT_NAME_220/> damn fox is really acting like a baby today i guess trump got under their skin  pretty biased <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.175253: step 6200, loss 9.13494, acc 0\n",
      "\n",
      "\n",
      "nevada today we caucus get to your caucus location by 11am if there is a large voter turnout we can win join history in the making <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.178638: step 6200, loss 8.00861, acc 0\n",
      "\n",
      "\n",
      "i don’t believe it is a terribly radical idea to say that someone who works 40 hours a week should not be living in poverty <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.182266: step 6200, loss 11.1879, acc 0\n",
      "\n",
      "\n",
      "never let them see you sweat <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.185509: step 6200, loss 8.27445, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1763/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.189000: step 6200, loss 1.64264, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2269/> in my opinion donald trump lives a life of loving and helping others as jesus taught in the great commandment falwell said <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.193753: step 6200, loss 7.97562, acc 0\n",
      "\n",
      "\n",
      "people of color are incarcerated policed and sentenced to death at significantly higher rates than their white counterparts <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.197375: step 6200, loss 7.33464, acc 0\n",
      "\n",
      "\n",
      "it is not a radical idea to state that no worker in this country working 40 hours a week should live in poverty #caucusforbernie <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.200888: step 6200, loss 10.0052, acc 0\n",
      "\n",
      "\n",
      "as president i will repair our broken immigration system by securing the border and restoring the concept of the american melting pot <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.205679: step 6200, loss 9.1308, acc 0\n",
      "\n",
      "\n",
      "sad to hear another #borinqueneer francisco torregrosa died before receiving congressional gold medal <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.208794: step 6200, loss 6.45612, acc 0\n",
      "\n",
      "\n",
      "our service men and women deserve the greatest care available the current state of the va is disgraceful and we must do better <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.212048: step 6200, loss 9.87455, acc 0\n",
      "\n",
      "\n",
      "before social security was signed into law nearly half of our senior citizens lived in poverty <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.215465: step 6200, loss 7.25473, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1996/> dishonest <AT_NAME_1997/> wont show their own poll for iowa now that they found one showing cruz doing better still losing <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.218840: step 6200, loss 10.405, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2508/> <AT_NAME_2509/> <AT_NAME_2510/> vote trump america get to know trump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.222282: step 6200, loss 10.9166, acc 0\n",
      "\n",
      "\n",
      "the new hampshire primary is today if youre with hillary let her know now <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.226467: step 6200, loss 12.1143, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1625/> says common law abroad supreme to constitutional and positive law of america accordingly nativeborn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.229627: step 6200, loss 10.6671, acc 0\n",
      "\n",
      "\n",
      "the time to get out and caucus is approaching #iacaucus #caucusfortrump #icaucsed #ivoted <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.233128: step 6200, loss 8.22798, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1083/> #thetruthisoutthere #disclosure #therockefellerinitiative <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.236454: step 6200, loss 6.13863, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1198/> 9yr old girl cries tears of joy when parents tell her she gets to see donald trump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.240159: step 6200, loss 9.4904, acc 0\n",
      "\n",
      "\n",
      "$15 and a union  a phrase that wont be heard in tonight’s debate that’s why we organize <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.243674: step 6200, loss 11.2381, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2301/> that is seriously my favorite bible verse <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.246836: step 6200, loss 4.68961, acc 0\n",
      "\n",
      "\n",
      "this election is about trust #caucusforcruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.249999: step 6200, loss 10.5596, acc 0\n",
      "\n",
      "\n",
      "jebs response to <AT_NAME_138/> summed up in one picture #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.253565: step 6200, loss 6.92528, acc 0\n",
      "\n",
      "\n",
      "these are just a few of the women leading our political revolution in new hampshire #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.257311: step 6200, loss 9.50079, acc 0\n",
      "\n",
      "\n",
      "closing statement on tonights #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.260714: step 6200, loss 10.7994, acc 0\n",
      "\n",
      "\n",
      "most polls close at 7pm but if you live in any of these places your polls are open until 8 get out vote #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.264755: step 6200, loss 9.33149, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2375/> what do you want me to do go down there with a mop u cannot handle nj how can you handle the usa #dropoutoftheracestupid <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.268807: step 6200, loss 6.69632, acc 0\n",
      "\n",
      "\n",
      "amazing that while i lead by big numbers in the new q and and usa today polls the the press only wants to report on the phony wsj/nbc poll <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.272731: step 6200, loss 7.12068, acc 0\n",
      "\n",
      "\n",
      "glad to have <AT_NAME_845/> support in this campaign welcome to the team <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.277145: step 6200, loss 10.0741, acc 0\n",
      "\n",
      "\n",
      "three major profitable corporations not only pay nothing in federal income taxes they actually got a rebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.282312: step 6200, loss 9.11346, acc 0\n",
      "\n",
      "\n",
      "icymi 5 qualities we need in our next president from the man who knows best → <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.285870: step 6200, loss 7.89991, acc 0\n",
      "\n",
      "\n",
      "this campaign is about bringing people together we understand that when we stand together there is nothing we cant accomplish <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.289807: step 6200, loss 12.4292, acc 0\n",
      "\n",
      "\n",
      "the united states can’t continue to withdraw from the threat of isis we have one choice to defeat it <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.293739: step 6200, loss 11.6583, acc 0\n",
      "\n",
      "\n",
      "will be interviewed by <AT_NAME_698/> tonight by phone a late show first <AT_NAME_699/> <AT_NAME_700/> enjoy #colbert #lssc <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.297523: step 6200, loss 10.2358, acc 0\n",
      "\n",
      "\n",
      "one of five people in this country that get a prescription from a doctor cannot afford to fill that prescription that’s wrong #bernieinmn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.301244: step 6200, loss 11.2336, acc 0\n",
      "\n",
      "\n",
      "todays the day the new hampshire primary is here rt to say youre with us #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.305014: step 6200, loss 10.1757, acc 0\n",
      "\n",
      "\n",
      "we need to crack down on wall street risk hillarys plan will actually do it <URL/> #demdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.308427: step 6200, loss 6.41251, acc 0\n",
      "\n",
      "\n",
      "#cruztovictory in new hampshire <URL/> #fitn #nhpolitics <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.311912: step 6200, loss 8.54083, acc 0\n",
      "\n",
      "\n",
      "its not tough to take an elderly womans home <AT_NAME_391/> #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.315784: step 6200, loss 6.4588, acc 0\n",
      "\n",
      "\n",
      "#cruzcrew tune in to the #gopdebate at 9 pm et on <AT_NAME_117/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.319412: step 6200, loss 7.888, acc 0\n",
      "\n",
      "\n",
      "today the eyes of the entire country are on new hampshire bring your friends and go vote in the #nhprimary <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.322967: step 6200, loss 8.19549, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2341/> voting for hilary is like allowing an accused pedophile to take a kindergarten teaching position #nowayhilary <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.326403: step 6200, loss 8.66048, acc 0\n",
      "\n",
      "\n",
      "was honored to give the opening prayer and some remarks at <AT_NAME_562/> last night in greenville sc <URL/> #scprimary <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.331234: step 6200, loss 11.2941, acc 0\n",
      "\n",
      "\n",
      "i want to end the sad reality of having more people in jail than any other country id rather we have the best educated population <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.334839: step 6200, loss 7.20785, acc 0\n",
      "\n",
      "\n",
      "join rapper and activist <AT_NAME_984/> at our national day of action phone bank happening right now in orangeburg <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.340292: step 6200, loss 9.21248, acc 0\n",
      "\n",
      "\n",
      "voting rights marriage equality campaign finance—all are at stake in the nomination of our next scotus justice <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.345089: step 6200, loss 7.94067, acc 0\n",
      "\n",
      "\n",
      "meeting and greeting caucus goers in linn county #caucusforcruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.348428: step 6200, loss 8.88865, acc 0\n",
      "\n",
      "\n",
      "i said this was happening long ago i will stop this immediately <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.352254: step 6200, loss 7.34403, acc 0\n",
      "\n",
      "\n",
      "friends dont let friends vote for con artists <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.356346: step 6200, loss 8.03559, acc 0\n",
      "\n",
      "\n",
      "we the people are not meant to be subservient to an overbearing bureaucratic government the american people deserve much better <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.360285: step 6200, loss 12.5379, acc 0\n",
      "\n",
      "\n",
      "you should never have to choose between your family your paycheck proud of all we accomplished with #fmla23—next paid leave for all h <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.365335: step 6200, loss 14.7363, acc 0\n",
      "\n",
      "\n",
      "polls are open in #fitn new hampshire primary i will be a president that will keep the promises that i have made #bc2dc16 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.368850: step 6200, loss 10.8842, acc 0\n",
      "\n",
      "\n",
      "wow was ted cruz disloyal to his very capable director of communication he used him as a scape goatfired like a dog ted panicked <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.372479: step 6200, loss 10.0157, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1122/> <AT_NAME_1123/> <AT_NAME_1124/> <AT_NAME_1125/> yep <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.375987: step 6200, loss 4.48826, acc 0\n",
      "\n",
      "\n",
      "watch and share our new tv ad and then chip in here to help us defeat hillary <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.379384: step 6200, loss 7.77428, acc 0\n",
      "\n",
      "\n",
      "sadly in the greatest democracy in the world the federal government no longer serves america it serves itself <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.384044: step 6200, loss 8.47286, acc 0\n",
      "\n",
      "\n",
      "#cruzcrew dont miss my conversation with <AT_NAME_450/> on <AT_NAME_451/> tonight at 10 pm et <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.387734: step 6200, loss 5.91662, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2294/> bethany blankley common core ties to libya qatar saudi arabia <URL/> via <AT_NAME_2295/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.391051: step 6200, loss 10.9128, acc 0\n",
      "\n",
      "\n",
      "thank you #cruzcrew <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.394985: step 6200, loss 5.49793, acc 0\n",
      "\n",
      "\n",
      "fmr pres of mexico vicente fox horribly used the f word when discussing the wall he must apologize if i did that there would be a uproar <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.398327: step 6200, loss 9.58183, acc 0\n",
      "\n",
      "\n",
      "watch live from manchester nh <URL/> #fitn #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.401916: step 6200, loss 0.228572, acc 1\n",
      "\n",
      "\n",
      "<URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.405614: step 6200, loss 2.09903, acc 0\n",
      "\n",
      "\n",
      "scenes from the homestretch in the granite state <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.409225: step 6200, loss 5.91918, acc 0\n",
      "\n",
      "\n",
      "welcome to the team denver <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.412400: step 6200, loss 7.33864, acc 0\n",
      "\n",
      "\n",
      "there is only one person who is ready to be commander in chief on stage at the #demdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.415972: step 6200, loss 7.00508, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1872/> call ted rafael all the way to caucus his real name <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.420287: step 6200, loss 8.25363, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1168/> everyone is running scared you so threaten their little empire go get em donald #jobs#immigration#truth <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.424404: step 6200, loss 10.7871, acc 0\n",
      "\n",
      "\n",
      "south carolina the polls are now open i hope i have earned your support bring a photo id a friend and go vote <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.427711: step 6200, loss 7.30695, acc 0\n",
      "\n",
      "\n",
      "get ready to cast your ballot on #supertuesday commit to #choosecruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.431229: step 6200, loss 13.9377, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2317/> here is why you should be in jersey nowthey pay you kapiche <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.435852: step 6200, loss 5.6976, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1371/> 💪 👍 👌 👊 👏 go trump 2016 ‼️ 🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.439083: step 6200, loss 9.45922, acc 0\n",
      "\n",
      "\n",
      "i am thrilled to earn the support of congressman <AT_NAME_709/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.443864: step 6200, loss 11.3904, acc 0\n",
      "\n",
      "\n",
      "looking forward to being on #kellyfile with <AT_NAME_768/> tonight at 9 pm (est) looking forward to presenting real solutions tune in <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.447935: step 6200, loss 9.32248, acc 0\n",
      "\n",
      "\n",
      "tonight 830 pm tune in to cnn for the #gopdebate <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.451536: step 6200, loss 6.21952, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2553/> the loser of losers #trump <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.455396: step 6200, loss 4.40574, acc 0\n",
      "\n",
      "\n",
      "the reality is there is one major country on earth that does not guarantee health care to all people the united states #demdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.458932: step 6200, loss 8.48661, acc 0\n",
      "\n",
      "\n",
      "this is a campaign to break down every barrier that holds us back if we work together anything is possible <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.462539: step 6200, loss 10.3928, acc 0\n",
      "\n",
      "\n",
      "three times <AT_NAME_145/> refused to answer the question #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.466127: step 6200, loss 10.8326, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1262/> have you thought about ben carson as your vp you should <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.469876: step 6200, loss 10.8023, acc 0\n",
      "\n",
      "\n",
      "#votetrumpnh #nhprimary #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.473383: step 6200, loss 5.78928, acc 0\n",
      "\n",
      "\n",
      "the likelihood is that wall street gets away with a lot more illegal behavior than we know of #fitn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.477205: step 6200, loss 9.83317, acc 0\n",
      "\n",
      "\n",
      "listen to why charlie is supporting our campaign #fits #choosecruz join him <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.480686: step 6200, loss 8.74006, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1952/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.484146: step 6200, loss 2.9784, acc 0\n",
      "\n",
      "\n",
      "5 days to iowa <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.489115: step 6200, loss 6.81752, acc 0\n",
      "\n",
      "\n",
      "tune in to bern your enthusiasm right now on <AT_NAME_402/> #bernieonsnl <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.493395: step 6200, loss 9.62747, acc 0\n",
      "\n",
      "\n",
      "i’m prolife for the baby in the womb and for the entire life of that child even when it gets more complicated <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.497097: step 6200, loss 6.22498, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1786/> <AT_NAME_1787/> <AT_NAME_1788/> so the fate of constitutional govt rests solely in the hands of one individual who decided that <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.501032: step 6200, loss 7.35314, acc 0\n",
      "\n",
      "\n",
      "i’m not running for president because it’s my turn but because it’s everyones turn to live in a nation of hope and opportunity for all <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.504534: step 6200, loss 8.51041, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1308/> <AT_NAME_1309/> <AT_NAME_1310/> no trump you are liberal con you should not be pres of a republic #iowacaucus #trump2016 #ccot <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.507730: step 6200, loss 10.8306, acc 0\n",
      "\n",
      "\n",
      "justice scalia’s passing underscores the enormous gravity of this election #scotus is now hanging in the balance #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.511293: step 6200, loss 10.0743, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1535/> well go get it and straighten this mess out <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.515187: step 6200, loss 6.28481, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1460/> knows his electorate he spent the last four years burning bridges to run for president #ambition <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.518950: step 6200, loss 6.9022, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1467/> thank you yes it is a disaster <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.522459: step 6200, loss 6.75964, acc 0\n",
      "\n",
      "\n",
      "tune into <AT_NAME_304/> tomorrow to see my interview with mom <AT_NAME_305/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.526631: step 6200, loss 7.38932, acc 0\n",
      "\n",
      "\n",
      "it wouldnt be a swing through the upstate without a stop at the beacon  a south carolina staple <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.531130: step 6200, loss 8.78874, acc 0\n",
      "\n",
      "\n",
      "[hillary] tried to help the person thats less fortunate” <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.534716: step 6200, loss 7.12805, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1104/> <AT_NAME_1105/> agreed so many fair balanced women <AT_NAME_1106/> capable of doing an honest job #ban megyn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.539567: step 6200, loss 10.9171, acc 0\n",
      "\n",
      "\n",
      "on the road lately ive been getting asked a lot of questions about my faith heres what i tell people <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.543343: step 6200, loss 10.2419, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2209/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.547020: step 6200, loss 1.71639, acc 0\n",
      "\n",
      "\n",
      "tonight is a victory for the grassroots for conservatives across iowa and conservatives across the nation <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.550257: step 6200, loss 4.89688, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1199/> <AT_NAME_1200/> <AT_NAME_1201/> <AT_NAME_1202/> <AT_NAME_1203/> #feelthebern <AT_NAME_1204/> <AT_NAME_1205/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.553924: step 6200, loss 4.50564, acc 0\n",
      "\n",
      "\n",
      "here we go at the cnn #gopdebate join us <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.557699: step 6200, loss 4.02616, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1394/> <AT_NAME_1395/> <AT_NAME_1396/> she has failed at everything she has done #notreadyforhillary <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.561317: step 6200, loss 7.89346, acc 0\n",
      "\n",
      "\n",
      "when i got to hq i found last nights pizza <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.564952: step 6200, loss 11.4342, acc 0\n",
      "\n",
      "\n",
      "in the last #gopdebate we saw the difference between the talkers leaders we need a proven conservative reformer <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.568769: step 6200, loss 13.6152, acc 0\n",
      "\n",
      "\n",
      "this election is going to be a turning point #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.572718: step 6200, loss 3.13144, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2336/> <AT_NAME_2337/> yet another reason not to vote for trump <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.576173: step 6200, loss 10.3236, acc 0\n",
      "\n",
      "\n",
      "watch live here #cruzcrew <URL/> #iacaucus #caucusforcruz <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.580274: step 6200, loss 5.59832, acc 0\n",
      "\n",
      "\n",
      "lets not make the same mistake twice <URL/> #fitn #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.585200: step 6200, loss 3.47408, acc 0\n",
      "\n",
      "\n",
      "thank you to my nh family <URL/> #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.588630: step 6200, loss 9.24999, acc 0\n",
      "\n",
      "\n",
      "thank you for your support friendship governor <AT_NAME_1026/> #makeamericagreatagain #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.593089: step 6200, loss 7.11366, acc 0\n",
      "\n",
      "\n",
      "the current federal minimum wage of $725 an hour is a starvation wage and must be raised to a living wage of $15 an hour <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.598173: step 6200, loss 8.8983, acc 0\n",
      "\n",
      "\n",
      "we need to be looking for solutions  its not about any candidate its about the american people #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.601740: step 6200, loss 13.026, acc 0\n",
      "\n",
      "\n",
      "if our next president is even half the president ronald reagan was america will be greater than its ever been <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.605823: step 6200, loss 9.7596, acc 0\n",
      "\n",
      "\n",
      "easley sc is ready to #choosecruz saturday <URL/> special thanks to <AT_NAME_760/> for joining <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.609504: step 6200, loss 9.20053, acc 0\n",
      "\n",
      "\n",
      "what we confirmed tonight is that were building the political revolution our nation needs this is your movement <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.613495: step 6200, loss 9.59342, acc 0\n",
      "\n",
      "\n",
      "a message to the great people of new hampshire on this important day #votetrumpnh video <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.616961: step 6200, loss 12.2438, acc 0\n",
      "\n",
      "\n",
      "im fighting for people who cannot wait for [change] and im not making promises that i cannot keep #demdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.620708: step 6200, loss 10.379, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1650/> democracy their goal is to affect urbanization and the current century of urbanization their goal is to affect the current <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.624783: step 6200, loss 8.17683, acc 0\n",
      "\n",
      "\n",
      "the #carsonflattax is a simple transparent fair tax that will empower we the people and energize our economy <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.628461: step 6200, loss 7.17047, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1043/> whats your involvement in #therockefellerinitiative #thetruthisoutthere #disclosure <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.632186: step 6200, loss 7.70077, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_2029/> <AT_NAME_2030/> <AT_NAME_2031/> <AT_NAME_2032/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.636981: step 6200, loss 9.00438, acc 0\n",
      "\n",
      "\n",
      "time to #gotvforhrc send an extra nudge to a nh voter to make sure they vote by texting note to 47246📱 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.640508: step 6200, loss 9.9388, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1546/> spending all his time apologizing to blm while the trump is getting 25% of the black vote <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.645579: step 6200, loss 8.68818, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_542/> <AT_NAME_543/> good i want to see the court decide what i was always told natural born means  born on american soil <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.649245: step 6200, loss 7.89952, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1159/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.653016: step 6200, loss 2.31886, acc 0\n",
      "\n",
      "\n",
      "you arent going to accomplish what we need for working families as long as big money interests control the us congress #demdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.657022: step 6200, loss 8.09208, acc 0\n",
      "\n",
      "\n",
      "it is insane that lowwage workers for companies like mcdonald’s must work when they are sick just because they can’t afford to stay home <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.660784: step 6200, loss 13.1513, acc 0\n",
      "\n",
      "\n",
      "“more than 700000 people have contributed to this campaign the vast majority giving less than $100” —hillary in nh <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.665537: step 6200, loss 12.5099, acc 0\n",
      "\n",
      "\n",
      "texans early voting begins today find your polling place and #choosecruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.669342: step 6200, loss 7.42244, acc 0\n",
      "\n",
      "\n",
      "today only contribute $10 or more to get your signed mini football <URL/> #sb50 #cruzcrew <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.672769: step 6200, loss 8.54002, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_591/> thank you <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.676417: step 6200, loss 5.0875, acc 0\n",
      "\n",
      "\n",
      "no questions on this at #demdebate but a reminder women deserve a champion in the white house <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.681138: step 6200, loss 5.03198, acc 0\n",
      "\n",
      "\n",
      "thank you for the kind words tonight <AT_NAME_151/> you were great see you soon <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.684721: step 6200, loss 7.92977, acc 0\n",
      "\n",
      "\n",
      "<AT_NAME_1936/> reminds me of arnold <AT_NAME_1937/> run for governator <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.688351: step 6200, loss 6.45287, acc 0\n",
      "\n",
      "\n",
      "here we go at the cbs #gopdebate tune in #cruzcrew <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-07T19:39:08.691952: step 6200, loss 6.83094, acc 0\n",
      "\n",
      "\n",
      "vladimir putin is an opportunist hes a bully and we have to face him head on #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.696741: step 6200, loss 1.74044e-05, acc 1\n",
      "\n",
      "\n",
      "what did i think of the #gopdebate not good enough watch <AT_NAME_400/> instead <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.700462: step 6200, loss 3.81469e-06, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1185/> <AT_NAME_1186/> <AT_NAME_1187/> trump 2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.703840: step 6200, loss 0.0175606, acc 1\n",
      "\n",
      "\n",
      "two of our star supporters helping get out the vote in south carolina 🇺🇸 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.707709: step 6200, loss 0.000111574, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2600/> #trump2016 great endorsement  congrats #maga <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.710911: step 6200, loss 0.000838167, acc 1\n",
      "\n",
      "\n",
      "we are the party of diversity not the democratic party #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.714733: step 6200, loss 0.000202754, acc 1\n",
      "\n",
      "\n",
      "watch live on <AT_NAME_375/> hillary talks to students at <AT_NAME_376/> about the issues on their mind <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.718121: step 6200, loss 6.12717e-05, acc 1\n",
      "\n",
      "\n",
      "#iacaucus #caucusfortrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.721464: step 6200, loss 0.00668586, acc 1\n",
      "\n",
      "\n",
      "nevada doors don’t close until noon find your caucus location here <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.725138: step 6200, loss 0.000141253, acc 1\n",
      "\n",
      "\n",
      "watch it here <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.728089: step 6200, loss 0.00225599, acc 1\n",
      "\n",
      "\n",
      "it’s hard to describe in words what parents and kids are going through in flint <AT_NAME_643/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.731481: step 6200, loss 9.94156e-05, acc 1\n",
      "\n",
      "\n",
      "executives for coal and gas companies have blocked every attempt to act on climate change and have bought the loyalty of elected officials <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.734797: step 6200, loss 1.00135e-05, acc 1\n",
      "\n",
      "\n",
      "jeb failed as jeb he gave up and enlisted mommy and his brother (who got us into the quicksand of iraq) spent $120 millionweakno chance <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.738592: step 6200, loss 8.68998e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2026/> <AT_NAME_2027/> <AT_NAME_2028/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.742398: step 6200, loss 0.000529388, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2407/> trump 2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.746028: step 6200, loss 0.00492846, acc 1\n",
      "\n",
      "\n",
      "really dumb <AT_NAME_341/> begged my people for a job turned her down twice and she went hostile major loser zero credibility <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.750613: step 6200, loss 0.00291946, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2442/> agreed <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.754366: step 6200, loss 0.0140354, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2056/> the fact that we cant get adequate healthcare to our veterans is also a national disgrace <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.757819: step 6200, loss 5.12599e-06, acc 1\n",
      "\n",
      "\n",
      "democrats win when the voter turnout is high when people come together and we reject division register to vote <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.761148: step 6200, loss 2.53913e-05, acc 1\n",
      "\n",
      "\n",
      "keep america safe keep gtmo open add your name  <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.764736: step 6200, loss 8.22541e-06, acc 1\n",
      "\n",
      "\n",
      "join me tomorrow in plymouth new hampshire #fitn #nhprimary <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.768930: step 6200, loss 1.95501e-05, acc 1\n",
      "\n",
      "\n",
      "we are not going to deport 12 million people demonizing immigrants is beneath our values #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.772707: step 6200, loss 1.21593e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1223/> <AT_NAME_1224/> <AT_NAME_1225/> <AT_NAME_1226/> <AT_NAME_1227/> #feelthebern <AT_NAME_1228/> <AT_NAME_1229/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.776061: step 6200, loss 0.000212885, acc 1\n",
      "\n",
      "\n",
      "innovation and business success should be rewarded but greed for the sake of greed is not something that public policy should support <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.779262: step 6200, loss 3.12323e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1929/> <AT_NAME_1930/> that isnt all that different from obama trying to ban fox from the white house briefings food for thought <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.784275: step 6200, loss 0.000120156, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1117/> hun he would need to have achieved progress to build on it <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.787589: step 6200, loss 3.42125e-05, acc 1\n",
      "\n",
      "\n",
      "i will be on <AT_NAME_872/> tonight from las vegas nevada at 10pme enjoy #hannity #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.791487: step 6200, loss 7.55758e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2583/> who one of the most respected thats like saying your one the most respected businessman not <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.794709: step 6200, loss 0.000515924, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2338/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.798314: step 6200, loss 0.04413, acc 1\n",
      "\n",
      "\n",
      "we’re bringing the message of liberty to the live free or die state <URL/> #fitn #nhpolitics <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.803004: step 6200, loss 0.000274382, acc 1\n",
      "\n",
      "\n",
      "we are building together is a political revolution that will bring tens of millions of people together join us <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.806515: step 6200, loss 0.000466834, acc 1\n",
      "\n",
      "\n",
      "we must increase the min wage to $15 which will increase the wages of nearly 60 percent of latinos #votetogether <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.809998: step 6200, loss 3.57627e-06, acc 1\n",
      "\n",
      "\n",
      "thank you iowa this is our time join us <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.814246: step 6200, loss 0.000107163, acc 1\n",
      "\n",
      "\n",
      "we honor sandra blands memory by acting to reform policing practices proud to have her mom geneva on this team <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.818017: step 6200, loss 1.31129e-05, acc 1\n",
      "\n",
      "\n",
      "#choosecruz before 7 pm et <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.821882: step 6200, loss 0.0045666, acc 1\n",
      "\n",
      "\n",
      "the kids did something they never get to do back in miami build snowmen <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.826263: step 6200, loss 2.38418e-06, acc 1\n",
      "\n",
      "\n",
      "caught a few minutes of a basketball game in daniel island today rooting for the cheetah tigers <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.830019: step 6200, loss 0.00158637, acc 1\n",
      "\n",
      "\n",
      "what an event i hope you’ll stand with me this saturday for the #fits primary join the team today <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.835143: step 6200, loss 9.02374e-05, acc 1\n",
      "\n",
      "\n",
      "the next president will not be able to serve 48 years without dealing with the national debt #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.838294: step 6200, loss 2.45568e-05, acc 1\n",
      "\n",
      "\n",
      "#makeamericagreatagain <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.841357: step 6200, loss 0.0670819, acc 1\n",
      "\n",
      "\n",
      "if <AT_NAME_531/> doesn’t clean up his act stop cheating doing negative ads i have standing to sue him for not being a natural born citizen <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.844946: step 6200, loss 0.00109708, acc 1\n",
      "\n",
      "\n",
      "when it comes to supporting real family values the united states lags behind virtually every major country on earth <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.848698: step 6200, loss 0.000157106, acc 1\n",
      "\n",
      "\n",
      "when i was leading the fight against amnesty <AT_NAME_1020/> was firing <AT_NAME_1021/> on tv #rhetoricvsrecord <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.852376: step 6200, loss 5.32851e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2126/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.856412: step 6200, loss 0.156057, acc 1\n",
      "\n",
      "\n",
      "the senate’s duty is to advise and consent you know what the senate is advising right now <URL/> #scotus <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.860006: step 6200, loss 3.85039e-05, acc 1\n",
      "\n",
      "\n",
      "lying ted cruz and lightweight choker marco rubio teamed up last night in a last ditch effort to stop our great movement they failed <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.863934: step 6200, loss 3.21865e-06, acc 1\n",
      "\n",
      "\n",
      "today i want you to remember that mother and father <URL/> #iacaucus <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.867728: step 6200, loss 0.00013291, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2555/> donaldtrump will be thenext potus the evolution is happening the revolution is real <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.871377: step 6200, loss 0.000474579, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1703/> the cost of war doesnt end until every vet receives best health care <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.875213: step 6200, loss 5.3644e-06, acc 1\n",
      "\n",
      "\n",
      "great speaking with <AT_NAME_23/> last night at our revive 714 event in iowa #candidcarson <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.878546: step 6200, loss 2.38419e-07, acc 1\n",
      "\n",
      "\n",
      "unlike <AT_NAME_638/> i fought against obamacare expansion every step of the way the next president must do the same <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.882383: step 6200, loss 0.000693915, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1982/> not believing you <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.885950: step 6200, loss 0.00142687, acc 1\n",
      "\n",
      "\n",
      "no matter who you are what you look like or where you come from—you deserve a president who will fight for you <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.889915: step 6200, loss 0.000949171, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1172/> <AT_NAME_1173/> <AT_NAME_1174/> except our lying eyes and ears say differently it is about beck (and megyn) rage @ trump is personal <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.893673: step 6200, loss 6.07949e-05, acc 1\n",
      "\n",
      "\n",
      "i am a progressive who likes to make progress i dont make promises i cant keep —hillary #demtownhall <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.897509: step 6200, loss 0.000928448, acc 1\n",
      "\n",
      "\n",
      "jeb won last night’s #gopdebate lets keep the momentum going — chip in $1 today <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.900693: step 6200, loss 0.00108731, acc 1\n",
      "\n",
      "\n",
      "big storm in new hampshire moved my event to monday will be there next four days <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.904383: step 6200, loss 0.000106448, acc 1\n",
      "\n",
      "\n",
      "we can and must do more to honor the commitment to serve the men and women who have served our great nation <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.909239: step 6200, loss 2.0623e-05, acc 1\n",
      "\n",
      "\n",
      "we can reignite the promise of america and with your help we will join the #cruzcrew <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.912870: step 6200, loss 9.64357e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1771/> is a scum <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.916759: step 6200, loss 0.124081, acc 1\n",
      "\n",
      "\n",
      "our health care plan is the only plan that would provide care for all americans regardless of income <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.919841: step 6200, loss 0.000612548, acc 1\n",
      "\n",
      "\n",
      "when i say i will secure the border and stop illegal immigration you can trust me #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.923619: step 6200, loss 2.53913e-05, acc 1\n",
      "\n",
      "\n",
      "caucus day is here iowa text caucus to 47246 to find your caucus location and make a plan to get there by 630 pm shes counting on you <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.927398: step 6200, loss 0.00179844, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1180/> <AT_NAME_1181/> take it from her shes been defunct since 1988 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.930812: step 6200, loss 0.000582764, acc 1\n",
      "\n",
      "\n",
      "join thousands of people saying #imwithher before the iowa caucus <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.934525: step 6200, loss 0.00471421, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1317/> <AT_NAME_1318/> the us taxpayer should not be paying for abortions thats what gop is sayingwhen dems rule <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.938207: step 6200, loss 0.00220223, acc 1\n",
      "\n",
      "\n",
      "ill be on the radio with <AT_NAME_297/> at 6pm listen live here <URL/> #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.941419: step 6200, loss 0.0182129, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1839/> <URL/> mr trumpplease tell my friend that youll defund planned parenthood thank you as always <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.944803: step 6200, loss 0.000275097, acc 1\n",
      "\n",
      "\n",
      "i will unite this country around common purpose because i did it as governor of florida #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.948285: step 6200, loss 4.70866e-05, acc 1\n",
      "\n",
      "\n",
      "look up your caucus location to #caucusforbernie in #iowatoday <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.952184: step 6200, loss 1.0848e-05, acc 1\n",
      "\n",
      "\n",
      "flexibility is a good thing but you shouldn’t be flexible on core principles #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.956390: step 6200, loss 0.000180586, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2330/> republican establishment is to blame for trump vote ted cruz <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.961259: step 6200, loss 0.0023402, acc 1\n",
      "\n",
      "\n",
      "send a message to hillary and tell her you’ve got her back all the way to the white house <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.965280: step 6200, loss 0.000154841, acc 1\n",
      "\n",
      "\n",
      "woah big turnout for our #gotvforbernie rally at <AT_NAME_452/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.968671: step 6200, loss 0.000243157, acc 1\n",
      "\n",
      "\n",
      "see you soon <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.972479: step 6200, loss 0.000347077, acc 1\n",
      "\n",
      "\n",
      "i’m a progressive who likes to get things done and i’ve learned that you have to be both a dreamer and a doer —hillary in nh <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.976525: step 6200, loss 0.000555599, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_51/> <AT_NAME_52/> our retrumplican trump support group of 9500 members say 97% will not watch the debate tonight <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.980277: step 6200, loss 0.000475532, acc 1\n",
      "\n",
      "\n",
      "the iowa caucus is coming up fast say #imwithher and commit to vote or caucus in your state <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.984362: step 6200, loss 0.000275574, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1071/> <AT_NAME_1072/> <AT_NAME_1073/> <AT_NAME_1074/> using kelly again is being provocative debate is about the candidates not fox <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.987927: step 6200, loss 8.34462e-06, acc 1\n",
      "\n",
      "\n",
      "like the worthless <AT_NAME_479/> looks like <AT_NAME_480/> will be going out of business bad reporting no money no cred <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.992106: step 6200, loss 0.00410061, acc 1\n",
      "\n",
      "\n",
      "thank you for your service luke i hope you can join us tonight <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:08.996715: step 6200, loss 0.00688845, acc 1\n",
      "\n",
      "\n",
      "i will defend the bill of rights every day for my children and for yours #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.000468: step 6200, loss 3.24244e-05, acc 1\n",
      "\n",
      "\n",
      "the polls show that i picked up many jeb bush supporters that is how i got to 46% when others drop out i will pick up more sad but true <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.004201: step 6200, loss 0.000397603, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1670/> throwing away all new york voters not too smart which is the next city youll offend rafael <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.007704: step 6200, loss 0.000181897, acc 1\n",
      "\n",
      "\n",
      "the polls are now showing that i am the best to win the general election states that are never in play for repubs will be won by me great <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.012227: step 6200, loss 8.78534e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1654/> <AT_NAME_1655/> <AT_NAME_1656/> <AT_NAME_1657/> <AT_NAME_1658/> obama and his muslim brotherhood has set up nwo nau very bad <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.015876: step 6200, loss 4.81594e-05, acc 1\n",
      "\n",
      "\n",
      "thank you america #trump2016 via <AT_NAME_172/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.019829: step 6200, loss 0.000150431, acc 1\n",
      "\n",
      "\n",
      "spartanburg sc it was great to join you today remember to #choosecruz on saturday <URL/> #fits <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.024009: step 6200, loss 2.33647e-05, acc 1\n",
      "\n",
      "\n",
      "christie stands out tackles questions others duck  <AT_NAME_136/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.027576: step 6200, loss 4.98282e-05, acc 1\n",
      "\n",
      "\n",
      "fighting for you and winning join the #cruzcrew <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.031371: step 6200, loss 0.00164933, acc 1\n",
      "\n",
      "\n",
      "watch jebs response on how hell destroy isis #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.035238: step 6200, loss 7.9271e-05, acc 1\n",
      "\n",
      "\n",
      "america becomes a greater nation when we stand together and say no to racism hatred and bigotry #americatogether <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.038946: step 6200, loss 0.000192266, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1624/> common core is the dumbest thing ive ever seen besides hilary voters <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.042482: step 6200, loss 9.42901e-05, acc 1\n",
      "\n",
      "\n",
      "if we face the reality of systemic racism we can begin to reform our broken criminal justice immigration systems <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.046004: step 6200, loss 9.77511e-06, acc 1\n",
      "\n",
      "\n",
      "3 more days join the team <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.051161: step 6200, loss 0.000833283, acc 1\n",
      "\n",
      "\n",
      "#cruzcrew dont miss tonights #gopdebate <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.054615: step 6200, loss 8.17742e-05, acc 1\n",
      "\n",
      "\n",
      "it never ends <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.058516: step 6200, loss 9.82236e-05, acc 1\n",
      "\n",
      "\n",
      "south carolina is jeb country <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.062999: step 6200, loss 0.00808124, acc 1\n",
      "\n",
      "\n",
      "2% economic growth means declining incomes fewer opportunities for future generations my plan for 4% growth → <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.066877: step 6200, loss 0.00018142, acc 1\n",
      "\n",
      "\n",
      "true freedom does not occur without economic security #bernieindenver <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.071308: step 6200, loss 0.00302619, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1776/> fuckin a speak truth to power <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.076326: step 6200, loss 0.00323737, acc 1\n",
      "\n",
      "\n",
      "trump has proven already that he is completely malleable <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.080105: step 6200, loss 1.3113e-06, acc 1\n",
      "\n",
      "\n",
      "this is it your last chance to say youre with hillary before the new hampshire primary commit today <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.084107: step 6200, loss 8.22541e-06, acc 1\n",
      "\n",
      "\n",
      "need to look up your polling location <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.087717: step 6200, loss 0.000113481, acc 1\n",
      "\n",
      "\n",
      "it’s not whether you get knocked down that matters it’s whether you get back up <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.091053: step 6200, loss 5.00666e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1830/> 50k veterans and 25 million children i am slightly more concerned about the kids sir #morethanslightly <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.097102: step 6200, loss 0.000235649, acc 1\n",
      "\n",
      "\n",
      "on the campaign trail in new hampshire with <AT_NAME_301/> and <AT_NAME_302/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.100666: step 6200, loss 2.02656e-06, acc 1\n",
      "\n",
      "\n",
      "thank you baton rouge louisiana we will #makeamericagreatagain #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.104321: step 6200, loss 0.000525694, acc 1\n",
      "\n",
      "\n",
      "in ohio and across the country republicans are once again attacking womens health we wont stand for this <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.107877: step 6200, loss 0.000125996, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2155/> is a hypocrite hes prolife then says he could shoot someone and still not lose voters #dumptrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.111766: step 6200, loss 3.96959e-05, acc 1\n",
      "\n",
      "\n",
      "be our light in dark places lord be a beacon when we have lost sight of you may we always remember your love and mercy in all that we do <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.116524: step 6200, loss 1.54972e-06, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_245/> that means ideas like more school choice to help parents get their kids the education they deserve <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.120516: step 6200, loss 7.15253e-06, acc 1\n",
      "\n",
      "\n",
      "want to show your support for marco right now chip in $5 now <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.124220: step 6200, loss 0.000198345, acc 1\n",
      "\n",
      "\n",
      "the children of flint deserve bright futures—and today those children need our help please chip in if youre able <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.128027: step 6200, loss 0.00049686, acc 1\n",
      "\n",
      "\n",
      "i dont believe i have been given any credit by the voters for selffunding my campaign the only one i will keep doing but not worth it <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.131451: step 6200, loss 0.000697608, acc 1\n",
      "\n",
      "\n",
      "the history of this nation has attempted to divide us that doesnt solve problems it makes them worse #bernieinnv <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.135049: step 6200, loss 3.09944e-06, acc 1\n",
      "\n",
      "\n",
      "im ben and im jerry im a person and im a person ben jerrys not a person go out and vote today <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.138797: step 6200, loss 0.000650909, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1870/> why dont you and #potus pay women equally <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.142799: step 6200, loss 0.0154427, acc 1\n",
      "\n",
      "\n",
      "iowa lets make a little bit of history today come out and caucus #iowatoday <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.145954: step 6200, loss 9.53629e-05, acc 1\n",
      "\n",
      "\n",
      "#cruzcrew dont miss tonights cbs #gopdebate tune in at 9pm et <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.149638: step 6200, loss 6.6159e-05, acc 1\n",
      "\n",
      "\n",
      "little rock youre next im hosting a rally in little rock this afternoon and i hope youll be there rsvp now <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.152986: step 6200, loss 3.99343e-05, acc 1\n",
      "\n",
      "\n",
      "couldnt agree more <AT_NAME_658/> its time to act on gun violence thank you for telling these stories today <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.156276: step 6200, loss 0.000579547, acc 1\n",
      "\n",
      "\n",
      "i ask you to join me in this campaign to build a future that works for all of us and not just the few on top <URL/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.160287: step 6200, loss 2.03846e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2567/>  thank you for being real <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.164170: step 6200, loss 0.00188019, acc 1\n",
      "\n",
      "\n",
      "watch our town hall from dover nh live here <URL/> #christie2016 #fitn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.168635: step 6200, loss 9.61973e-05, acc 1\n",
      "\n",
      "\n",
      "my sister doro spent yesterday campaigning — and taking selfies — with our south carolina volunteers <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.172611: step 6200, loss 5.59076e-05, acc 1\n",
      "\n",
      "\n",
      "this shows what a complete total liar ted cruz is he said he wouldnt have nominated john roberts really <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.175898: step 6200, loss 4.99474e-05, acc 1\n",
      "\n",
      "\n",
      "we should be putting people into guantanamo not emptying it out #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.179374: step 6200, loss 0.00023553, acc 1\n",
      "\n",
      "\n",
      "thanks for hosting <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.183505: step 6200, loss 0.0517998, acc 1\n",
      "\n",
      "\n",
      "sending warmest wishes to everyone celebrating the lunar new year may you have peace and prosperity in the year ahead h <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.187494: step 6200, loss 6.84238e-05, acc 1\n",
      "\n",
      "\n",
      "i commend the florida legislature for taking a stand against companies participating in antiisraeli boycotts we must stand w/ israel <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.191300: step 6200, loss 0.000636852, acc 1\n",
      "\n",
      "\n",
      "we had an awesome crowd at our rally in minneapolis yesterday see for yourself <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.194837: step 6200, loss 0.00159173, acc 1\n",
      "\n",
      "\n",
      "your volunteer shifts are working and we are almost there sign up for as many phone bank shifts as you can <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.198898: step 6200, loss 0.00211432, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2547/> <AT_NAME_2548/> #votetrump <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.202515: step 6200, loss 0.131305, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1903/> <AT_NAME_1904/> <AT_NAME_1905/> or hes more liked than trump in every demographic <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.206333: step 6200, loss 0.00150639, acc 1\n",
      "\n",
      "\n",
      "no other candidate has a more detailed conservative governing agenda my plan for a safer stronger freer americahttps//tco/8jebyrcfa9 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.210173: step 6200, loss 0.0046162, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2304/> itd be nice to hear that youll work together to not youll do it on your own were a republic not dictatorship <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.213450: step 6200, loss 4.64915e-06, acc 1\n",
      "\n",
      "\n",
      "obamacare has brought millions of americans health care they need they cant wait for promises that cant be kept <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.218148: step 6200, loss 1.28745e-05, acc 1\n",
      "\n",
      "\n",
      "as president i will tear down the epa’s blend wall #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.223113: step 6200, loss 0.000395577, acc 1\n",
      "\n",
      "\n",
      "millions of americans lost their jobs homes and life savings because of a handful of people on wall street #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.227115: step 6200, loss 0.000141253, acc 1\n",
      "\n",
      "\n",
      "looking forward to our rally in derry nh tonight not too late to rsvp <URL/> #fitn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.230413: step 6200, loss 0.000450748, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_745/> im convinced ted cruz is the leader who will keep us safe support our troops defend our nation <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.234454: step 6200, loss 6.54438e-05, acc 1\n",
      "\n",
      "\n",
      "the iranians know a mark when they see one want more concessions before obama leaves office <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.238306: step 6200, loss 0.00281902, acc 1\n",
      "\n",
      "\n",
      "join me live now in las vegas nevada we will make america safe great again #votetrumpnv #nevadacaucus <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.241966: step 6200, loss 0.000120513, acc 1\n",
      "\n",
      "\n",
      "donald trump and planned parenthood watch  <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.245264: step 6200, loss 0.000624223, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1567/> <AT_NAME_1568/> read this) <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.249227: step 6200, loss 0.105334, acc 1\n",
      "\n",
      "\n",
      "tens of thousands of fellow bernie supporters are on their way to the caucus get to your caucus location now <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.252742: step 6200, loss 9.41709e-05, acc 1\n",
      "\n",
      "\n",
      "will be interviewed on <AT_NAME_455/> at 800 am enjoy <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.256568: step 6200, loss 4.81594e-05, acc 1\n",
      "\n",
      "\n",
      "i believe that i am ready ive been tested ive been pushed and i have kept moving forward <URL/> #fitn #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.259986: step 6200, loss 0.000407851, acc 1\n",
      "\n",
      "\n",
      "i love new hampshire  will be an exciting evening <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.263905: step 6200, loss 0.000285942, acc 1\n",
      "\n",
      "\n",
      "listen to caucusgoers from all over iowa share why theyre on #teammarco <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.267393: step 6200, loss 3.36165e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1556/> <AT_NAME_1557/> <AT_NAME_1558/> wrong again donald <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.271104: step 6200, loss 0.00102849, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1818/> donald i think if we want our military strong we need to hire more idealists and inventors in star wars attack of the <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.276104: step 6200, loss 0.000102396, acc 1\n",
      "\n",
      "\n",
      "our nation is not based on race or ethnicity its based on shared values #gopdebate 🇺🇸 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.279651: step 6200, loss 1.95501e-05, acc 1\n",
      "\n",
      "\n",
      "awesome grateful for the support <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.283103: step 6200, loss 0.00507768, acc 1\n",
      "\n",
      "\n",
      "my conversation with <AT_NAME_331/> is beginning now watch here <URL/> #cruzcrew <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.286490: step 6200, loss 0.00208458, acc 1\n",
      "\n",
      "\n",
      "i will be interviewed by anderson cooper at 8pm on <AT_NAME_300/> from new hampshire should be very interesting <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.289906: step 6200, loss 0.000297621, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2134/> what 94 million out of work5 terrorist attacks in usworse race relations in 30 yrswages at a 7 yr low19 trillion debt <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.293311: step 6200, loss 4.17224e-05, acc 1\n",
      "\n",
      "\n",
      "they say you dont care they say you wont caucus they say we cant win prove them wrong #caucusforbernie <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.296783: step 6200, loss 1.19209e-06, acc 1\n",
      "\n",
      "\n",
      "why did chris christie get down on one knee in nh today via <AT_NAME_449/> #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.300269: step 6200, loss 0.000189406, acc 1\n",
      "\n",
      "\n",
      "christie woos voters with experience candor httphttps//tco/mechfujtmp #fitn #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.304019: step 6200, loss 0.00259135, acc 1\n",
      "\n",
      "\n",
      "dont let #duckingdonald skip the debate get your #maketrumpdebateagain hat today  <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.307570: step 6200, loss 0.000436449, acc 1\n",
      "\n",
      "\n",
      "google ben carson during the debate tonight to see realtime responses from the carson team <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.311173: step 6200, loss 0.000339927, acc 1\n",
      "\n",
      "\n",
      "affordable health care should be a human right in america not a privilege we will not let republicans repeal the aca #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.316332: step 6200, loss 3.45706e-06, acc 1\n",
      "\n",
      "\n",
      "about to be on the radio with <AT_NAME_159/> listen live <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.320456: step 6200, loss 0.00439618, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1078/> true and it could happen if you and bill just leave <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.323980: step 6200, loss 0.00316191, acc 1\n",
      "\n",
      "\n",
      "2 hours to get out and vote in the #scprimary find your polling place here <URL/> #scformarco <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.329566: step 6200, loss 0.0101168, acc 1\n",
      "\n",
      "\n",
      "just one day until south carolina votes join the team before the #scprimary <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.333190: step 6200, loss 0.000314186, acc 1\n",
      "\n",
      "\n",
      "the debate tonight will be a total disaster  low ratings with advertisers and advertising rates dropping like a rock i hate to see this <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.336650: step 6200, loss 0.000445267, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2164/> <AT_NAME_2165/> <AT_NAME_2166/> you coward i hope her husband punches you in the nose for calling her a bimbo on twitter <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.340655: step 6200, loss 0.000520213, acc 1\n",
      "\n",
      "\n",
      "#caucusforcruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.344139: step 6200, loss 0.000708925, acc 1\n",
      "\n",
      "\n",
      "watch and rt  <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.347711: step 6200, loss 0.0211491, acc 1\n",
      "\n",
      "\n",
      "our team continues to grow proud to have the support of one of the senates strongest advocates for life <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.351234: step 6200, loss 5.00666e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1968/> <AT_NAME_1969/> <AT_NAME_1970/> dont cry she doesnt cheer you and that disappoint you <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.355175: step 6200, loss 0.000420482, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1597/> thank god america has allowed functional illiterates to leave schools for decades education has ceased to exist <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.358942: step 6200, loss 2.30071e-05, acc 1\n",
      "\n",
      "\n",
      "appalling for <AT_NAME_753/> to legitimize the castro regime with a visit before freedom for cuban people watch <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.362786: step 6200, loss 0.000248044, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2521/> really i was going to vote for you until i read this falwell is an ass clown <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.367836: step 6200, loss 0.00019322, acc 1\n",
      "\n",
      "\n",
      "my last visit to the palmetto state was a good one check out highlights from the debate watch <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.372406: step 6200, loss 0.00319946, acc 1\n",
      "\n",
      "\n",
      "live on the <AT_NAME_742/> now tune in #cruzcrew <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.376182: step 6200, loss 0.000985137, acc 1\n",
      "\n",
      "\n",
      "thank you cheryl <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.380718: step 6200, loss 0.00555851, acc 1\n",
      "\n",
      "\n",
      "want to help amplify bernies message with other supporters using social media join connect <URL/> #votetogether <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.384271: step 6200, loss 6.07966e-06, acc 1\n",
      "\n",
      "\n",
      "my plan to stop the nlrb so we can grow the economy increase employment and raise middle class incomes → <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.387473: step 6200, loss 1.83581e-05, acc 1\n",
      "\n",
      "\n",
      "we will stop heroin and other drugs from coming into new hampshire from our open southern border we will build a wall and have security <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.391186: step 6200, loss 1.44242e-05, acc 1\n",
      "\n",
      "\n",
      "coming up next on cnns #goptownhall tune in #cruzcrew <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.394723: step 6200, loss 1.21593e-05, acc 1\n",
      "\n",
      "\n",
      "my daughters joined in with our supporters for a few chants outside a polling place earlier today #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.398446: step 6200, loss 0.00114828, acc 1\n",
      "\n",
      "\n",
      "#cruzcrew ill be on <AT_NAME_624/> and <AT_NAME_625/> <AT_NAME_626/> this morning hope youll tune in <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.402410: step 6200, loss 0.000407494, acc 1\n",
      "\n",
      "\n",
      "we are a nation of immigrants but we are also a sovereign nation of laws our sovereignty demands that we protect our borders <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.406356: step 6200, loss 2.86102e-06, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1486/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.410020: step 6200, loss 0.0442959, acc 1\n",
      "\n",
      "\n",
      "this campaign is about a political revolution — millions of people standing up and saying enough is enough #americatogether <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.413422: step 6200, loss 0.000446697, acc 1\n",
      "\n",
      "\n",
      "its no accident a machetewielding terrorist attacked a restaurant named nazareth and owned by an israeli immigrant <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.417466: step 6200, loss 0.0147673, acc 1\n",
      "\n",
      "\n",
      "in case you missed the #goptownhall its reairing on <AT_NAME_749/> at 12am et/9pm pt <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.420989: step 6200, loss 0.000411307, acc 1\n",
      "\n",
      "\n",
      "thank you tiffany <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.424589: step 6200, loss 0.130133, acc 1\n",
      "\n",
      "\n",
      "donald trump has spent a career sticking it to working americans <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.428435: step 6200, loss 0.00364817, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_239/> encourages iowans to get out and #caucusforcruz today <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.432673: step 6200, loss 0.000366263, acc 1\n",
      "\n",
      "\n",
      "statement on the passing of supreme court justice antonin scalia <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.435962: step 6200, loss 6.25829e-05, acc 1\n",
      "\n",
      "\n",
      "i hope to see as many texans as possible for our predebate rally in houston this afternoon rsvp <URL/> <AT_NAME_926/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.439665: step 6200, loss 0.00022099, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1684/> you were born to be real <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.443890: step 6200, loss 0.000471719, acc 1\n",
      "\n",
      "\n",
      "we can live in a country where all of our qualified young people regardless of income can go to college <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.447092: step 6200, loss 0.000114434, acc 1\n",
      "\n",
      "\n",
      "looking forward to our town hall in spartanburg sc w/ <AT_NAME_481/> <AT_NAME_482/> at 12pm rsvp here <URL/> #scformarco <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.450748: step 6200, loss 2.14576e-06, acc 1\n",
      "\n",
      "\n",
      "some of the toughest hardest working leaders in america joined hillary on the campaign trail in new hampshire <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.454337: step 6200, loss 5.3644e-06, acc 1\n",
      "\n",
      "\n",
      "whats happening in flint should break all our hearts—and spur us to action if you are able give to <URL/> h <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.458067: step 6200, loss 1.00135e-05, acc 1\n",
      "\n",
      "\n",
      "the ability to exercise your reproductive rights should be a reality for all not just those who can afford it <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.461594: step 6200, loss 0.000148047, acc 1\n",
      "\n",
      "\n",
      "in 1968 shirley chisholm became the first african american woman elected to congress #blackhistorymonth <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.465654: step 6200, loss 0.000220513, acc 1\n",
      "\n",
      "\n",
      "<URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.469821: step 6200, loss 0.0261472, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1860/> <AT_NAME_1861/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.473386: step 6200, loss 0.0146162, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1251/> <AT_NAME_1252/> <AT_NAME_1253/> #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.477391: step 6200, loss 2.64641e-05, acc 1\n",
      "\n",
      "\n",
      "flashback sanders endorses jesse jackson for president in ‘88 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.481190: step 6200, loss 0.000122778, acc 1\n",
      "\n",
      "\n",
      "failed presidential candidate <AT_NAME_997/> was made to look like a fool by senator harry reid didnt release his tax returns until 9/21/12 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.485476: step 6200, loss 0.000367097, acc 1\n",
      "\n",
      "\n",
      "nevada if you’re in line and have a question or need help at your caucus location call 17025508008 #nvdemscaucus <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.489136: step 6200, loss 8.61846e-05, acc 1\n",
      "\n",
      "\n",
      "we’ve leveled the playing field so billionaires are no longer able to buy our candidates and elections #nhprimary <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.492883: step 6200, loss 1.75236e-05, acc 1\n",
      "\n",
      "\n",
      "trump couple turns to carson for ‘intelligence noncombativeness <URL/> via <AT_NAME_231/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.496632: step 6200, loss 2.6226e-06, acc 1\n",
      "\n",
      "\n",
      "instead of cutting programs for seniors children and veterans we are gonna demand large corporations and the wealthy pay their fair share <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.501002: step 6200, loss 8.35622e-05, acc 1\n",
      "\n",
      "\n",
      "#caucusforcruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.504391: step 6200, loss 0.000708925, acc 1\n",
      "\n",
      "\n",
      "i guess firstterm senators stick together cc <AT_NAME_394/> <URL/> #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.508022: step 6200, loss 3.89807e-05, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_1640/> <AT_NAME_1641/> <AT_NAME_1642/> <AT_NAME_1643/> <AT_NAME_1644/> boycott fox then <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.511815: step 6200, loss 0.00140009, acc 1\n",
      "\n",
      "\n",
      "<AT_NAME_2065/> common core is a disaster lets push the stop loss button and cut our losses on a losing program <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.515717: step 6200, loss 0.000195961, acc 1\n",
      "\n",
      "\n",
      "looking forward to tonights #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.519130: step 6200, loss 3.18284e-05, acc 1\n",
      "\n",
      "\n",
      "ted cruz along with jeb bush pushed justice john roberts onto the supreme court roberts could have killed obamacare twice but didnt <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.523028: step 6200, loss 6.44901e-05, acc 1\n",
      "\n",
      "\n",
      "we do not represent the interests of wall street or the billionaire class we dont want their money this is a peoples campaign <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-07T19:39:09.526663: step 6200, loss 7.37878e-05, acc 1\n",
      "\n",
      "\n",
      "2016-03-07T19:39:09.532230: step 6200, loss 1.74044e-05, acc 1\n",
      "2016-03-07T19:39:09.537303: step 6200, loss 3.81469e-06, acc 1\n",
      "2016-03-07T19:39:09.541009: step 6200, loss 0.0175606, acc 1\n",
      "2016-03-07T19:39:09.544750: step 6200, loss 0.000111574, acc 1\n",
      "2016-03-07T19:39:09.547755: step 6200, loss 0.000838167, acc 1\n",
      "2016-03-07T19:39:09.551254: step 6200, loss 0.000202754, acc 1\n",
      "2016-03-07T19:39:09.554604: step 6200, loss 6.12717e-05, acc 1\n",
      "2016-03-07T19:39:09.559745: step 6200, loss 0.00668586, acc 1\n",
      "2016-03-07T19:39:09.563577: step 6200, loss 0.000141253, acc 1\n",
      "2016-03-07T19:39:09.567478: step 6200, loss 0.00225599, acc 1\n",
      "2016-03-07T19:39:09.571933: step 6200, loss 9.94156e-05, acc 1\n",
      "2016-03-07T19:39:09.575279: step 6200, loss 1.00135e-05, acc 1\n",
      "2016-03-07T19:39:09.578941: step 6200, loss 8.68998e-05, acc 1\n",
      "2016-03-07T19:39:09.583699: step 6200, loss 0.000529388, acc 1\n",
      "2016-03-07T19:39:09.587780: step 6200, loss 0.00492846, acc 1\n",
      "2016-03-07T19:39:09.591463: step 6200, loss 0.00291946, acc 1\n",
      "2016-03-07T19:39:09.595075: step 6200, loss 0.0140354, acc 1\n",
      "2016-03-07T19:39:09.598144: step 6200, loss 5.12599e-06, acc 1\n",
      "2016-03-07T19:39:09.601510: step 6200, loss 2.53913e-05, acc 1\n",
      "2016-03-07T19:39:09.606042: step 6200, loss 8.22541e-06, acc 1\n",
      "2016-03-07T19:39:09.609614: step 6200, loss 1.95501e-05, acc 1\n",
      "2016-03-07T19:39:09.613282: step 6200, loss 1.21593e-05, acc 1\n",
      "2016-03-07T19:39:09.616731: step 6200, loss 0.000212885, acc 1\n",
      "2016-03-07T19:39:09.620020: step 6200, loss 3.12323e-05, acc 1\n",
      "2016-03-07T19:39:09.623553: step 6200, loss 13.5571, acc 0\n",
      "2016-03-07T19:39:09.627270: step 6200, loss 0.000120156, acc 1\n",
      "2016-03-07T19:39:09.630999: step 6200, loss 3.42125e-05, acc 1\n",
      "2016-03-07T19:39:09.634231: step 6200, loss 8.75266, acc 0\n",
      "2016-03-07T19:39:09.638011: step 6200, loss 7.55758e-05, acc 1\n",
      "2016-03-07T19:39:09.642022: step 6200, loss 0.000515924, acc 1\n",
      "2016-03-07T19:39:09.645930: step 6200, loss 0.04413, acc 1\n",
      "2016-03-07T19:39:09.649230: step 6200, loss 0.000274382, acc 1\n",
      "2016-03-07T19:39:09.652554: step 6200, loss 0.000466834, acc 1\n",
      "2016-03-07T19:39:09.655837: step 6200, loss 3.57627e-06, acc 1\n",
      "2016-03-07T19:39:09.659672: step 6200, loss 0.000107163, acc 1\n",
      "2016-03-07T19:39:09.663115: step 6200, loss 7.69328, acc 0\n",
      "2016-03-07T19:39:09.666181: step 6200, loss 1.31129e-05, acc 1\n",
      "2016-03-07T19:39:09.669711: step 6200, loss 0.0045666, acc 1\n",
      "2016-03-07T19:39:09.673591: step 6200, loss 10.5568, acc 0\n",
      "2016-03-07T19:39:09.677354: step 6200, loss 2.38418e-06, acc 1\n",
      "2016-03-07T19:39:09.680397: step 6200, loss 0.00158637, acc 1\n",
      "2016-03-07T19:39:09.684202: step 6200, loss 9.02374e-05, acc 1\n",
      "2016-03-07T19:39:09.688009: step 6200, loss 2.45568e-05, acc 1\n",
      "2016-03-07T19:39:09.691798: step 6200, loss 0.0670819, acc 1\n",
      "2016-03-07T19:39:09.695908: step 6200, loss 0.00109708, acc 1\n",
      "2016-03-07T19:39:09.699381: step 6200, loss 0.000157106, acc 1\n",
      "2016-03-07T19:39:09.702690: step 6200, loss 5.32851e-05, acc 1\n",
      "2016-03-07T19:39:09.706763: step 6200, loss 0.156057, acc 1\n",
      "2016-03-07T19:39:09.710431: step 6200, loss 3.85039e-05, acc 1\n",
      "2016-03-07T19:39:09.714163: step 6200, loss 3.21865e-06, acc 1\n",
      "2016-03-07T19:39:09.717327: step 6200, loss 0.00013291, acc 1\n",
      "2016-03-07T19:39:09.721349: step 6200, loss 0.000474579, acc 1\n",
      "2016-03-07T19:39:09.726300: step 6200, loss 5.3644e-06, acc 1\n",
      "2016-03-07T19:39:09.730107: step 6200, loss 2.38419e-07, acc 1\n",
      "2016-03-07T19:39:09.734013: step 6200, loss 9.97718, acc 0\n",
      "2016-03-07T19:39:09.736965: step 6200, loss 0.000693915, acc 1\n",
      "2016-03-07T19:39:09.740455: step 6200, loss 0.00142687, acc 1\n",
      "2016-03-07T19:39:09.744153: step 6200, loss 0.000949171, acc 1\n",
      "2016-03-07T19:39:09.747986: step 6200, loss 6.07949e-05, acc 1\n",
      "2016-03-07T19:39:09.751560: step 6200, loss 0.000928448, acc 1\n",
      "2016-03-07T19:39:09.754805: step 6200, loss 0.00108731, acc 1\n",
      "2016-03-07T19:39:09.760294: step 6200, loss 0.000106448, acc 1\n",
      "2016-03-07T19:39:09.764064: step 6200, loss 2.0623e-05, acc 1\n",
      "2016-03-07T19:39:09.767688: step 6200, loss 9.64357e-05, acc 1\n",
      "2016-03-07T19:39:09.771659: step 6200, loss 10.8651, acc 0\n",
      "2016-03-07T19:39:09.775597: step 6200, loss 0.124081, acc 1\n",
      "2016-03-07T19:39:09.779755: step 6200, loss 10.2109, acc 0\n",
      "2016-03-07T19:39:09.784143: step 6200, loss 0.000612548, acc 1\n",
      "2016-03-07T19:39:09.787630: step 6200, loss 2.53913e-05, acc 1\n",
      "2016-03-07T19:39:09.791440: step 6200, loss 8.66618, acc 0\n",
      "2016-03-07T19:39:09.794944: step 6200, loss 0.00179844, acc 1\n",
      "2016-03-07T19:39:09.799101: step 6200, loss 0.000582764, acc 1\n",
      "2016-03-07T19:39:09.803039: step 6200, loss 0.00471421, acc 1\n",
      "2016-03-07T19:39:09.806549: step 6200, loss 7.09154, acc 0\n",
      "2016-03-07T19:39:09.809887: step 6200, loss 0.00220223, acc 1\n",
      "2016-03-07T19:39:09.813531: step 6200, loss 9.77685, acc 0\n",
      "2016-03-07T19:39:09.816847: step 6200, loss 0.0182129, acc 1\n",
      "2016-03-07T19:39:09.820823: step 6200, loss 0.000275097, acc 1\n",
      "2016-03-07T19:39:09.824376: step 6200, loss 4.70866e-05, acc 1\n",
      "2016-03-07T19:39:09.828188: step 6200, loss 1.0848e-05, acc 1\n",
      "2016-03-07T19:39:09.831343: step 6200, loss 0.000180586, acc 1\n",
      "2016-03-07T19:39:09.835029: step 6200, loss 0.0023402, acc 1\n",
      "2016-03-07T19:39:09.838732: step 6200, loss 0.000154841, acc 1\n",
      "2016-03-07T19:39:09.842598: step 6200, loss 0.000243157, acc 1\n",
      "2016-03-07T19:39:09.846278: step 6200, loss 0.000347077, acc 1\n",
      "2016-03-07T19:39:09.850333: step 6200, loss 0.000555599, acc 1\n",
      "2016-03-07T19:39:09.854107: step 6200, loss 0.000475532, acc 1\n",
      "2016-03-07T19:39:09.857476: step 6200, loss 0.000275574, acc 1\n",
      "2016-03-07T19:39:09.861277: step 6200, loss 8.34462e-06, acc 1\n",
      "2016-03-07T19:39:09.865060: step 6200, loss 0.00410061, acc 1\n",
      "2016-03-07T19:39:09.868666: step 6200, loss 0.00688845, acc 1\n",
      "2016-03-07T19:39:09.872505: step 6200, loss 8.55906, acc 0\n",
      "2016-03-07T19:39:09.875836: step 6200, loss 3.24244e-05, acc 1\n",
      "2016-03-07T19:39:09.879117: step 6200, loss 0.000397603, acc 1\n",
      "2016-03-07T19:39:09.882497: step 6200, loss 0.000181897, acc 1\n",
      "2016-03-07T19:39:09.886310: step 6200, loss 8.78534e-05, acc 1\n",
      "2016-03-07T19:39:09.889942: step 6200, loss 10.7951, acc 0\n",
      "2016-03-07T19:39:09.893706: step 6200, loss 7.403, acc 0\n",
      "2016-03-07T19:39:09.897192: step 6200, loss 4.81594e-05, acc 1\n",
      "2016-03-07T19:39:09.901677: step 6200, loss 0.000150431, acc 1\n",
      "2016-03-07T19:39:09.905875: step 6200, loss 2.33647e-05, acc 1\n",
      "2016-03-07T19:39:09.909913: step 6200, loss 4.98282e-05, acc 1\n",
      "2016-03-07T19:39:09.913700: step 6200, loss 0.00164933, acc 1\n",
      "2016-03-07T19:39:09.917247: step 6200, loss 7.9271e-05, acc 1\n",
      "2016-03-07T19:39:09.922133: step 6200, loss 0.000192266, acc 1\n",
      "2016-03-07T19:39:09.927157: step 6200, loss 7.49749, acc 0\n",
      "2016-03-07T19:39:09.930547: step 6200, loss 9.42901e-05, acc 1\n",
      "2016-03-07T19:39:09.934254: step 6200, loss 9.77511e-06, acc 1\n",
      "2016-03-07T19:39:09.937483: step 6200, loss 0.000833283, acc 1\n",
      "2016-03-07T19:39:09.940959: step 6200, loss 2.69339, acc 0\n",
      "2016-03-07T19:39:09.944434: step 6200, loss 8.17742e-05, acc 1\n",
      "2016-03-07T19:39:09.948082: step 6200, loss 9.82236e-05, acc 1\n",
      "2016-03-07T19:39:09.953634: step 6200, loss 0.00808124, acc 1\n",
      "2016-03-07T19:39:09.956852: step 6200, loss 0.00018142, acc 1\n",
      "2016-03-07T19:39:09.960813: step 6200, loss 0.00302619, acc 1\n",
      "2016-03-07T19:39:09.964572: step 6200, loss 0.00323737, acc 1\n",
      "2016-03-07T19:39:09.968035: step 6200, loss 1.3113e-06, acc 1\n",
      "2016-03-07T19:39:09.971507: step 6200, loss 8.22541e-06, acc 1\n",
      "2016-03-07T19:39:09.975322: step 6200, loss 0.000113481, acc 1\n",
      "2016-03-07T19:39:09.979026: step 6200, loss 5.00666e-05, acc 1\n",
      "2016-03-07T19:39:09.982308: step 6200, loss 0.000235649, acc 1\n",
      "2016-03-07T19:39:09.985796: step 6200, loss 2.02656e-06, acc 1\n",
      "2016-03-07T19:39:09.989150: step 6200, loss 0.000525694, acc 1\n",
      "2016-03-07T19:39:09.992220: step 6200, loss 0.000125996, acc 1\n",
      "2016-03-07T19:39:09.996782: step 6200, loss 3.96959e-05, acc 1\n",
      "2016-03-07T19:39:10.000215: step 6200, loss 1.54972e-06, acc 1\n",
      "2016-03-07T19:39:10.004513: step 6200, loss 7.15253e-06, acc 1\n",
      "2016-03-07T19:39:10.008323: step 6200, loss 0.000198345, acc 1\n",
      "2016-03-07T19:39:10.013165: step 6200, loss 0.00049686, acc 1\n",
      "2016-03-07T19:39:10.016930: step 6200, loss 0.000697608, acc 1\n",
      "2016-03-07T19:39:10.020575: step 6200, loss 3.84603, acc 0\n",
      "2016-03-07T19:39:10.023934: step 6200, loss 3.09944e-06, acc 1\n",
      "2016-03-07T19:39:10.027469: step 6200, loss 0.000650909, acc 1\n",
      "2016-03-07T19:39:10.031401: step 6200, loss 0.0154427, acc 1\n",
      "2016-03-07T19:39:10.035830: step 6200, loss 9.53629e-05, acc 1\n",
      "2016-03-07T19:39:10.039152: step 6200, loss 6.6159e-05, acc 1\n",
      "2016-03-07T19:39:10.042664: step 6200, loss 3.99343e-05, acc 1\n",
      "2016-03-07T19:39:10.046245: step 6200, loss 8.61643, acc 0\n",
      "2016-03-07T19:39:10.050055: step 6200, loss 0.000579547, acc 1\n",
      "2016-03-07T19:39:10.053643: step 6200, loss 2.03846e-05, acc 1\n",
      "2016-03-07T19:39:10.057818: step 6200, loss 0.00188019, acc 1\n",
      "2016-03-07T19:39:10.061236: step 6200, loss 9.61973e-05, acc 1\n",
      "2016-03-07T19:39:10.064691: step 6200, loss 5.59076e-05, acc 1\n",
      "2016-03-07T19:39:10.067994: step 6200, loss 4.99474e-05, acc 1\n",
      "2016-03-07T19:39:10.072102: step 6200, loss 0.00023553, acc 1\n",
      "2016-03-07T19:39:10.075730: step 6200, loss 0.0517998, acc 1\n",
      "2016-03-07T19:39:10.080597: step 6200, loss 6.84238e-05, acc 1\n",
      "2016-03-07T19:39:10.084106: step 6200, loss 0.000636852, acc 1\n",
      "2016-03-07T19:39:10.087455: step 6200, loss 0.00159173, acc 1\n",
      "2016-03-07T19:39:10.092510: step 6200, loss 0.00211432, acc 1\n",
      "2016-03-07T19:39:10.097173: step 6200, loss 0.131305, acc 1\n",
      "2016-03-07T19:39:10.100246: step 6200, loss 10.3276, acc 0\n",
      "2016-03-07T19:39:10.103684: step 6200, loss 0.00150639, acc 1\n",
      "2016-03-07T19:39:10.108219: step 6200, loss 0.0046162, acc 1\n",
      "2016-03-07T19:39:10.112558: step 6200, loss 4.64915e-06, acc 1\n",
      "2016-03-07T19:39:10.116177: step 6200, loss 1.28745e-05, acc 1\n",
      "2016-03-07T19:39:10.119579: step 6200, loss 0.000395577, acc 1\n",
      "2016-03-07T19:39:10.123426: step 6200, loss 0.000141253, acc 1\n",
      "2016-03-07T19:39:10.127018: step 6200, loss 0.000450748, acc 1\n",
      "2016-03-07T19:39:10.130193: step 6200, loss 6.54438e-05, acc 1\n",
      "2016-03-07T19:39:10.133607: step 6200, loss 0.00281902, acc 1\n",
      "2016-03-07T19:39:10.137149: step 6200, loss 0.000120513, acc 1\n",
      "2016-03-07T19:39:10.140593: step 6200, loss 0.000624223, acc 1\n",
      "2016-03-07T19:39:10.144130: step 6200, loss 8.71742, acc 0\n",
      "2016-03-07T19:39:10.147412: step 6200, loss 0.105334, acc 1\n",
      "2016-03-07T19:39:10.150966: step 6200, loss 9.41709e-05, acc 1\n",
      "2016-03-07T19:39:10.154380: step 6200, loss 4.81594e-05, acc 1\n",
      "2016-03-07T19:39:10.158097: step 6200, loss 0.000407851, acc 1\n",
      "2016-03-07T19:39:10.161953: step 6200, loss 0.000285942, acc 1\n",
      "2016-03-07T19:39:10.166104: step 6200, loss 3.36165e-05, acc 1\n",
      "2016-03-07T19:39:10.169657: step 6200, loss 0.00102849, acc 1\n",
      "2016-03-07T19:39:10.172883: step 6200, loss 0.000102396, acc 1\n",
      "2016-03-07T19:39:10.176578: step 6200, loss 1.95501e-05, acc 1\n",
      "2016-03-07T19:39:10.180135: step 6200, loss 0.00507768, acc 1\n",
      "2016-03-07T19:39:10.183376: step 6200, loss 7.84717, acc 0\n",
      "2016-03-07T19:39:10.186563: step 6200, loss 0.00208458, acc 1\n",
      "2016-03-07T19:39:10.190026: step 6200, loss 8.73326, acc 0\n",
      "2016-03-07T19:39:10.193425: step 6200, loss 0.000297621, acc 1\n",
      "2016-03-07T19:39:10.197181: step 6200, loss 4.17224e-05, acc 1\n",
      "2016-03-07T19:39:10.200500: step 6200, loss 1.19209e-06, acc 1\n",
      "2016-03-07T19:39:10.204423: step 6200, loss 0.000189406, acc 1\n",
      "2016-03-07T19:39:10.208221: step 6200, loss 0.00259135, acc 1\n",
      "2016-03-07T19:39:10.213375: step 6200, loss 7.99022, acc 0\n",
      "2016-03-07T19:39:10.218044: step 6200, loss 0.000436449, acc 1\n",
      "2016-03-07T19:39:10.221481: step 6200, loss 0.000339927, acc 1\n",
      "2016-03-07T19:39:10.225119: step 6200, loss 3.45706e-06, acc 1\n",
      "2016-03-07T19:39:10.228760: step 6200, loss 0.00439618, acc 1\n",
      "2016-03-07T19:39:10.232235: step 6200, loss 0.00316191, acc 1\n",
      "2016-03-07T19:39:10.235451: step 6200, loss 0.0101168, acc 1\n",
      "2016-03-07T19:39:10.239667: step 6200, loss 0.000314186, acc 1\n",
      "2016-03-07T19:39:10.242941: step 6200, loss 0.000445267, acc 1\n",
      "2016-03-07T19:39:10.246720: step 6200, loss 0.000520213, acc 1\n",
      "2016-03-07T19:39:10.250230: step 6200, loss 0.000708925, acc 1\n",
      "2016-03-07T19:39:10.253686: step 6200, loss 0.0211491, acc 1\n",
      "2016-03-07T19:39:10.257099: step 6200, loss 5.00666e-05, acc 1\n",
      "2016-03-07T19:39:10.260842: step 6200, loss 0.000420482, acc 1\n",
      "2016-03-07T19:39:10.264447: step 6200, loss 2.30071e-05, acc 1\n",
      "2016-03-07T19:39:10.268566: step 6200, loss 0.000248044, acc 1\n",
      "2016-03-07T19:39:10.272295: step 6200, loss 0.00019322, acc 1\n",
      "2016-03-07T19:39:10.275657: step 6200, loss 0.00319946, acc 1\n",
      "2016-03-07T19:39:10.279089: step 6200, loss 0.000985137, acc 1\n",
      "2016-03-07T19:39:10.282340: step 6200, loss 0.00555851, acc 1\n",
      "2016-03-07T19:39:10.285996: step 6200, loss 6.07966e-06, acc 1\n",
      "2016-03-07T19:39:10.289517: step 6200, loss 1.83581e-05, acc 1\n",
      "2016-03-07T19:39:10.294208: step 6200, loss 1.44242e-05, acc 1\n",
      "2016-03-07T19:39:10.297575: step 6200, loss 1.21593e-05, acc 1\n",
      "2016-03-07T19:39:10.301560: step 6200, loss 0.00114828, acc 1\n",
      "2016-03-07T19:39:10.305327: step 6200, loss 0.000407494, acc 1\n",
      "2016-03-07T19:39:10.308752: step 6200, loss 2.86102e-06, acc 1\n",
      "2016-03-07T19:39:10.313232: step 6200, loss 0.0442959, acc 1\n",
      "2016-03-07T19:39:10.316285: step 6200, loss 0.000446697, acc 1\n",
      "2016-03-07T19:39:10.320369: step 6200, loss 0.0147673, acc 1\n",
      "2016-03-07T19:39:10.324209: step 6200, loss 0.000411307, acc 1\n",
      "2016-03-07T19:39:10.327672: step 6200, loss 0.130133, acc 1\n",
      "2016-03-07T19:39:10.331549: step 6200, loss 0.00364817, acc 1\n",
      "2016-03-07T19:39:10.335295: step 6200, loss 0.000366263, acc 1\n",
      "2016-03-07T19:39:10.339032: step 6200, loss 9.38872, acc 0\n",
      "2016-03-07T19:39:10.342270: step 6200, loss 6.25829e-05, acc 1\n",
      "2016-03-07T19:39:10.347097: step 6200, loss 6.97135, acc 0\n",
      "2016-03-07T19:39:10.350550: step 6200, loss 0.00022099, acc 1\n",
      "2016-03-07T19:39:10.354407: step 6200, loss 0.000471719, acc 1\n",
      "2016-03-07T19:39:10.357956: step 6200, loss 0.000114434, acc 1\n",
      "2016-03-07T19:39:10.361605: step 6200, loss 2.14576e-06, acc 1\n",
      "2016-03-07T19:39:10.364891: step 6200, loss 5.3644e-06, acc 1\n",
      "2016-03-07T19:39:10.368497: step 6200, loss 1.00135e-05, acc 1\n",
      "2016-03-07T19:39:10.372486: step 6200, loss 0.000148047, acc 1\n",
      "2016-03-07T19:39:10.376224: step 6200, loss 0.000220513, acc 1\n",
      "2016-03-07T19:39:10.379456: step 6200, loss 0.0261472, acc 1\n",
      "2016-03-07T19:39:10.383938: step 6200, loss 0.0146162, acc 1\n",
      "2016-03-07T19:39:10.387782: step 6200, loss 2.64641e-05, acc 1\n",
      "2016-03-07T19:39:10.392496: step 6200, loss 0.000122778, acc 1\n",
      "2016-03-07T19:39:10.395562: step 6200, loss 8.65365, acc 0\n",
      "2016-03-07T19:39:10.399575: step 6200, loss 0.000367097, acc 1\n",
      "2016-03-07T19:39:10.403204: step 6200, loss 8.61846e-05, acc 1\n",
      "2016-03-07T19:39:10.406472: step 6200, loss 1.75236e-05, acc 1\n",
      "2016-03-07T19:39:10.409504: step 6200, loss 8.79612, acc 0\n",
      "2016-03-07T19:39:10.412415: step 6200, loss 2.6226e-06, acc 1\n",
      "2016-03-07T19:39:10.415854: step 6200, loss 8.35622e-05, acc 1\n",
      "2016-03-07T19:39:10.419574: step 6200, loss 0.000708925, acc 1\n",
      "2016-03-07T19:39:10.424037: step 6200, loss 3.89807e-05, acc 1\n",
      "2016-03-07T19:39:10.427774: step 6200, loss 0.00140009, acc 1\n",
      "2016-03-07T19:39:10.431473: step 6200, loss 0.000195961, acc 1\n",
      "2016-03-07T19:39:10.434717: step 6200, loss 3.18284e-05, acc 1\n",
      "2016-03-07T19:39:10.438385: step 6200, loss 6.44901e-05, acc 1\n",
      "2016-03-07T19:39:10.441797: step 6200, loss 7.37878e-05, acc 1\n",
      "2016-03-07T19:39:10.446305: step 6200, loss 0.000149835, acc 1\n",
      "2016-03-07T19:39:10.449661: step 6200, loss 5.12599e-06, acc 1\n",
      "2016-03-07T19:39:10.453251: step 6200, loss 0.00572612, acc 1\n",
      "2016-03-07T19:39:10.456512: step 6200, loss 5.47156e-05, acc 1\n",
      "2016-03-07T19:39:10.459867: step 6200, loss 5.09011e-05, acc 1\n",
      "2016-03-07T19:39:10.463140: step 6200, loss 0.00323725, acc 1\n",
      "2016-03-07T19:39:10.466958: step 6200, loss 0.0172449, acc 1\n",
      "2016-03-07T19:39:10.470675: step 6200, loss 0.0059244, acc 1\n",
      "2016-03-07T19:39:10.474917: step 6200, loss 0.000276408, acc 1\n",
      "2016-03-07T19:39:10.478706: step 6200, loss 9.6077, acc 0\n",
      "2016-03-07T19:39:10.481966: step 6200, loss 2.20535e-05, acc 1\n",
      "2016-03-07T19:39:10.485071: step 6200, loss 0.000166045, acc 1\n",
      "2016-03-07T19:39:10.488398: step 6200, loss 4.29153e-06, acc 1\n",
      "2016-03-07T19:39:10.492507: step 6200, loss 0.135201, acc 1\n",
      "2016-03-07T19:39:10.496392: step 6200, loss 0.00841027, acc 1\n",
      "2016-03-07T19:39:10.499807: step 6200, loss 2.99211e-05, acc 1\n",
      "2016-03-07T19:39:10.503487: step 6200, loss 7.31918e-05, acc 1\n",
      "2016-03-07T19:39:10.507159: step 6200, loss 0.000951314, acc 1\n",
      "2016-03-07T19:39:10.510611: step 6200, loss 0.000204542, acc 1\n",
      "2016-03-07T19:39:10.513858: step 6200, loss 0.0257728, acc 1\n",
      "2016-03-07T19:39:10.517229: step 6200, loss 5.75146, acc 0\n",
      "2016-03-07T19:39:10.520671: step 6200, loss 7.14038e-05, acc 1\n",
      "2016-03-07T19:39:10.524483: step 6200, loss 0.000286538, acc 1\n",
      "2016-03-07T19:39:10.528794: step 6200, loss 2.20535e-05, acc 1\n",
      "2016-03-07T19:39:10.532703: step 6200, loss 7.84366e-05, acc 1\n",
      "2016-03-07T19:39:10.535676: step 6200, loss 9.52627, acc 0\n",
      "2016-03-07T19:39:10.539176: step 6200, loss 0.000514018, acc 1\n",
      "2016-03-07T19:39:10.542475: step 6200, loss 0.0699769, acc 1\n",
      "2016-03-07T19:39:10.546656: step 6200, loss 0.00278062, acc 1\n",
      "2016-03-07T19:39:10.550710: step 6200, loss 3.02599, acc 0\n",
      "2016-03-07T19:39:10.554204: step 6200, loss 0.000152695, acc 1\n",
      "2016-03-07T19:39:10.557989: step 6200, loss 1.89541e-05, acc 1\n",
      "2016-03-07T19:39:10.561846: step 6200, loss 0.435296, acc 1\n",
      "2016-03-07T19:39:10.565490: step 6200, loss 0.0100255, acc 1\n",
      "2016-03-07T19:39:10.570460: step 6200, loss 5.23315e-05, acc 1\n",
      "2016-03-07T19:39:10.574117: step 6200, loss 0.000212051, acc 1\n",
      "2016-03-07T19:39:10.579033: step 6200, loss 2.39608e-05, acc 1\n",
      "2016-03-07T19:39:10.583943: step 6200, loss 0.0024577, acc 1\n",
      "2016-03-07T19:39:10.587496: step 6200, loss 0.00104611, acc 1\n",
      "2016-03-07T19:39:10.591382: step 6200, loss 9.10314, acc 0\n",
      "2016-03-07T19:39:10.594993: step 6200, loss 0.000195007, acc 1\n",
      "2016-03-07T19:39:10.598577: step 6200, loss 0.882846, acc 0\n",
      "2016-03-07T19:39:10.601936: step 6200, loss 0.000405468, acc 1\n",
      "2016-03-07T19:39:10.605355: step 6200, loss 0.000386402, acc 1\n",
      "2016-03-07T19:39:10.609327: step 6200, loss 9.63927, acc 0\n",
      "2016-03-07T19:39:10.612434: step 6200, loss 12.0021, acc 0\n",
      "2016-03-07T19:39:10.616103: step 6200, loss 7.08053, acc 0\n",
      "2016-03-07T19:39:10.619816: step 6200, loss 0.000513541, acc 1\n",
      "2016-03-07T19:39:10.623046: step 6200, loss 0.000679381, acc 1\n",
      "2016-03-07T19:39:10.626633: step 6200, loss 2.5272e-05, acc 1\n",
      "2016-03-07T19:39:10.630526: step 6200, loss 7.51016e-06, acc 1\n",
      "2016-03-07T19:39:10.635405: step 6200, loss 0.00023982, acc 1\n",
      "2016-03-07T19:39:10.638823: step 6200, loss 3.57506, acc 0\n",
      "2016-03-07T19:39:10.642406: step 6200, loss 5.2452e-06, acc 1\n",
      "2016-03-07T19:39:10.646382: step 6200, loss 0.000608855, acc 1\n",
      "2016-03-07T19:39:10.649714: step 6200, loss 0.00735397, acc 1\n",
      "2016-03-07T19:39:10.653566: step 6200, loss 0.000600515, acc 1\n",
      "2016-03-07T19:39:10.656880: step 6200, loss 0.00252452, acc 1\n",
      "2016-03-07T19:39:10.660338: step 6200, loss 0.000109071, acc 1\n",
      "2016-03-07T19:39:10.664052: step 6200, loss 0.00214858, acc 1\n",
      "2016-03-07T19:39:10.667585: step 6200, loss 0.00253939, acc 1\n",
      "2016-03-07T19:39:10.671743: step 6200, loss 2.74181e-06, acc 1\n",
      "2016-03-07T19:39:10.675312: step 6200, loss 0.00285991, acc 1\n",
      "2016-03-07T19:39:10.679082: step 6200, loss 0.00771945, acc 1\n",
      "2016-03-07T19:39:10.682402: step 6200, loss 0.000108117, acc 1\n",
      "2016-03-07T19:39:10.687338: step 6200, loss 2.67025e-05, acc 1\n",
      "2016-03-07T19:39:10.690930: step 6200, loss 2.21727e-05, acc 1\n",
      "2016-03-07T19:39:10.694476: step 6200, loss 3.24244e-05, acc 1\n",
      "2016-03-07T19:39:10.699353: step 6200, loss 0.000125639, acc 1\n",
      "2016-03-07T19:39:10.702935: step 6200, loss 0.000389858, acc 1\n",
      "2016-03-07T19:39:10.706670: step 6200, loss 14.4332, acc 0\n",
      "2016-03-07T19:39:10.710096: step 6200, loss 7.43838e-05, acc 1\n",
      "2016-03-07T19:39:10.714505: step 6200, loss 11.5751, acc 0\n",
      "2016-03-07T19:39:10.718374: step 6200, loss 2.72986e-05, acc 1\n",
      "2016-03-07T19:39:10.723035: step 6200, loss 1.27553e-05, acc 1\n",
      "2016-03-07T19:39:10.727571: step 6200, loss 0.000199775, acc 1\n",
      "2016-03-07T19:39:10.730881: step 6200, loss 0.000224208, acc 1\n",
      "2016-03-07T19:39:10.734630: step 6200, loss 0.000265325, acc 1\n",
      "2016-03-07T19:39:10.738733: step 6200, loss 3.95767e-05, acc 1\n",
      "2016-03-07T19:39:10.742458: step 6200, loss 2.92059e-05, acc 1\n",
      "2016-03-07T19:39:10.746103: step 6200, loss 8.94066e-06, acc 1\n",
      "2016-03-07T19:39:10.749437: step 6200, loss 0.0273529, acc 1\n",
      "2016-03-07T19:39:10.752882: step 6200, loss 0.000346958, acc 1\n",
      "2016-03-07T19:39:10.756296: step 6200, loss 2.38416e-05, acc 1\n",
      "2016-03-07T19:39:10.759940: step 6200, loss 7.9867e-05, acc 1\n",
      "2016-03-07T19:39:10.763587: step 6200, loss 5.59076e-05, acc 1\n",
      "2016-03-07T19:39:10.766672: step 6200, loss 0.000357445, acc 1\n",
      "2016-03-07T19:39:10.770370: step 6200, loss 11.6112, acc 0\n",
      "2016-03-07T19:39:10.773639: step 6200, loss 0.00122234, acc 1\n",
      "2016-03-07T19:39:10.778621: step 6200, loss 0.0027528, acc 1\n",
      "2016-03-07T19:39:10.781962: step 6200, loss 9.28011, acc 0\n",
      "2016-03-07T19:39:10.785044: step 6200, loss 1.657e-05, acc 1\n",
      "2016-03-07T19:39:10.788369: step 6200, loss 5.44772e-05, acc 1\n",
      "2016-03-07T19:39:10.792603: step 6200, loss 0.000627678, acc 1\n",
      "2016-03-07T19:39:10.796626: step 6200, loss 6.97567, acc 0\n",
      "2016-03-07T19:39:10.800841: step 6200, loss 0.00120567, acc 1\n",
      "2016-03-07T19:39:10.804556: step 6200, loss 5.59076e-05, acc 1\n",
      "2016-03-07T19:39:10.808139: step 6200, loss 0.000980612, acc 1\n",
      "2016-03-07T19:39:10.811259: step 6200, loss 2.83714e-05, acc 1\n",
      "2016-03-07T19:39:10.814574: step 6200, loss 0.0191759, acc 1\n",
      "2016-03-07T19:39:10.818609: step 6200, loss 10.1898, acc 0\n",
      "2016-03-07T19:39:10.822206: step 6200, loss 0.0622627, acc 1\n",
      "2016-03-07T19:39:10.825787: step 6200, loss 0.00059432, acc 1\n",
      "2016-03-07T19:39:10.829584: step 6200, loss 5.18189, acc 0\n",
      "2016-03-07T19:39:10.833057: step 6200, loss 4.39873e-05, acc 1\n",
      "2016-03-07T19:39:10.836433: step 6200, loss 1.13248e-05, acc 1\n",
      "2016-03-07T19:39:10.839764: step 6200, loss 4.41065e-05, acc 1\n",
      "2016-03-07T19:39:10.844352: step 6200, loss 0.000429419, acc 1\n",
      "2016-03-07T19:39:10.847783: step 6200, loss 0.000132433, acc 1\n",
      "2016-03-07T19:39:10.851661: step 6200, loss 0.000107521, acc 1\n",
      "2016-03-07T19:39:10.855199: step 6200, loss 0.00197192, acc 1\n",
      "2016-03-07T19:39:10.858252: step 6200, loss 0.00182557, acc 1\n",
      "2016-03-07T19:39:10.861749: step 6200, loss 0.015858, acc 1\n",
      "2016-03-07T19:39:10.865146: step 6200, loss 8.1062e-06, acc 1\n",
      "2016-03-07T19:39:10.868754: step 6200, loss 3.39741e-05, acc 1\n",
      "2016-03-07T19:39:10.872583: step 6200, loss 0.045948, acc 1\n",
      "2016-03-07T19:39:10.876079: step 6200, loss 0.00784199, acc 1\n",
      "2016-03-07T19:39:10.879851: step 6200, loss 0.00073537, acc 1\n",
      "2016-03-07T19:39:10.883501: step 6200, loss 0.0190085, acc 1\n",
      "2016-03-07T19:39:10.887292: step 6200, loss 11.3733, acc 0\n",
      "2016-03-07T19:39:10.890867: step 6200, loss 0.0256848, acc 1\n",
      "2016-03-07T19:39:10.895030: step 6200, loss 3.09939e-05, acc 1\n",
      "2016-03-07T19:39:10.898555: step 6200, loss 2.0623e-05, acc 1\n",
      "2016-03-07T19:39:10.903474: step 6200, loss 10.7713, acc 0\n",
      "2016-03-07T19:39:10.907045: step 6200, loss 0.00188804, acc 1\n",
      "2016-03-07T19:39:10.910500: step 6200, loss 7.78509, acc 0\n",
      "2016-03-07T19:39:10.914267: step 6200, loss 0.000222658, acc 1\n",
      "2016-03-07T19:39:10.917900: step 6200, loss 0.000158059, acc 1\n",
      "2016-03-07T19:39:10.921278: step 6200, loss 3.159e-05, acc 1\n",
      "2016-03-07T19:39:10.924970: step 6200, loss 0.000141134, acc 1\n",
      "2016-03-07T19:39:10.928804: step 6200, loss 0.000550714, acc 1\n",
      "2016-03-07T19:39:10.932188: step 6200, loss 3.73272, acc 0\n",
      "2016-03-07T19:39:10.935311: step 6200, loss 4.92322e-05, acc 1\n",
      "2016-03-07T19:39:10.939404: step 6200, loss 0.000504962, acc 1\n",
      "2016-03-07T19:39:10.943611: step 6200, loss 0.0286163, acc 1\n",
      "2016-03-07T19:39:10.949983: step 6200, loss 5.35235e-05, acc 1\n",
      "2016-03-07T19:39:10.953622: step 6200, loss 0.00186305, acc 1\n",
      "2016-03-07T19:39:10.957220: step 6200, loss 9.50053e-05, acc 1\n",
      "2016-03-07T19:39:10.960843: step 6200, loss 0.000971441, acc 1\n",
      "2016-03-07T19:39:10.964188: step 6200, loss 0.000108355, acc 1\n",
      "2016-03-07T19:39:10.967978: step 6200, loss 5.91261e-05, acc 1\n",
      "2016-03-07T19:39:10.971024: step 6200, loss 0.00219236, acc 1\n",
      "2016-03-07T19:39:10.974993: step 6200, loss 5.24507e-05, acc 1\n",
      "2016-03-07T19:39:10.978353: step 6200, loss 1.19209e-05, acc 1\n",
      "2016-03-07T19:39:10.981909: step 6200, loss 2.14576e-06, acc 1\n",
      "2016-03-07T19:39:10.985316: step 6200, loss 0.00017117, acc 1\n",
      "2016-03-07T19:39:10.989342: step 6200, loss 0.000640664, acc 1\n",
      "2016-03-07T19:39:10.993386: step 6200, loss 0.000102038, acc 1\n",
      "2016-03-07T19:39:10.997163: step 6200, loss 0.00027462, acc 1\n",
      "2016-03-07T19:39:11.001085: step 6200, loss 3.69548e-06, acc 1\n",
      "2016-03-07T19:39:11.004733: step 6200, loss 0.00230761, acc 1\n",
      "2016-03-07T19:39:11.008044: step 6200, loss 0.00318318, acc 1\n",
      "2016-03-07T19:39:11.011992: step 6200, loss 0.00385399, acc 1\n",
      "2016-03-07T19:39:11.015607: step 6200, loss 0.00041083, acc 1\n",
      "2016-03-07T19:39:11.018890: step 6200, loss 0.00561079, acc 1\n",
      "2016-03-07T19:39:11.022292: step 6200, loss 1.31129e-05, acc 1\n",
      "2016-03-07T19:39:11.025883: step 6200, loss 0.00769165, acc 1\n",
      "2016-03-07T19:39:11.029693: step 6200, loss 0.00243843, acc 1\n",
      "2016-03-07T19:39:11.032896: step 6200, loss 0.000683432, acc 1\n",
      "2016-03-07T19:39:11.037583: step 6200, loss 0.000207403, acc 1\n",
      "2016-03-07T19:39:11.041131: step 6200, loss 0.000237198, acc 1\n",
      "2016-03-07T19:39:11.044810: step 6200, loss 5.05435e-05, acc 1\n",
      "2016-03-07T19:39:11.048257: step 6200, loss 6.81854e-05, acc 1\n",
      "2016-03-07T19:39:11.052516: step 6200, loss 0.00244188, acc 1\n",
      "2016-03-07T19:39:11.055979: step 6200, loss 6.38422, acc 0\n",
      "2016-03-07T19:39:11.059812: step 6200, loss 1.34706e-05, acc 1\n",
      "2016-03-07T19:39:11.063928: step 6200, loss 6.00797e-05, acc 1\n",
      "2016-03-07T19:39:11.067567: step 6200, loss 8.59462e-05, acc 1\n",
      "2016-03-07T19:39:11.071000: step 6200, loss 7.02118e-05, acc 1\n",
      "2016-03-07T19:39:11.074677: step 6200, loss 6.79491e-06, acc 1\n",
      "2016-03-07T19:39:11.078138: step 6200, loss 0.000230047, acc 1\n",
      "2016-03-07T19:39:11.081520: step 6200, loss 0.0142806, acc 1\n",
      "2016-03-07T19:39:11.084631: step 6200, loss 4.52994e-06, acc 1\n",
      "2016-03-07T19:39:11.088177: step 6200, loss 0.00699535, acc 1\n",
      "2016-03-07T19:39:11.091899: step 6200, loss 0.0510896, acc 1\n",
      "2016-03-07T19:39:11.095390: step 6200, loss 3.90999e-05, acc 1\n",
      "2016-03-07T19:39:11.099164: step 6200, loss 0.00115983, acc 1\n",
      "2016-03-07T19:39:11.103322: step 6200, loss 0.0120742, acc 1\n",
      "2016-03-07T19:39:11.106842: step 6200, loss 0.000259127, acc 1\n",
      "2016-03-07T19:39:11.110437: step 6200, loss 0.00197299, acc 1\n",
      "2016-03-07T19:39:11.114169: step 6200, loss 0.000113481, acc 1\n",
      "2016-03-07T19:39:11.118026: step 6200, loss 0.0307982, acc 1\n",
      "2016-03-07T19:39:11.121840: step 6200, loss 9.33701, acc 0\n",
      "2016-03-07T19:39:11.125151: step 6200, loss 0.000207283, acc 1\n",
      "2016-03-07T19:39:11.128406: step 6200, loss 2.59873e-05, acc 1\n",
      "2016-03-07T19:39:11.131543: step 6200, loss 0.000111216, acc 1\n",
      "2016-03-07T19:39:11.134945: step 6200, loss 0.000152338, acc 1\n",
      "2016-03-07T19:39:11.138773: step 6200, loss 0.00357798, acc 1\n",
      "2016-03-07T19:39:11.142485: step 6200, loss 8.9403e-05, acc 1\n",
      "2016-03-07T19:39:11.146279: step 6200, loss 8.96414e-05, acc 1\n",
      "2016-03-07T19:39:11.149457: step 6200, loss 4.78018e-05, acc 1\n",
      "2016-03-07T19:39:11.152968: step 6200, loss 0.00360743, acc 1\n",
      "2016-03-07T19:39:11.157319: step 6200, loss 3.16602, acc 0\n",
      "2016-03-07T19:39:11.161216: step 6200, loss 10.3917, acc 0\n",
      "2016-03-07T19:39:11.164565: step 6200, loss 10.3006, acc 0\n",
      "2016-03-07T19:39:11.168344: step 6200, loss 2.02656e-06, acc 1\n",
      "2016-03-07T19:39:11.171814: step 6200, loss 3.71926e-05, acc 1\n",
      "2016-03-07T19:39:11.175045: step 6200, loss 0.00027331, acc 1\n",
      "2016-03-07T19:39:11.178389: step 6200, loss 2.53913e-05, acc 1\n",
      "2016-03-07T19:39:11.182066: step 6200, loss 0.00112744, acc 1\n",
      "2016-03-07T19:39:11.185761: step 6200, loss 0.00925453, acc 1\n",
      "2016-03-07T19:39:11.189353: step 6200, loss 0.114626, acc 1\n",
      "2016-03-07T19:39:11.192899: step 6200, loss 0.452497, acc 1\n",
      "2016-03-07T19:39:11.196468: step 6200, loss 0.117266, acc 1\n",
      "2016-03-07T19:39:11.199968: step 6200, loss 2.59873e-05, acc 1\n",
      "2016-03-07T19:39:11.203516: step 6200, loss 0.000106329, acc 1\n",
      "2016-03-07T19:39:11.207134: step 6200, loss 8.12772, acc 0\n",
      "2016-03-07T19:39:11.211292: step 6200, loss 7.26693, acc 0\n",
      "2016-03-07T19:39:11.216266: step 6200, loss 0.000136128, acc 1\n",
      "2016-03-07T19:39:11.220376: step 6200, loss 0.000389143, acc 1\n",
      "2016-03-07T19:39:11.224352: step 6200, loss 0.000671281, acc 1\n",
      "2016-03-07T19:39:11.227635: step 6200, loss 0.000264371, acc 1\n",
      "2016-03-07T19:39:11.231177: step 6200, loss 0.00994737, acc 1\n",
      "2016-03-07T19:39:11.234349: step 6200, loss 2.38418e-06, acc 1\n",
      "2016-03-07T19:39:11.237890: step 6200, loss 0.00711941, acc 1\n",
      "2016-03-07T19:39:11.241410: step 6200, loss 0.000100131, acc 1\n",
      "2016-03-07T19:39:11.244879: step 6200, loss 0.000313829, acc 1\n",
      "2016-03-07T19:39:11.247767: step 6200, loss 0.000133744, acc 1\n",
      "2016-03-07T19:39:11.251278: step 6200, loss 2.90866e-05, acc 1\n",
      "2016-03-07T19:39:11.254641: step 6200, loss 0.000208356, acc 1\n",
      "2016-03-07T19:39:11.258446: step 6200, loss 0.000222181, acc 1\n",
      "2016-03-07T19:39:11.262691: step 6200, loss 0.000671042, acc 1\n",
      "2016-03-07T19:39:11.266432: step 6200, loss 1.68084e-05, acc 1\n",
      "2016-03-07T19:39:11.270032: step 6200, loss 1.09672e-05, acc 1\n",
      "2016-03-07T19:39:11.273064: step 6200, loss 0.000370196, acc 1\n",
      "2016-03-07T19:39:11.276472: step 6200, loss 7.27174e-06, acc 1\n",
      "2016-03-07T19:39:11.279995: step 6200, loss 0.00010621, acc 1\n",
      "2016-03-07T19:39:11.283853: step 6200, loss 0.000977396, acc 1\n",
      "2016-03-07T19:39:11.287169: step 6200, loss 0.00146806, acc 1\n",
      "2016-03-07T19:39:11.290893: step 6200, loss 9.67933e-05, acc 1\n",
      "2016-03-07T19:39:11.294437: step 6200, loss 0.000170574, acc 1\n",
      "2016-03-07T19:39:11.297723: step 6200, loss 0.000378418, acc 1\n",
      "2016-03-07T19:39:11.300986: step 6200, loss 0.0145289, acc 1\n",
      "2016-03-07T19:39:11.304275: step 6200, loss 3.48085e-05, acc 1\n",
      "2016-03-07T19:39:11.308218: step 6200, loss 11.4693, acc 0\n",
      "2016-03-07T19:39:11.311965: step 6200, loss 7.6887e-05, acc 1\n",
      "2016-03-07T19:39:11.316590: step 6200, loss 0.273872, acc 1\n",
      "2016-03-07T19:39:11.320588: step 6200, loss 0.00258005, acc 1\n",
      "2016-03-07T19:39:11.324107: step 6200, loss 5.12599e-06, acc 1\n",
      "2016-03-07T19:39:11.327877: step 6200, loss 0.000254003, acc 1\n",
      "2016-03-07T19:39:11.331492: step 6200, loss 7.9805, acc 0\n",
      "2016-03-07T19:39:11.335323: step 6200, loss 0.000351963, acc 1\n",
      "2016-03-07T19:39:11.338831: step 6200, loss 0.101047, acc 1\n",
      "2016-03-07T19:39:11.342382: step 6200, loss 8.01054e-05, acc 1\n",
      "2016-03-07T19:39:11.345723: step 6200, loss 0.000357206, acc 1\n",
      "2016-03-07T19:39:11.348921: step 6200, loss 0.000882951, acc 1\n",
      "2016-03-07T19:39:11.352665: step 6200, loss 0.000881879, acc 1\n",
      "2016-03-07T19:39:11.356844: step 6200, loss 1.39474e-05, acc 1\n",
      "2016-03-07T19:39:11.360795: step 6200, loss 5.75764e-05, acc 1\n",
      "2016-03-07T19:39:11.364404: step 6200, loss 2.27687e-05, acc 1\n",
      "2016-03-07T19:39:11.368340: step 6200, loss 0.00010168, acc 1\n",
      "2016-03-07T19:39:11.371671: step 6200, loss 12.6878, acc 0\n",
      "2016-03-07T19:39:11.375263: step 6200, loss 0.112947, acc 1\n",
      "2016-03-07T19:39:11.378755: step 6200, loss 0.000280103, acc 1\n",
      "2016-03-07T19:39:11.382177: step 6200, loss 9.11737, acc 0\n",
      "2016-03-07T19:39:11.385986: step 6200, loss 0.00031085, acc 1\n",
      "2016-03-07T19:39:11.389395: step 6200, loss 3.21865e-06, acc 1\n",
      "2016-03-07T19:39:11.393197: step 6200, loss 0.0385569, acc 1\n",
      "2016-03-07T19:39:11.396056: step 6200, loss 4.87348, acc 0\n",
      "2016-03-07T19:39:11.399749: step 6200, loss 0.000288683, acc 1\n",
      "2016-03-07T19:39:11.403066: step 6200, loss 8.46382e-06, acc 1\n",
      "2016-03-07T19:39:11.406715: step 6200, loss 2.33647e-05, acc 1\n",
      "2016-03-07T19:39:11.409776: step 6200, loss 0.000562033, acc 1\n",
      "2016-03-07T19:39:11.413726: step 6200, loss 0.0190797, acc 1\n",
      "2016-03-07T19:39:11.417357: step 6200, loss 9.53842, acc 0\n",
      "2016-03-07T19:39:11.421396: step 6200, loss 5.96046e-07, acc 1\n",
      "2016-03-07T19:39:11.424993: step 6200, loss 7.58692, acc 0\n",
      "2016-03-07T19:39:11.428654: step 6200, loss 4.17224e-05, acc 1\n",
      "2016-03-07T19:39:11.432026: step 6200, loss 4.88757e-06, acc 1\n",
      "2016-03-07T19:39:11.435209: step 6200, loss 0.00022826, acc 1\n",
      "2016-03-07T19:39:11.438813: step 6200, loss 0.000101084, acc 1\n",
      "2016-03-07T19:39:11.442154: step 6200, loss 1.1444e-05, acc 1\n",
      "2016-03-07T19:39:11.445424: step 6200, loss 1.41858e-05, acc 1\n",
      "2016-03-07T19:39:11.449035: step 6200, loss 0.00305745, acc 1\n",
      "2016-03-07T19:39:11.452333: step 6200, loss 5.12599e-06, acc 1\n",
      "2016-03-07T19:39:11.455813: step 6200, loss 1.657e-05, acc 1\n",
      "2016-03-07T19:39:11.459251: step 6200, loss 5.78148e-05, acc 1\n",
      "2016-03-07T19:39:11.462723: step 6200, loss 3.69542e-05, acc 1\n",
      "2016-03-07T19:39:11.466417: step 6200, loss 2.8583, acc 0\n",
      "2016-03-07T19:39:11.470243: step 6200, loss 1.80004e-05, acc 1\n",
      "2016-03-07T19:39:11.474701: step 6200, loss 0.00547801, acc 1\n",
      "2016-03-07T19:39:11.478030: step 6200, loss 0.000119441, acc 1\n",
      "2016-03-07T19:39:11.481657: step 6200, loss 0.00033802, acc 1\n",
      "2016-03-07T19:39:11.486924: step 6200, loss 0.000608021, acc 1\n",
      "2016-03-07T19:39:11.490141: step 6200, loss 4.19608e-05, acc 1\n",
      "2016-03-07T19:39:11.493514: step 6200, loss 7.65212, acc 0\n",
      "2016-03-07T19:39:11.497306: step 6200, loss 6.74519, acc 0\n",
      "2016-03-07T19:39:11.501196: step 6200, loss 1.49011e-05, acc 1\n",
      "2016-03-07T19:39:11.504850: step 6200, loss 0.085512, acc 1\n",
      "2016-03-07T19:39:11.508215: step 6200, loss 3.92191e-05, acc 1\n",
      "2016-03-07T19:39:11.511467: step 6200, loss 1.01327e-05, acc 1\n",
      "2016-03-07T19:39:11.514682: step 6200, loss 0.000107163, acc 1\n",
      "2016-03-07T19:39:11.517995: step 6200, loss 3.37357e-05, acc 1\n",
      "2016-03-07T19:39:11.521075: step 6200, loss 10.8704, acc 0\n",
      "2016-03-07T19:39:11.526131: step 6200, loss 2.32455e-05, acc 1\n",
      "2016-03-07T19:39:11.530213: step 6200, loss 0.0293516, acc 1\n",
      "2016-03-07T19:39:11.533771: step 6200, loss 2.05038e-05, acc 1\n",
      "2016-03-07T19:39:11.537470: step 6200, loss 0.00158006, acc 1\n",
      "2016-03-07T19:39:11.541978: step 6200, loss 10.9191, acc 0\n",
      "2016-03-07T19:39:11.545381: step 6200, loss 6.19869e-05, acc 1\n",
      "2016-03-07T19:39:11.548599: step 6200, loss 0.000374009, acc 1\n",
      "2016-03-07T19:39:11.552330: step 6200, loss 3.24983, acc 0\n",
      "2016-03-07T19:39:11.555892: step 6200, loss 0.000187737, acc 1\n",
      "2016-03-07T19:39:11.559363: step 6200, loss 0.000274859, acc 1\n",
      "2016-03-07T19:39:11.563079: step 6200, loss 0.000317166, acc 1\n",
      "2016-03-07T19:39:11.566677: step 6200, loss 1.97885e-05, acc 1\n",
      "2016-03-07T19:39:11.569792: step 6200, loss 2.02656e-06, acc 1\n",
      "2016-03-07T19:39:11.573719: step 6200, loss 9.29828e-06, acc 1\n",
      "2016-03-07T19:39:11.578149: step 6200, loss 0.000132075, acc 1\n",
      "2016-03-07T19:39:11.581458: step 6200, loss 9.21445e-05, acc 1\n",
      "2016-03-07T19:39:11.585380: step 6200, loss 0.00014173, acc 1\n",
      "2016-03-07T19:39:11.589252: step 6200, loss 0.000390096, acc 1\n",
      "2016-03-07T19:39:11.592958: step 6200, loss 0.00154709, acc 1\n",
      "2016-03-07T19:39:11.597051: step 6200, loss 8.216, acc 0\n",
      "2016-03-07T19:39:11.600720: step 6200, loss 7.37622, acc 0\n",
      "2016-03-07T19:39:11.604166: step 6200, loss 6.20218, acc 0\n",
      "2016-03-07T19:39:11.607449: step 6200, loss 0.000217891, acc 1\n",
      "2016-03-07T19:39:11.610700: step 6200, loss 1.54971e-05, acc 1\n",
      "2016-03-07T19:39:11.614396: step 6200, loss 8.81895, acc 0\n",
      "2016-03-07T19:39:11.619244: step 6200, loss 0.00161708, acc 1\n",
      "2016-03-07T19:39:11.622264: step 6200, loss 0.000206807, acc 1\n",
      "2016-03-07T19:39:11.626127: step 6200, loss 12.0998, acc 0\n",
      "2016-03-07T19:39:11.630426: step 6200, loss 3.159e-05, acc 1\n",
      "2016-03-07T19:39:11.634399: step 6200, loss 8.78534e-05, acc 1\n",
      "2016-03-07T19:39:11.637840: step 6200, loss 0.000290233, acc 1\n",
      "2016-03-07T19:39:11.641419: step 6200, loss 0.000292974, acc 1\n",
      "2016-03-07T19:39:11.646162: step 6200, loss 2.43184e-05, acc 1\n",
      "2016-03-07T19:39:11.649578: step 6200, loss 0.000102276, acc 1\n",
      "2016-03-07T19:39:11.654047: step 6200, loss 0.00033373, acc 1\n",
      "2016-03-07T19:39:11.657702: step 6200, loss 4.61209, acc 0\n",
      "2016-03-07T19:39:11.661730: step 6200, loss 0.000587887, acc 1\n",
      "2016-03-07T19:39:11.665071: step 6200, loss 0.00424508, acc 1\n",
      "2016-03-07T19:39:11.668327: step 6200, loss 6.5563e-05, acc 1\n",
      "2016-03-07T19:39:11.671713: step 6200, loss 0.00125973, acc 1\n",
      "2016-03-07T19:39:11.675009: step 6200, loss 4.23184e-05, acc 1\n",
      "2016-03-07T19:39:11.678579: step 6200, loss 1.16824e-05, acc 1\n",
      "2016-03-07T19:39:11.682849: step 6200, loss 7.36214, acc 0\n",
      "2016-03-07T19:39:11.686890: step 6200, loss 6.94966e-05, acc 1\n",
      "2016-03-07T19:39:11.691113: step 6200, loss 0.0307413, acc 1\n",
      "2016-03-07T19:39:11.694337: step 6200, loss 0.00282508, acc 1\n",
      "2016-03-07T19:39:11.697881: step 6200, loss 7.5695e-05, acc 1\n",
      "2016-03-07T19:39:11.701341: step 6200, loss 2.80129, acc 0\n",
      "2016-03-07T19:39:11.704692: step 6200, loss 0.000145186, acc 1\n",
      "2016-03-07T19:39:11.708240: step 6200, loss 0.000150073, acc 1\n",
      "2016-03-07T19:39:11.711507: step 6200, loss 0.000255195, acc 1\n",
      "2016-03-07T19:39:11.715611: step 6200, loss 11.1798, acc 0\n",
      "2016-03-07T19:39:11.719381: step 6200, loss 7.84366e-05, acc 1\n",
      "2016-03-07T19:39:11.723411: step 6200, loss 2.38418e-06, acc 1\n",
      "2016-03-07T19:39:11.727023: step 6200, loss 0.0417251, acc 1\n",
      "2016-03-07T19:39:11.730384: step 6200, loss 0.000278434, acc 1\n",
      "2016-03-07T19:39:11.734572: step 6200, loss 1.25169e-05, acc 1\n",
      "2016-03-07T19:39:11.737968: step 6200, loss 0.000218844, acc 1\n",
      "2016-03-07T19:39:11.741104: step 6200, loss 0.000409162, acc 1\n",
      "2016-03-07T19:39:11.744375: step 6200, loss 9.07142e-05, acc 1\n",
      "2016-03-07T19:39:11.747861: step 6200, loss 0.000441454, acc 1\n",
      "2016-03-07T19:39:11.751161: step 6200, loss 0.000257936, acc 1\n",
      "2016-03-07T19:39:11.754732: step 6200, loss 0.000339689, acc 1\n",
      "2016-03-07T19:39:11.758072: step 6200, loss 0.00016819, acc 1\n",
      "2016-03-07T19:39:11.762398: step 6200, loss 3.48085e-05, acc 1\n",
      "2016-03-07T19:39:11.765969: step 6200, loss 0.000703802, acc 1\n",
      "2016-03-07T19:39:11.769362: step 6200, loss 0.000232908, acc 1\n",
      "2016-03-07T19:39:11.772994: step 6200, loss 0.000178679, acc 1\n",
      "2016-03-07T19:39:11.776653: step 6200, loss 5.12599e-06, acc 1\n",
      "2016-03-07T19:39:11.780357: step 6200, loss 2.36032e-05, acc 1\n",
      "2016-03-07T19:39:11.783599: step 6200, loss 7.15253e-06, acc 1\n",
      "2016-03-07T19:39:11.787713: step 6200, loss 0.000118964, acc 1\n",
      "2016-03-07T19:39:11.790928: step 6200, loss 2.0027e-05, acc 1\n",
      "2016-03-07T19:39:11.795856: step 6200, loss 0.00347855, acc 1\n",
      "2016-03-07T19:39:11.799610: step 6200, loss 3.40933e-05, acc 1\n",
      "2016-03-07T19:39:11.803209: step 6200, loss 0.00010466, acc 1\n",
      "2016-03-07T19:39:11.806965: step 6200, loss 8.66614e-05, acc 1\n",
      "2016-03-07T19:39:11.810830: step 6200, loss 0.000686529, acc 1\n",
      "2016-03-07T19:39:11.814227: step 6200, loss 0.130764, acc 1\n",
      "2016-03-07T19:39:11.817737: step 6200, loss 0.0254041, acc 1\n",
      "2016-03-07T19:39:11.821318: step 6200, loss 0.0195161, acc 1\n",
      "2016-03-07T19:39:11.824888: step 6200, loss 0.000572279, acc 1\n",
      "2016-03-07T19:39:11.829189: step 6200, loss 1.01327e-05, acc 1\n",
      "2016-03-07T19:39:11.833083: step 6200, loss 0.00033671, acc 1\n",
      "2016-03-07T19:39:11.837120: step 6200, loss 0.0226276, acc 1\n",
      "2016-03-07T19:39:11.841365: step 6200, loss 0.000161873, acc 1\n",
      "2016-03-07T19:39:11.844855: step 6200, loss 0.00141044, acc 1\n",
      "2016-03-07T19:39:11.848427: step 6200, loss 4.29153e-06, acc 1\n",
      "2016-03-07T19:39:11.851926: step 6200, loss 7.07396, acc 0\n",
      "2016-03-07T19:39:11.855276: step 6200, loss 1.3709e-05, acc 1\n",
      "2016-03-07T19:39:11.858445: step 6200, loss 1.85965e-05, acc 1\n",
      "2016-03-07T19:39:11.862401: step 6200, loss 1.59739e-05, acc 1\n",
      "2016-03-07T19:39:11.866304: step 6200, loss 0.00873719, acc 1\n",
      "2016-03-07T19:39:11.869801: step 6200, loss 5.59076e-05, acc 1\n",
      "2016-03-07T19:39:11.873530: step 6200, loss 0.00039665, acc 1\n",
      "2016-03-07T19:39:11.877298: step 6200, loss 9.32136, acc 0\n",
      "2016-03-07T19:39:11.881103: step 6200, loss 0.000600754, acc 1\n",
      "2016-03-07T19:39:11.884456: step 6200, loss 0.000872231, acc 1\n",
      "2016-03-07T19:39:11.888036: step 6200, loss 9.04758e-05, acc 1\n",
      "2016-03-07T19:39:11.893736: step 6200, loss 0.233319, acc 1\n",
      "2016-03-07T19:39:11.897066: step 6200, loss 5.2452e-06, acc 1\n",
      "2016-03-07T19:39:11.901664: step 6200, loss 8.1062e-06, acc 1\n",
      "2016-03-07T19:39:11.904635: step 6200, loss 0.000886405, acc 1\n",
      "2016-03-07T19:39:11.907883: step 6200, loss 0.0029884, acc 1\n",
      "2016-03-07T19:39:11.911276: step 6200, loss 6.17485e-05, acc 1\n",
      "2016-03-07T19:39:11.914245: step 6200, loss 6.28213e-05, acc 1\n",
      "2016-03-07T19:39:11.917787: step 6200, loss 0.00025007, acc 1\n",
      "2016-03-07T19:39:11.921095: step 6200, loss 0.00315811, acc 1\n",
      "2016-03-07T19:39:11.924354: step 6200, loss 0.000201324, acc 1\n",
      "2016-03-07T19:39:11.927754: step 6200, loss 0.000781826, acc 1\n",
      "2016-03-07T19:39:11.931629: step 6200, loss 0.000385329, acc 1\n",
      "2016-03-07T19:39:11.935009: step 6200, loss 0.00108612, acc 1\n",
      "2016-03-07T19:39:11.938508: step 6200, loss 0.000382112, acc 1\n",
      "2016-03-07T19:39:11.942160: step 6200, loss 10.4147, acc 0\n",
      "2016-03-07T19:39:11.947960: step 6200, loss 0.130391, acc 1\n",
      "2016-03-07T19:39:11.951483: step 6200, loss 4.74737, acc 0\n",
      "2016-03-07T19:39:11.955390: step 6200, loss 0.0014252, acc 1\n",
      "2016-03-07T19:39:11.959346: step 6200, loss 0.000310969, acc 1\n",
      "2016-03-07T19:39:11.963132: step 6200, loss 0.000197153, acc 1\n",
      "2016-03-07T19:39:11.966690: step 6200, loss 3.13516e-05, acc 1\n",
      "2016-03-07T19:39:11.970345: step 6200, loss 4.17232e-06, acc 1\n",
      "2016-03-07T19:39:11.973730: step 6200, loss 0.678769, acc 1\n",
      "2016-03-07T19:39:11.977675: step 6200, loss 0.000521166, acc 1\n",
      "2016-03-07T19:39:11.982623: step 6200, loss 4.86362e-05, acc 1\n",
      "2016-03-07T19:39:11.986230: step 6200, loss 3.49277e-05, acc 1\n",
      "2016-03-07T19:39:11.989725: step 6200, loss 0.000317166, acc 1\n",
      "2016-03-07T19:39:11.993598: step 6200, loss 0.00123329, acc 1\n",
      "2016-03-07T19:39:11.997721: step 6200, loss 0.000510205, acc 1\n",
      "2016-03-07T19:39:12.001042: step 6200, loss 6.05565e-05, acc 1\n",
      "2016-03-07T19:39:12.004200: step 6200, loss 7.49767, acc 0\n",
      "2016-03-07T19:39:12.007770: step 6200, loss 0.00012838, acc 1\n",
      "2016-03-07T19:39:12.011361: step 6200, loss 8.27769, acc 0\n",
      "2016-03-07T19:39:12.014565: step 6200, loss 1.04904e-05, acc 1\n",
      "2016-03-07T19:39:12.017930: step 6200, loss 0.00228727, acc 1\n",
      "2016-03-07T19:39:12.021263: step 6200, loss 5.72203e-06, acc 1\n",
      "2016-03-07T19:39:12.024550: step 6200, loss 0.000982279, acc 1\n",
      "2016-03-07T19:39:12.028058: step 6200, loss 0.00103253, acc 1\n",
      "2016-03-07T19:39:12.031557: step 6200, loss 0.000595273, acc 1\n",
      "2016-03-07T19:39:12.034636: step 6200, loss 0.00632842, acc 1\n",
      "2016-03-07T19:39:12.038104: step 6200, loss 3.51661e-05, acc 1\n",
      "2016-03-07T19:39:12.041496: step 6200, loss 0.0118186, acc 1\n",
      "2016-03-07T19:39:12.045155: step 6200, loss 0.000984065, acc 1\n",
      "2016-03-07T19:39:12.051110: step 6200, loss 0.000992759, acc 1\n",
      "2016-03-07T19:39:12.054344: step 6200, loss 7.1523e-05, acc 1\n",
      "2016-03-07T19:39:12.058142: step 6200, loss 0.0797919, acc 1\n",
      "2016-03-07T19:39:12.061719: step 6200, loss 0.000149716, acc 1\n",
      "2016-03-07T19:39:12.065049: step 6200, loss 3.57627e-06, acc 1\n",
      "2016-03-07T19:39:12.069189: step 6200, loss 1.43051e-06, acc 1\n",
      "2016-03-07T19:39:12.072664: step 6200, loss 7.85558e-05, acc 1\n",
      "2016-03-07T19:39:12.076025: step 6200, loss 2.38416e-05, acc 1\n",
      "2016-03-07T19:39:12.079073: step 6200, loss 0.00334537, acc 1\n",
      "2016-03-07T19:39:12.082035: step 6200, loss 4.94706e-05, acc 1\n",
      "2016-03-07T19:39:12.085404: step 6200, loss 3.79078e-05, acc 1\n",
      "2016-03-07T19:39:12.088741: step 6200, loss 0.000298575, acc 1\n",
      "2016-03-07T19:39:12.092203: step 6200, loss 3.7431e-05, acc 1\n",
      "2016-03-07T19:39:12.095421: step 6200, loss 0.0265191, acc 1\n",
      "2016-03-07T19:39:12.098885: step 6200, loss 0.000258293, acc 1\n",
      "2016-03-07T19:39:12.104727: step 6200, loss 3.57627e-06, acc 1\n",
      "2016-03-07T19:39:12.107983: step 6200, loss 2.8729e-05, acc 1\n",
      "2016-03-07T19:39:12.111145: step 6200, loss 2.96827e-05, acc 1\n",
      "2016-03-07T19:39:12.114244: step 6200, loss 4.56561e-05, acc 1\n",
      "2016-03-07T19:39:12.117678: step 6200, loss 0.000325031, acc 1\n",
      "2016-03-07T19:39:12.122042: step 6200, loss 2.26495e-05, acc 1\n",
      "2016-03-07T19:39:12.125597: step 6200, loss 9.07142e-05, acc 1\n",
      "2016-03-07T19:39:12.129090: step 6200, loss 0.00348888, acc 1\n",
      "2016-03-07T19:39:12.132760: step 6200, loss 0.00097168, acc 1\n",
      "2016-03-07T19:39:12.135975: step 6200, loss 3.58814e-05, acc 1\n",
      "2016-03-07T19:39:12.138941: step 6200, loss 0.000118129, acc 1\n",
      "2016-03-07T19:39:12.142100: step 6200, loss 13.2056, acc 0\n",
      "2016-03-07T19:39:12.145170: step 6200, loss 0.00020192, acc 1\n",
      "2016-03-07T19:39:12.148211: step 6200, loss 0.000382708, acc 1\n",
      "2016-03-07T19:39:12.151481: step 6200, loss 0.0298917, acc 1\n",
      "2016-03-07T19:39:12.155722: step 6200, loss 4.55369e-05, acc 1\n",
      "2016-03-07T19:39:12.159168: step 6200, loss 8.22541e-06, acc 1\n",
      "2016-03-07T19:39:12.162834: step 6200, loss 0.000210859, acc 1\n",
      "2016-03-07T19:39:12.166667: step 6200, loss 0.00459389, acc 1\n",
      "2016-03-07T19:39:12.170133: step 6200, loss 0.000190717, acc 1\n",
      "2016-03-07T19:39:12.173327: step 6200, loss 8.71382e-05, acc 1\n",
      "2016-03-07T19:39:12.176765: step 6200, loss 0.011872, acc 1\n",
      "2016-03-07T19:39:12.180723: step 6200, loss 0.00198025, acc 1\n",
      "2016-03-07T19:39:12.183849: step 6200, loss 1.62123e-05, acc 1\n",
      "2016-03-07T19:39:12.187059: step 6200, loss 7.28342e-05, acc 1\n",
      "2016-03-07T19:39:12.190445: step 6200, loss 0.000845909, acc 1\n",
      "2016-03-07T19:39:12.194386: step 6200, loss 5.96045e-06, acc 1\n",
      "2016-03-07T19:39:12.197721: step 6200, loss 9.13494, acc 0\n",
      "2016-03-07T19:39:12.201562: step 6200, loss 5.63844e-05, acc 1\n",
      "2016-03-07T19:39:12.205302: step 6200, loss 0.00745609, acc 1\n",
      "2016-03-07T19:39:12.210091: step 6200, loss 0.000111693, acc 1\n",
      "2016-03-07T19:39:12.213855: step 6200, loss 0.000237913, acc 1\n",
      "2016-03-07T19:39:12.217425: step 6200, loss 0.00013875, acc 1\n",
      "2016-03-07T19:39:12.220899: step 6200, loss 8.38006e-05, acc 1\n",
      "2016-03-07T19:39:12.224765: step 6200, loss 0.000301792, acc 1\n",
      "2016-03-07T19:39:12.228104: step 6200, loss 2.53913e-05, acc 1\n",
      "2016-03-07T19:39:12.231375: step 6200, loss 0.000120037, acc 1\n",
      "2016-03-07T19:39:12.236020: step 6200, loss 0.000185711, acc 1\n",
      "2016-03-07T19:39:12.239568: step 6200, loss 3.02787e-05, acc 1\n",
      "2016-03-07T19:39:12.242984: step 6200, loss 8.00861, acc 0\n",
      "2016-03-07T19:39:12.246489: step 6200, loss 7.71254e-05, acc 1\n",
      "2016-03-07T19:39:12.249656: step 6200, loss 2.26495e-05, acc 1\n",
      "2016-03-07T19:39:12.252935: step 6200, loss 5.84108e-05, acc 1\n",
      "2016-03-07T19:39:12.256153: step 6200, loss 4.05303e-05, acc 1\n",
      "2016-03-07T19:39:12.260040: step 6200, loss 0.000296668, acc 1\n",
      "2016-03-07T19:39:12.263473: step 6200, loss 0.0305261, acc 1\n",
      "2016-03-07T19:39:12.266881: step 6200, loss 0.000109905, acc 1\n",
      "2016-03-07T19:39:12.270551: step 6200, loss 0.000320383, acc 1\n",
      "2016-03-07T19:39:12.273902: step 6200, loss 0.000101204, acc 1\n",
      "2016-03-07T19:39:12.277741: step 6200, loss 0.000507584, acc 1\n",
      "2016-03-07T19:39:12.281477: step 6200, loss 0.0471487, acc 1\n",
      "2016-03-07T19:39:12.285148: step 6200, loss 0.0296043, acc 1\n",
      "2016-03-07T19:39:12.288766: step 6200, loss 0.000236126, acc 1\n",
      "2016-03-07T19:39:12.293688: step 6200, loss 0.00071095, acc 1\n",
      "2016-03-07T19:39:12.298228: step 6200, loss 0.000220275, acc 1\n",
      "2016-03-07T19:39:12.301482: step 6200, loss 0.0174151, acc 1\n",
      "2016-03-07T19:39:12.304788: step 6200, loss 0.00559431, acc 1\n",
      "2016-03-07T19:39:12.308290: step 6200, loss 0.000107759, acc 1\n",
      "2016-03-07T19:39:12.312640: step 6200, loss 2.49144e-05, acc 1\n",
      "2016-03-07T19:39:12.316093: step 6200, loss 2.64641e-05, acc 1\n",
      "2016-03-07T19:39:12.319973: step 6200, loss 0.000202278, acc 1\n",
      "2016-03-07T19:39:12.323201: step 6200, loss 11.1879, acc 0\n",
      "2016-03-07T19:39:12.326313: step 6200, loss 0.00109386, acc 1\n",
      "2016-03-07T19:39:12.329533: step 6200, loss 0.00243189, acc 1\n",
      "2016-03-07T19:39:12.332603: step 6200, loss 0.00150496, acc 1\n",
      "2016-03-07T19:39:12.336093: step 6200, loss 1.54972e-06, acc 1\n",
      "2016-03-07T19:39:12.339244: step 6200, loss 8.27445, acc 0\n",
      "2016-03-07T19:39:12.342367: step 6200, loss 1.3113e-06, acc 1\n",
      "2016-03-07T19:39:12.345523: step 6200, loss 6.74702e-05, acc 1\n",
      "2016-03-07T19:39:12.348902: step 6200, loss 1.64264, acc 0\n",
      "2016-03-07T19:39:12.351666: step 6200, loss 0.000227545, acc 1\n",
      "2016-03-07T19:39:12.354801: step 6200, loss 0.00174846, acc 1\n",
      "2016-03-07T19:39:12.358378: step 6200, loss 0.010287, acc 1\n",
      "2016-03-07T19:39:12.361589: step 6200, loss 7.97562, acc 0\n",
      "2016-03-07T19:39:12.365546: step 6200, loss 0.000149835, acc 1\n",
      "2016-03-07T19:39:12.368587: step 6200, loss 7.33464, acc 0\n",
      "2016-03-07T19:39:12.371482: step 6200, loss 0.00156816, acc 1\n",
      "2016-03-07T19:39:12.374934: step 6200, loss 2.14576e-06, acc 1\n",
      "2016-03-07T19:39:12.378132: step 6200, loss 3.71926e-05, acc 1\n",
      "2016-03-07T19:39:12.381436: step 6200, loss 10.0052, acc 0\n",
      "2016-03-07T19:39:12.385054: step 6200, loss 0.000245064, acc 1\n",
      "2016-03-07T19:39:12.388985: step 6200, loss 0.000168309, acc 1\n",
      "2016-03-07T19:39:12.392870: step 6200, loss 0.000105971, acc 1\n",
      "2016-03-07T19:39:12.397020: step 6200, loss 9.1308, acc 0\n",
      "2016-03-07T19:39:12.400645: step 6200, loss 0.000143637, acc 1\n",
      "2016-03-07T19:39:12.404226: step 6200, loss 0.000160562, acc 1\n",
      "2016-03-07T19:39:12.407677: step 6200, loss 1.01327e-05, acc 1\n",
      "2016-03-07T19:39:12.411127: step 6200, loss 0.00610973, acc 1\n",
      "2016-03-07T19:39:12.414553: step 6200, loss 0.0018784, acc 1\n",
      "2016-03-07T19:39:12.420124: step 6200, loss 8.12974e-05, acc 1\n",
      "2016-03-07T19:39:12.424599: step 6200, loss 6.45612, acc 0\n",
      "2016-03-07T19:39:12.427561: step 6200, loss 2.7537e-05, acc 1\n",
      "2016-03-07T19:39:12.430537: step 6200, loss 6.28213e-05, acc 1\n",
      "2016-03-07T19:39:12.433981: step 6200, loss 5.4954e-05, acc 1\n",
      "2016-03-07T19:39:12.437190: step 6200, loss 6.30597e-05, acc 1\n",
      "2016-03-07T19:39:12.441010: step 6200, loss 0.000701896, acc 1\n",
      "2016-03-07T19:39:12.444375: step 6200, loss 4.19608e-05, acc 1\n",
      "2016-03-07T19:39:12.447904: step 6200, loss 0.002064, acc 1\n",
      "2016-03-07T19:39:12.451425: step 6200, loss 9.28597e-05, acc 1\n",
      "2016-03-07T19:39:12.455312: step 6200, loss 2.50339e-06, acc 1\n",
      "2016-03-07T19:39:12.458753: step 6200, loss 0.000732273, acc 1\n",
      "2016-03-07T19:39:12.462456: step 6200, loss 0.000601945, acc 1\n",
      "2016-03-07T19:39:12.467096: step 6200, loss 1.78814e-06, acc 1\n",
      "2016-03-07T19:39:12.471541: step 6200, loss 9.87455, acc 0\n",
      "2016-03-07T19:39:12.475194: step 6200, loss 7.25473, acc 0\n",
      "2016-03-07T19:39:12.478252: step 6200, loss 0.000257578, acc 1\n",
      "2016-03-07T19:39:12.481523: step 6200, loss 0.000474698, acc 1\n",
      "2016-03-07T19:39:12.484869: step 6200, loss 0.00294026, acc 1\n",
      "2016-03-07T19:39:12.488036: step 6200, loss 10.405, acc 0\n",
      "2016-03-07T19:39:12.492007: step 6200, loss 6.87814e-05, acc 1\n",
      "2016-03-07T19:39:12.495883: step 6200, loss 10.9166, acc 0\n",
      "2016-03-07T19:39:12.500784: step 6200, loss 0.000273071, acc 1\n",
      "2016-03-07T19:39:12.503901: step 6200, loss 0.000133148, acc 1\n",
      "2016-03-07T19:39:12.507585: step 6200, loss 12.1143, acc 0\n",
      "2016-03-07T19:39:12.510750: step 6200, loss 7.62937e-06, acc 1\n",
      "2016-03-07T19:39:12.514189: step 6200, loss 3.33785e-06, acc 1\n",
      "2016-03-07T19:39:12.517641: step 6200, loss 0.0114515, acc 1\n",
      "2016-03-07T19:39:12.521027: step 6200, loss 10.6671, acc 0\n",
      "2016-03-07T19:39:12.526437: step 6200, loss 5.48361e-06, acc 1\n",
      "2016-03-07T19:39:12.529822: step 6200, loss 4.72058e-05, acc 1\n",
      "2016-03-07T19:39:12.533794: step 6200, loss 0.00020931, acc 1\n",
      "2016-03-07T19:39:12.537556: step 6200, loss 1.32321e-05, acc 1\n",
      "2016-03-07T19:39:12.541211: step 6200, loss 0.000579785, acc 1\n",
      "2016-03-07T19:39:12.544638: step 6200, loss 1.50203e-05, acc 1\n",
      "2016-03-07T19:39:12.547770: step 6200, loss 8.22798, acc 0\n",
      "2016-03-07T19:39:12.551370: step 6200, loss 0.000236841, acc 1\n",
      "2016-03-07T19:39:12.555016: step 6200, loss 3.61198e-05, acc 1\n",
      "2016-03-07T19:39:12.557997: step 6200, loss 0.000116341, acc 1\n",
      "2016-03-07T19:39:12.561070: step 6200, loss 4.55369e-05, acc 1\n",
      "2016-03-07T19:39:12.564172: step 6200, loss 0.000369361, acc 1\n",
      "2016-03-07T19:39:12.567430: step 6200, loss 2.83714e-05, acc 1\n",
      "2016-03-07T19:39:12.570875: step 6200, loss 0.000708925, acc 1\n",
      "2016-03-07T19:39:12.574379: step 6200, loss 6.13863, acc 0\n",
      "2016-03-07T19:39:12.579328: step 6200, loss 9.4904, acc 0\n",
      "2016-03-07T19:39:12.582947: step 6200, loss 0.008656, acc 1\n",
      "2016-03-07T19:39:12.586841: step 6200, loss 0.000184638, acc 1\n",
      "2016-03-07T19:39:12.590401: step 6200, loss 3.55237e-05, acc 1\n",
      "2016-03-07T19:39:12.594076: step 6200, loss 0.000191193, acc 1\n",
      "2016-03-07T19:39:12.597978: step 6200, loss 2.88482e-05, acc 1\n",
      "2016-03-07T19:39:12.601326: step 6200, loss 0.000145902, acc 1\n",
      "2016-03-07T19:39:12.606611: step 6200, loss 0.0031991, acc 1\n",
      "2016-03-07T19:39:12.610552: step 6200, loss 0.0102136, acc 1\n",
      "2016-03-07T19:39:12.613970: step 6200, loss 4.11264e-05, acc 1\n",
      "2016-03-07T19:39:12.617074: step 6200, loss 0.0571401, acc 1\n",
      "2016-03-07T19:39:12.620566: step 6200, loss 0.000201086, acc 1\n",
      "2016-03-07T19:39:12.624068: step 6200, loss 0.000137677, acc 1\n",
      "2016-03-07T19:39:12.627296: step 6200, loss 0.000207283, acc 1\n",
      "2016-03-07T19:39:12.631462: step 6200, loss 0.000182612, acc 1\n",
      "2016-03-07T19:39:12.635118: step 6200, loss 0.000748712, acc 1\n",
      "2016-03-07T19:39:12.638482: step 6200, loss 7.11654e-05, acc 1\n",
      "2016-03-07T19:39:12.641823: step 6200, loss 0.00086973, acc 1\n",
      "2016-03-07T19:39:12.645469: step 6200, loss 1.3709e-05, acc 1\n",
      "2016-03-07T19:39:12.649582: step 6200, loss 2.47952e-05, acc 1\n",
      "2016-03-07T19:39:12.653317: step 6200, loss 4.99474e-05, acc 1\n",
      "2016-03-07T19:39:12.656695: step 6200, loss 4.41073e-06, acc 1\n",
      "2016-03-07T19:39:12.660741: step 6200, loss 0.000101084, acc 1\n",
      "2016-03-07T19:39:12.664402: step 6200, loss 11.2381, acc 0\n",
      "2016-03-07T19:39:12.667378: step 6200, loss 0.000314782, acc 1\n",
      "2016-03-07T19:39:12.670682: step 6200, loss 0.00402308, acc 1\n",
      "2016-03-07T19:39:12.674562: step 6200, loss 0.00140175, acc 1\n",
      "2016-03-07T19:39:12.678141: step 6200, loss 3.9339e-06, acc 1\n",
      "2016-03-07T19:39:12.683936: step 6200, loss 4.68961, acc 0\n",
      "2016-03-07T19:39:12.686859: step 6200, loss 0.000105495, acc 1\n",
      "2016-03-07T19:39:12.690340: step 6200, loss 0.00164778, acc 1\n",
      "2016-03-07T19:39:12.694836: step 6200, loss 10.5596, acc 0\n",
      "2016-03-07T19:39:12.698152: step 6200, loss 1.54971e-05, acc 1\n",
      "2016-03-07T19:39:12.701231: step 6200, loss 0.000272714, acc 1\n",
      "2016-03-07T19:39:12.704645: step 6200, loss 2.24111e-05, acc 1\n",
      "2016-03-07T19:39:12.708250: step 6200, loss 0.000120871, acc 1\n",
      "2016-03-07T19:39:12.711554: step 6200, loss 0.000740135, acc 1\n",
      "2016-03-07T19:39:12.715113: step 6200, loss 1.3113e-06, acc 1\n",
      "2016-03-07T19:39:12.720004: step 6200, loss 0.000171647, acc 1\n",
      "2016-03-07T19:39:12.723029: step 6200, loss 0.000840192, acc 1\n",
      "2016-03-07T19:39:12.726902: step 6200, loss 0.000145544, acc 1\n",
      "2016-03-07T19:39:12.731602: step 6200, loss 0.0995112, acc 1\n",
      "2016-03-07T19:39:12.735761: step 6200, loss 0.000222301, acc 1\n",
      "2016-03-07T19:39:12.739461: step 6200, loss 1.92996, acc 0\n",
      "2016-03-07T19:39:12.743349: step 6200, loss 0.000164138, acc 1\n",
      "2016-03-07T19:39:12.747252: step 6200, loss 1.93117e-05, acc 1\n",
      "2016-03-07T19:39:12.750502: step 6200, loss 0.000277957, acc 1\n",
      "2016-03-07T19:39:12.754137: step 6200, loss 1.74044e-05, acc 1\n",
      "2016-03-07T19:39:12.757733: step 6200, loss 6.92528, acc 0\n",
      "2016-03-07T19:39:12.761491: step 6200, loss 6.07966e-06, acc 1\n",
      "2016-03-07T19:39:12.765298: step 6200, loss 0.00153936, acc 1\n",
      "2016-03-07T19:39:12.769594: step 6200, loss 0.0141576, acc 1\n",
      "2016-03-07T19:39:12.772947: step 6200, loss 0.00194004, acc 1\n",
      "2016-03-07T19:39:12.776847: step 6200, loss 1.72852e-05, acc 1\n",
      "2016-03-07T19:39:12.780394: step 6200, loss 0.00838604, acc 1\n",
      "2016-03-07T19:39:12.784096: step 6200, loss 3.9339e-06, acc 1\n",
      "2016-03-07T19:39:12.788151: step 6200, loss 0.000649361, acc 1\n",
      "2016-03-07T19:39:12.792102: step 6200, loss 0.154155, acc 1\n",
      "2016-03-07T19:39:12.796055: step 6200, loss 5.6146e-05, acc 1\n",
      "2016-03-07T19:39:12.800012: step 6200, loss 0.000934403, acc 1\n",
      "2016-03-07T19:39:12.803577: step 6200, loss 2.38416e-05, acc 1\n",
      "2016-03-07T19:39:12.807004: step 6200, loss 8.34465e-07, acc 1\n",
      "2016-03-07T19:39:12.810593: step 6200, loss 0.000120037, acc 1\n",
      "2016-03-07T19:39:12.814079: step 6200, loss 7.58142e-05, acc 1\n",
      "2016-03-07T19:39:12.817579: step 6200, loss 0.0146139, acc 1\n",
      "2016-03-07T19:39:12.821134: step 6200, loss 3.45706e-06, acc 1\n",
      "2016-03-07T19:39:12.824699: step 6200, loss 1.43051e-06, acc 1\n",
      "2016-03-07T19:39:12.828453: step 6200, loss 0.000208952, acc 1\n",
      "2016-03-07T19:39:12.831840: step 6200, loss 0.00363903, acc 1\n",
      "2016-03-07T19:39:12.835433: step 6200, loss 2.7537e-05, acc 1\n",
      "2016-03-07T19:39:12.839501: step 6200, loss 7.86778e-06, acc 1\n",
      "2016-03-07T19:39:12.843113: step 6200, loss 8.4039e-05, acc 1\n",
      "2016-03-07T19:39:12.846402: step 6200, loss 0.00131521, acc 1\n",
      "2016-03-07T19:39:12.850301: step 6200, loss 0.0973425, acc 1\n",
      "2016-03-07T19:39:12.854304: step 6200, loss 0.000364237, acc 1\n",
      "2016-03-07T19:39:12.857632: step 6200, loss 0.000997046, acc 1\n",
      "2016-03-07T19:39:12.860958: step 6200, loss 0.00141925, acc 1\n",
      "2016-03-07T19:39:12.864517: step 6200, loss 9.50079, acc 0\n",
      "2016-03-07T19:39:12.867940: step 6200, loss 3.96959e-05, acc 1\n",
      "2016-03-07T19:39:12.871084: step 6200, loss 10.7994, acc 0\n",
      "2016-03-07T19:39:12.874609: step 6200, loss 0.00854526, acc 1\n",
      "2016-03-07T19:39:12.878272: step 6200, loss 0.000664728, acc 1\n",
      "2016-03-07T19:39:12.881819: step 6200, loss 9.33149, acc 0\n",
      "2016-03-07T19:39:12.885170: step 6200, loss 0.000165687, acc 1\n",
      "2016-03-07T19:39:12.888520: step 6200, loss 3.06363e-05, acc 1\n",
      "2016-03-07T19:39:12.893328: step 6200, loss 0.000127903, acc 1\n",
      "2016-03-07T19:39:12.897243: step 6200, loss 6.69632, acc 0\n",
      "2016-03-07T19:39:12.900465: step 6200, loss 0.000166522, acc 1\n",
      "2016-03-07T19:39:12.904072: step 6200, loss 7.12068, acc 0\n",
      "2016-03-07T19:39:12.907302: step 6200, loss 1.62123e-05, acc 1\n",
      "2016-03-07T19:39:12.910940: step 6200, loss 2.86102e-06, acc 1\n",
      "2016-03-07T19:39:12.914223: step 6200, loss 0.000128857, acc 1\n",
      "2016-03-07T19:39:12.917799: step 6200, loss 0.00461644, acc 1\n",
      "2016-03-07T19:39:12.920820: step 6200, loss 0.000121109, acc 1\n",
      "2016-03-07T19:39:12.924307: step 6200, loss 0.00318318, acc 1\n",
      "2016-03-07T19:39:12.927713: step 6200, loss 0.000562152, acc 1\n",
      "2016-03-07T19:39:12.930756: step 6200, loss 3.08747e-05, acc 1\n",
      "2016-03-07T19:39:12.934134: step 6200, loss 0.000113481, acc 1\n",
      "2016-03-07T19:39:12.937583: step 6200, loss 1.3709e-05, acc 1\n",
      "2016-03-07T19:39:12.941418: step 6200, loss 0.000172958, acc 1\n",
      "2016-03-07T19:39:12.946882: step 6200, loss 1.57355e-05, acc 1\n",
      "2016-03-07T19:39:12.950606: step 6200, loss 5.30467e-05, acc 1\n",
      "2016-03-07T19:39:12.954549: step 6200, loss 2.72986e-05, acc 1\n",
      "2016-03-07T19:39:12.958125: step 6200, loss 9.88196e-05, acc 1\n",
      "2016-03-07T19:39:12.961727: step 6200, loss 0.000143637, acc 1\n",
      "2016-03-07T19:39:12.965893: step 6200, loss 0.123887, acc 1\n",
      "2016-03-07T19:39:12.969634: step 6200, loss 0.0001931, acc 1\n",
      "2016-03-07T19:39:12.973875: step 6200, loss 10.0741, acc 0\n",
      "2016-03-07T19:39:12.977551: step 6200, loss 1.53779e-05, acc 1\n",
      "2016-03-07T19:39:12.980844: step 6200, loss 0.000315855, acc 1\n",
      "2016-03-07T19:39:12.984845: step 6200, loss 3.6239e-05, acc 1\n",
      "2016-03-07T19:39:12.988564: step 6200, loss 0.000259723, acc 1\n",
      "2016-03-07T19:39:12.992143: step 6200, loss 6.79491e-06, acc 1\n",
      "2016-03-07T19:39:12.996150: step 6200, loss 0.00130199, acc 1\n",
      "2016-03-07T19:39:12.999779: step 6200, loss 0.283914, acc 1\n",
      "2016-03-07T19:39:13.003156: step 6200, loss 2.59873e-05, acc 1\n",
      "2016-03-07T19:39:13.006898: step 6200, loss 0.00422063, acc 1\n",
      "2016-03-07T19:39:13.010976: step 6200, loss 4.72058e-05, acc 1\n",
      "2016-03-07T19:39:13.014108: step 6200, loss 4.02919e-05, acc 1\n",
      "2016-03-07T19:39:13.017809: step 6200, loss 0.00056942, acc 1\n",
      "2016-03-07T19:39:13.021753: step 6200, loss 0.0012021, acc 1\n",
      "2016-03-07T19:39:13.025044: step 6200, loss 3.82655e-05, acc 1\n",
      "2016-03-07T19:39:13.028709: step 6200, loss 9.11346, acc 0\n",
      "2016-03-07T19:39:13.032542: step 6200, loss 1.21593e-05, acc 1\n",
      "2016-03-07T19:39:13.036277: step 6200, loss 0.000102634, acc 1\n",
      "2016-03-07T19:39:13.040887: step 6200, loss 0.00282508, acc 1\n",
      "2016-03-07T19:39:13.044270: step 6200, loss 0.000333611, acc 1\n",
      "2016-03-07T19:39:13.048852: step 6200, loss 6.01989e-05, acc 1\n",
      "2016-03-07T19:39:13.052975: step 6200, loss 0.000578117, acc 1\n",
      "2016-03-07T19:39:13.056249: step 6200, loss 0.000341119, acc 1\n",
      "2016-03-07T19:39:13.060345: step 6200, loss 0.00133759, acc 1\n",
      "2016-03-07T19:39:13.063993: step 6200, loss 7.89991, acc 0\n",
      "2016-03-07T19:39:13.067609: step 6200, loss 0.000159609, acc 1\n",
      "2016-03-07T19:39:13.071261: step 6200, loss 0.000136485, acc 1\n",
      "2016-03-07T19:39:13.074690: step 6200, loss 8.12974e-05, acc 1\n",
      "2016-03-07T19:39:13.077808: step 6200, loss 0.000136724, acc 1\n",
      "2016-03-07T19:39:13.081420: step 6200, loss 0.000243753, acc 1\n",
      "2016-03-07T19:39:13.084872: step 6200, loss 0.000322052, acc 1\n",
      "2016-03-07T19:39:13.088226: step 6200, loss 0.000303342, acc 1\n",
      "2016-03-07T19:39:13.092244: step 6200, loss 0.000171527, acc 1\n",
      "2016-03-07T19:39:13.095845: step 6200, loss 1.88349e-05, acc 1\n",
      "2016-03-07T19:39:13.100502: step 6200, loss 0.000169144, acc 1\n",
      "2016-03-07T19:39:13.104217: step 6200, loss 4.04111e-05, acc 1\n",
      "2016-03-07T19:39:13.107755: step 6200, loss 0.00193349, acc 1\n",
      "2016-03-07T19:39:13.111517: step 6200, loss 0.000128499, acc 1\n",
      "2016-03-07T19:39:13.115136: step 6200, loss 12.4292, acc 0\n",
      "2016-03-07T19:39:13.118779: step 6200, loss 11.6583, acc 0\n",
      "2016-03-07T19:39:13.121914: step 6200, loss 4.16032e-05, acc 1\n",
      "2016-03-07T19:39:13.128780: step 6200, loss 0.000200847, acc 1\n",
      "2016-03-07T19:39:13.132412: step 6200, loss 0.000112408, acc 1\n",
      "2016-03-07T19:39:13.135596: step 6200, loss 7.00926e-05, acc 1\n",
      "2016-03-07T19:39:13.139262: step 6200, loss 0.00263582, acc 1\n",
      "2016-03-07T19:39:13.142481: step 6200, loss 0.000506511, acc 1\n",
      "2016-03-07T19:39:13.146569: step 6200, loss 10.2358, acc 0\n",
      "2016-03-07T19:39:13.150749: step 6200, loss 0.000154483, acc 1\n",
      "2016-03-07T19:39:13.154232: step 6200, loss 0.0001931, acc 1\n",
      "2016-03-07T19:39:13.157686: step 6200, loss 0.00330331, acc 1\n",
      "2016-03-07T19:39:13.161218: step 6200, loss 0.0010498, acc 1\n",
      "2016-03-07T19:39:13.164791: step 6200, loss 7.8079e-05, acc 1\n",
      "2016-03-07T19:39:13.168442: step 6200, loss 0.00452851, acc 1\n",
      "2016-03-07T19:39:13.172225: step 6200, loss 11.2336, acc 0\n",
      "2016-03-07T19:39:13.175666: step 6200, loss 0.000513779, acc 1\n",
      "2016-03-07T19:39:13.179238: step 6200, loss 0.000380205, acc 1\n",
      "2016-03-07T19:39:13.182727: step 6200, loss 0.00120365, acc 1\n",
      "2016-03-07T19:39:13.186435: step 6200, loss 3.25436e-05, acc 1\n",
      "2016-03-07T19:39:13.190208: step 6200, loss 1.66893e-06, acc 1\n",
      "2016-03-07T19:39:13.194043: step 6200, loss 4.05311e-06, acc 1\n",
      "2016-03-07T19:39:13.197586: step 6200, loss 0.000166522, acc 1\n",
      "2016-03-07T19:39:13.201166: step 6200, loss 2.33647e-05, acc 1\n",
      "2016-03-07T19:39:13.205295: step 6200, loss 0.00359199, acc 1\n",
      "2016-03-07T19:39:13.209148: step 6200, loss 0.00012695, acc 1\n",
      "2016-03-07T19:39:13.212308: step 6200, loss 6.8543e-05, acc 1\n",
      "2016-03-07T19:39:13.215954: step 6200, loss 10.1757, acc 0\n",
      "2016-03-07T19:39:13.219705: step 6200, loss 2.25303e-05, acc 1\n",
      "2016-03-07T19:39:13.223158: step 6200, loss 6.41251, acc 0\n",
      "2016-03-07T19:39:13.226904: step 6200, loss 8.54083, acc 0\n",
      "2016-03-07T19:39:13.230675: step 6200, loss 8.24894e-05, acc 1\n",
      "2016-03-07T19:39:13.234151: step 6200, loss 6.4588, acc 0\n",
      "2016-03-07T19:39:13.237391: step 6200, loss 0.00551275, acc 1\n",
      "2016-03-07T19:39:13.241098: step 6200, loss 0.000208356, acc 1\n",
      "2016-03-07T19:39:13.244899: step 6200, loss 0.000164614, acc 1\n",
      "2016-03-07T19:39:13.248370: step 6200, loss 0.000915942, acc 1\n",
      "2016-03-07T19:39:13.252229: step 6200, loss 0.000185115, acc 1\n",
      "2016-03-07T19:39:13.256548: step 6200, loss 7.86778e-06, acc 1\n",
      "2016-03-07T19:39:13.260107: step 6200, loss 0.00590331, acc 1\n",
      "2016-03-07T19:39:13.263272: step 6200, loss 0.00174846, acc 1\n",
      "2016-03-07T19:39:13.266964: step 6200, loss 6.31807e-06, acc 1\n",
      "2016-03-07T19:39:13.270416: step 6200, loss 0.000885571, acc 1\n",
      "2016-03-07T19:39:13.274265: step 6200, loss 0.000196795, acc 1\n",
      "2016-03-07T19:39:13.277942: step 6200, loss 0.00302798, acc 1\n",
      "2016-03-07T19:39:13.281466: step 6200, loss 7.4503e-05, acc 1\n",
      "2016-03-07T19:39:13.285465: step 6200, loss 2.57489e-05, acc 1\n",
      "2016-03-07T19:39:13.288650: step 6200, loss 0.123073, acc 1\n",
      "2016-03-07T19:39:13.292385: step 6200, loss 3.45706e-06, acc 1\n",
      "2016-03-07T19:39:13.295958: step 6200, loss 7.888, acc 0\n",
      "2016-03-07T19:39:13.299550: step 6200, loss 4.95898e-05, acc 1\n",
      "2016-03-07T19:39:13.302912: step 6200, loss 1.40666e-05, acc 1\n",
      "2016-03-07T19:39:13.307009: step 6200, loss 0.000400344, acc 1\n",
      "2016-03-07T19:39:13.311155: step 6200, loss 0.00326422, acc 1\n",
      "2016-03-07T19:39:13.314568: step 6200, loss 8.19549, acc 0\n",
      "2016-03-07T19:39:13.318133: step 6200, loss 8.66048, acc 0\n",
      "2016-03-07T19:39:13.321797: step 6200, loss 11.2941, acc 0\n",
      "2016-03-07T19:39:13.325862: step 6200, loss 0.00017844, acc 1\n",
      "2016-03-07T19:39:13.329486: step 6200, loss 0.0159631, acc 1\n",
      "2016-03-07T19:39:13.333570: step 6200, loss 0.00142544, acc 1\n",
      "2016-03-07T19:39:13.337112: step 6200, loss 0.0295869, acc 1\n",
      "2016-03-07T19:39:13.340745: step 6200, loss 7.20785, acc 0\n",
      "2016-03-07T19:39:13.344455: step 6200, loss 2.6226e-06, acc 1\n",
      "2016-03-07T19:39:13.347789: step 6200, loss 0.00158673, acc 1\n",
      "2016-03-07T19:39:13.351451: step 6200, loss 0.000360543, acc 1\n",
      "2016-03-07T19:39:13.354750: step 6200, loss 3.98151e-05, acc 1\n",
      "2016-03-07T19:39:13.358614: step 6200, loss 0.00168325, acc 1\n",
      "2016-03-07T19:39:13.364081: step 6200, loss 4.76837e-07, acc 1\n",
      "2016-03-07T19:39:13.367458: step 6200, loss 1.59739e-05, acc 1\n",
      "2016-03-07T19:39:13.370857: step 6200, loss 8.66614e-05, acc 1\n",
      "2016-03-07T19:39:13.374142: step 6200, loss 0.019034, acc 1\n",
      "2016-03-07T19:39:13.377539: step 6200, loss 1.40666e-05, acc 1\n",
      "2016-03-07T19:39:13.380893: step 6200, loss 9.21248, acc 0\n",
      "2016-03-07T19:39:13.383954: step 6200, loss 0.000731558, acc 1\n",
      "2016-03-07T19:39:13.387633: step 6200, loss 0.000309658, acc 1\n",
      "2016-03-07T19:39:13.391209: step 6200, loss 7.94067, acc 0\n",
      "2016-03-07T19:39:13.394777: step 6200, loss 3.26628e-05, acc 1\n",
      "2016-03-07T19:39:13.398243: step 6200, loss 0.132353, acc 1\n",
      "2016-03-07T19:39:13.401100: step 6200, loss 1.07288e-05, acc 1\n",
      "2016-03-07T19:39:13.405080: step 6200, loss 0.00190672, acc 1\n",
      "2016-03-07T19:39:13.409048: step 6200, loss 0.0020063, acc 1\n",
      "2016-03-07T19:39:13.412770: step 6200, loss 0.000545949, acc 1\n",
      "2016-03-07T19:39:13.417972: step 6200, loss 1.94309e-05, acc 1\n",
      "2016-03-07T19:39:13.421711: step 6200, loss 2.72986e-05, acc 1\n",
      "2016-03-07T19:39:13.425020: step 6200, loss 1.4305e-05, acc 1\n",
      "2016-03-07T19:39:13.428364: step 6200, loss 6.22253e-05, acc 1\n",
      "2016-03-07T19:39:13.431911: step 6200, loss 0.000107759, acc 1\n",
      "2016-03-07T19:39:13.435238: step 6200, loss 8.88865, acc 0\n",
      "2016-03-07T19:39:13.438083: step 6200, loss 6.04373e-05, acc 1\n",
      "2016-03-07T19:39:13.441734: step 6200, loss 8.7615e-05, acc 1\n",
      "2016-03-07T19:39:13.445303: step 6200, loss 6.8543e-05, acc 1\n",
      "2016-03-07T19:39:13.448979: step 6200, loss 0.000111931, acc 1\n",
      "2016-03-07T19:39:13.452147: step 6200, loss 2.0027e-05, acc 1\n",
      "2016-03-07T19:39:13.455610: step 6200, loss 8.85686e-05, acc 1\n",
      "2016-03-07T19:39:13.459081: step 6200, loss 8.71382e-05, acc 1\n",
      "2016-03-07T19:39:13.462775: step 6200, loss 3.9339e-06, acc 1\n",
      "2016-03-07T19:39:13.466678: step 6200, loss 0.000148285, acc 1\n",
      "2016-03-07T19:39:13.472315: step 6200, loss 4.43449e-05, acc 1\n",
      "2016-03-07T19:39:13.475840: step 6200, loss 4.88757e-06, acc 1\n",
      "2016-03-07T19:39:13.479596: step 6200, loss 0.000253288, acc 1\n",
      "2016-03-07T19:39:13.483674: step 6200, loss 0.00183759, acc 1\n",
      "2016-03-07T19:39:13.487381: step 6200, loss 5.48361e-06, acc 1\n",
      "2016-03-07T19:39:13.491553: step 6200, loss 2.37224e-05, acc 1\n",
      "2016-03-07T19:39:13.496340: step 6200, loss 9.51245e-05, acc 1\n",
      "2016-03-07T19:39:13.500834: step 6200, loss 0.000217772, acc 1\n",
      "2016-03-07T19:39:13.504281: step 6200, loss 7.34403, acc 0\n",
      "2016-03-07T19:39:13.508031: step 6200, loss 0.000219321, acc 1\n",
      "2016-03-07T19:39:13.511706: step 6200, loss 0.000369123, acc 1\n",
      "2016-03-07T19:39:13.514977: step 6200, loss 7.03332e-06, acc 1\n",
      "2016-03-07T19:39:13.518667: step 6200, loss 4.45833e-05, acc 1\n",
      "2016-03-07T19:39:13.524362: step 6200, loss 9.89432e-06, acc 1\n",
      "2016-03-07T19:39:13.527909: step 6200, loss 0.000146974, acc 1\n",
      "2016-03-07T19:39:13.531293: step 6200, loss 0.00194694, acc 1\n",
      "2016-03-07T19:39:13.534545: step 6200, loss 0.00120281, acc 1\n",
      "2016-03-07T19:39:13.538190: step 6200, loss 0.0120691, acc 1\n",
      "2016-03-07T19:39:13.543031: step 6200, loss 1.72852e-05, acc 1\n",
      "2016-03-07T19:39:13.546482: step 6200, loss 2.96827e-05, acc 1\n",
      "2016-03-07T19:39:13.550109: step 6200, loss 6.43728e-06, acc 1\n",
      "2016-03-07T19:39:13.553711: step 6200, loss 2.58681e-05, acc 1\n",
      "2016-03-07T19:39:13.557360: step 6200, loss 9.36941e-05, acc 1\n",
      "2016-03-07T19:39:13.560950: step 6200, loss 0.000282367, acc 1\n",
      "2016-03-07T19:39:13.564518: step 6200, loss 5.48361e-06, acc 1\n",
      "2016-03-07T19:39:13.567673: step 6200, loss 8.03559, acc 0\n",
      "2016-03-07T19:39:13.570815: step 6200, loss 0.0527768, acc 1\n",
      "2016-03-07T19:39:13.575280: step 6200, loss 12.5379, acc 0\n",
      "2016-03-07T19:39:13.578950: step 6200, loss 2.58681e-05, acc 1\n",
      "2016-03-07T19:39:13.583166: step 6200, loss 0.000123255, acc 1\n",
      "2016-03-07T19:39:13.587147: step 6200, loss 1.76428e-05, acc 1\n",
      "2016-03-07T19:39:13.590851: step 6200, loss 0.00150305, acc 1\n",
      "2016-03-07T19:39:13.595957: step 6200, loss 6.31807e-06, acc 1\n",
      "2016-03-07T19:39:13.599515: step 6200, loss 0.130764, acc 1\n",
      "2016-03-07T19:39:13.603250: step 6200, loss 0.000383542, acc 1\n",
      "2016-03-07T19:39:13.606695: step 6200, loss 4.45833e-05, acc 1\n",
      "2016-03-07T19:39:13.610369: step 6200, loss 3.05171e-05, acc 1\n",
      "2016-03-07T19:39:13.614071: step 6200, loss 7.03332e-06, acc 1\n",
      "2016-03-07T19:39:13.617508: step 6200, loss 9.29828e-06, acc 1\n",
      "2016-03-07T19:39:13.621428: step 6200, loss 0.000726436, acc 1\n",
      "2016-03-07T19:39:13.625004: step 6200, loss 0.0002341, acc 1\n",
      "2016-03-07T19:39:13.631224: step 6200, loss 0.000831139, acc 1\n",
      "2016-03-07T19:39:13.634803: step 6200, loss 6.74702e-05, acc 1\n",
      "2016-03-07T19:39:13.638065: step 6200, loss 0.000462544, acc 1\n",
      "2016-03-07T19:39:13.641849: step 6200, loss 0.0231064, acc 1\n",
      "2016-03-07T19:39:13.645674: step 6200, loss 0.000239105, acc 1\n",
      "2016-03-07T19:39:13.649339: step 6200, loss 0.000167594, acc 1\n",
      "2016-03-07T19:39:13.652537: step 6200, loss 3.76694e-05, acc 1\n",
      "2016-03-07T19:39:13.656011: step 6200, loss 5.76956e-05, acc 1\n",
      "2016-03-07T19:39:13.659576: step 6200, loss 7.48606e-05, acc 1\n",
      "2016-03-07T19:39:13.663308: step 6200, loss 0.0144942, acc 1\n",
      "2016-03-07T19:39:13.666771: step 6200, loss 0.000660321, acc 1\n",
      "2016-03-07T19:39:13.670386: step 6200, loss 0.000114911, acc 1\n",
      "2016-03-07T19:39:13.674117: step 6200, loss 3.20668e-05, acc 1\n",
      "2016-03-07T19:39:13.677757: step 6200, loss 14.7363, acc 0\n",
      "2016-03-07T19:39:13.683411: step 6200, loss 8.70224e-06, acc 1\n",
      "2016-03-07T19:39:13.687328: step 6200, loss 0.147886, acc 1\n",
      "2016-03-07T19:39:13.690868: step 6200, loss 10.8842, acc 0\n",
      "2016-03-07T19:39:13.694113: step 6200, loss 0.000530222, acc 1\n",
      "2016-03-07T19:39:13.697901: step 6200, loss 1.91925e-05, acc 1\n",
      "2016-03-07T19:39:13.701371: step 6200, loss 0.000149477, acc 1\n",
      "2016-03-07T19:39:13.704896: step 6200, loss 0.000110858, acc 1\n",
      "2016-03-07T19:39:13.708600: step 6200, loss 10.0157, acc 0\n",
      "2016-03-07T19:39:13.712074: step 6200, loss 1.29937e-05, acc 1\n",
      "2016-03-07T19:39:13.715793: step 6200, loss 0.000266278, acc 1\n",
      "2016-03-07T19:39:13.719549: step 6200, loss 2.14574e-05, acc 1\n",
      "2016-03-07T19:39:13.723473: step 6200, loss 2.50336e-05, acc 1\n",
      "2016-03-07T19:39:13.727399: step 6200, loss 0.00159732, acc 1\n",
      "2016-03-07T19:39:13.730833: step 6200, loss 0.0452865, acc 1\n",
      "2016-03-07T19:39:13.734926: step 6200, loss 0.00115947, acc 1\n",
      "2016-03-07T19:39:13.738897: step 6200, loss 1.4305e-05, acc 1\n",
      "2016-03-07T19:39:13.742520: step 6200, loss 3.09944e-06, acc 1\n",
      "2016-03-07T19:39:13.746157: step 6200, loss 0.000647335, acc 1\n",
      "2016-03-07T19:39:13.749487: step 6200, loss 8.39198e-05, acc 1\n",
      "2016-03-07T19:39:13.753151: step 6200, loss 1.51395e-05, acc 1\n",
      "2016-03-07T19:39:13.757513: step 6200, loss 4.48826, acc 0\n",
      "2016-03-07T19:39:13.761131: step 6200, loss 0.0103939, acc 1\n",
      "2016-03-07T19:39:13.764953: step 6200, loss 0.01713, acc 1\n",
      "2016-03-07T19:39:13.768076: step 6200, loss 0.0629451, acc 1\n",
      "2016-03-07T19:39:13.771839: step 6200, loss 2.86102e-06, acc 1\n",
      "2016-03-07T19:39:13.775278: step 6200, loss 0.000106329, acc 1\n",
      "2016-03-07T19:39:13.778812: step 6200, loss 4.43449e-05, acc 1\n",
      "2016-03-07T19:39:13.782601: step 6200, loss 1.46626e-05, acc 1\n",
      "2016-03-07T19:39:13.788309: step 6200, loss 2.63449e-05, acc 1\n",
      "2016-03-07T19:39:13.791746: step 6200, loss 0.000244349, acc 1\n",
      "2016-03-07T19:39:13.797063: step 6200, loss 1.16824e-05, acc 1\n",
      "2016-03-07T19:39:13.800437: step 6200, loss 3.29012e-05, acc 1\n",
      "2016-03-07T19:39:13.804027: step 6200, loss 0.000548451, acc 1\n",
      "2016-03-07T19:39:13.807613: step 6200, loss 0.000111454, acc 1\n",
      "2016-03-07T19:39:13.811444: step 6200, loss 7.52182e-05, acc 1\n",
      "2016-03-07T19:39:13.815070: step 6200, loss 9.64357e-05, acc 1\n",
      "2016-03-07T19:39:13.818579: step 6200, loss 0.00153805, acc 1\n",
      "2016-03-07T19:39:13.822521: step 6200, loss 8.34465e-07, acc 1\n",
      "2016-03-07T19:39:13.826049: step 6200, loss 0.000514732, acc 1\n",
      "2016-03-07T19:39:13.829783: step 6200, loss 0.000201563, acc 1\n",
      "2016-03-07T19:39:13.833703: step 6200, loss 0.0193051, acc 1\n",
      "2016-03-07T19:39:13.838874: step 6200, loss 5.23315e-05, acc 1\n",
      "2016-03-07T19:39:13.842534: step 6200, loss 0.000215388, acc 1\n",
      "2016-03-07T19:39:13.847441: step 6200, loss 7.77428, acc 0\n",
      "2016-03-07T19:39:13.851476: step 6200, loss 1.85965e-05, acc 1\n",
      "2016-03-07T19:39:13.855228: step 6200, loss 0.000634945, acc 1\n",
      "2016-03-07T19:39:13.858780: step 6200, loss 1.09672e-05, acc 1\n",
      "2016-03-07T19:39:13.862481: step 6200, loss 0.000334922, acc 1\n",
      "2016-03-07T19:39:13.865980: step 6200, loss 0.000136843, acc 1\n",
      "2016-03-07T19:39:13.869535: step 6200, loss 0.000520452, acc 1\n",
      "2016-03-07T19:39:13.873725: step 6200, loss 1.80004e-05, acc 1\n",
      "2016-03-07T19:39:13.877951: step 6200, loss 0.000298098, acc 1\n",
      "2016-03-07T19:39:13.881308: step 6200, loss 0.00318318, acc 1\n",
      "2016-03-07T19:39:13.886254: step 6200, loss 0.000796001, acc 1\n",
      "2016-03-07T19:39:13.891339: step 6200, loss 0.00244676, acc 1\n",
      "2016-03-07T19:39:13.894846: step 6200, loss 0.00020633, acc 1\n",
      "2016-03-07T19:39:13.897815: step 6200, loss 0.0173265, acc 1\n",
      "2016-03-07T19:39:13.900977: step 6200, loss 0.000704279, acc 1\n",
      "2016-03-07T19:39:13.904435: step 6200, loss 2.8133e-05, acc 1\n",
      "2016-03-07T19:39:13.908278: step 6200, loss 8.47286, acc 0\n",
      "2016-03-07T19:39:13.911854: step 6200, loss 1.4305e-05, acc 1\n",
      "2016-03-07T19:39:13.915645: step 6200, loss 0.000328606, acc 1\n",
      "2016-03-07T19:39:13.918973: step 6200, loss 0.00127008, acc 1\n",
      "2016-03-07T19:39:13.922254: step 6200, loss 0.0190172, acc 1\n",
      "2016-03-07T19:39:13.927171: step 6200, loss 3.96959e-05, acc 1\n",
      "2016-03-07T19:39:13.930792: step 6200, loss 0.033554, acc 1\n",
      "2016-03-07T19:39:13.935418: step 6200, loss 0.00544861, acc 1\n",
      "2016-03-07T19:39:13.939842: step 6200, loss 5.20931e-05, acc 1\n",
      "2016-03-07T19:39:13.944156: step 6200, loss 0.000405229, acc 1\n",
      "2016-03-07T19:39:13.948007: step 6200, loss 0.000255671, acc 1\n",
      "2016-03-07T19:39:13.951059: step 6200, loss 0.0384814, acc 1\n",
      "2016-03-07T19:39:13.954240: step 6200, loss 0.000497456, acc 1\n",
      "2016-03-07T19:39:13.957884: step 6200, loss 0.00100479, acc 1\n",
      "2016-03-07T19:39:13.961556: step 6200, loss 0.000249116, acc 1\n",
      "2016-03-07T19:39:13.965508: step 6200, loss 1.09672e-05, acc 1\n",
      "2016-03-07T19:39:13.968915: step 6200, loss 3.60006e-05, acc 1\n",
      "2016-03-07T19:39:13.972588: step 6200, loss 0.011993, acc 1\n",
      "2016-03-07T19:39:13.975843: step 6200, loss 3.49277e-05, acc 1\n",
      "2016-03-07T19:39:13.979973: step 6200, loss 5.91662, acc 0\n",
      "2016-03-07T19:39:13.983586: step 6200, loss 0.000125162, acc 1\n",
      "2016-03-07T19:39:13.988220: step 6200, loss 0.000471838, acc 1\n",
      "2016-03-07T19:39:13.991312: step 6200, loss 6.16293e-05, acc 1\n",
      "2016-03-07T19:39:13.996775: step 6200, loss 7.1523e-05, acc 1\n",
      "2016-03-07T19:39:14.000693: step 6200, loss 2.96827e-05, acc 1\n",
      "2016-03-07T19:39:14.004538: step 6200, loss 4.86362e-05, acc 1\n",
      "2016-03-07T19:39:14.008390: step 6200, loss 7.97478e-05, acc 1\n",
      "2016-03-07T19:39:14.011654: step 6200, loss 0.00027617, acc 1\n",
      "2016-03-07T19:39:14.014873: step 6200, loss 0.00228252, acc 1\n",
      "2016-03-07T19:39:14.018882: step 6200, loss 0.000553455, acc 1\n",
      "2016-03-07T19:39:14.022903: step 6200, loss 9.05987e-06, acc 1\n",
      "2016-03-07T19:39:14.026825: step 6200, loss 2.49144e-05, acc 1\n",
      "2016-03-07T19:39:14.030674: step 6200, loss 4.52994e-06, acc 1\n",
      "2016-03-07T19:39:14.034638: step 6200, loss 10.9128, acc 0\n",
      "2016-03-07T19:39:14.039520: step 6200, loss 7.55758e-05, acc 1\n",
      "2016-03-07T19:39:14.042980: step 6200, loss 0.000170932, acc 1\n",
      "2016-03-07T19:39:14.048158: step 6200, loss 0.00010621, acc 1\n",
      "2016-03-07T19:39:14.051987: step 6200, loss 0.000259842, acc 1\n",
      "2016-03-07T19:39:14.055678: step 6200, loss 0.386141, acc 1\n",
      "2016-03-07T19:39:14.059494: step 6200, loss 4.38681e-05, acc 1\n",
      "2016-03-07T19:39:14.062506: step 6200, loss 0.00034243, acc 1\n",
      "2016-03-07T19:39:14.065920: step 6200, loss 5.49793, acc 0\n",
      "2016-03-07T19:39:14.069089: step 6200, loss 2.65833e-05, acc 1\n",
      "2016-03-07T19:39:14.073215: step 6200, loss 2.25303e-05, acc 1\n",
      "2016-03-07T19:39:14.077041: step 6200, loss 0.000451701, acc 1\n",
      "2016-03-07T19:39:14.080748: step 6200, loss 0.00079219, acc 1\n",
      "2016-03-07T19:39:14.084393: step 6200, loss 0.0609028, acc 1\n",
      "2016-03-07T19:39:14.088304: step 6200, loss 9.58183, acc 0\n",
      "2016-03-07T19:39:14.092339: step 6200, loss 0.000313352, acc 1\n",
      "2016-03-07T19:39:14.096477: step 6200, loss 0.000139823, acc 1\n",
      "2016-03-07T19:39:14.100722: step 6200, loss 0.0356638, acc 1\n",
      "2016-03-07T19:39:14.104484: step 6200, loss 1.91925e-05, acc 1\n",
      "2016-03-07T19:39:14.107684: step 6200, loss 0.000130764, acc 1\n",
      "2016-03-07T19:39:14.111681: step 6200, loss 5.31659e-05, acc 1\n",
      "2016-03-07T19:39:14.115024: step 6200, loss 9.29789e-05, acc 1\n",
      "2016-03-07T19:39:14.119350: step 6200, loss 3.33785e-06, acc 1\n",
      "2016-03-07T19:39:14.123151: step 6200, loss 8.34462e-06, acc 1\n",
      "2016-03-07T19:39:14.127024: step 6200, loss 0.228572, acc 1\n",
      "2016-03-07T19:39:14.129913: step 6200, loss 3.82655e-05, acc 1\n",
      "2016-03-07T19:39:14.133388: step 6200, loss 0.0022083, acc 1\n",
      "2016-03-07T19:39:14.136527: step 6200, loss 0.000100012, acc 1\n",
      "2016-03-07T19:39:14.139917: step 6200, loss 0.00123008, acc 1\n",
      "2016-03-07T19:39:14.143637: step 6200, loss 1.19209e-05, acc 1\n",
      "2016-03-07T19:39:14.147427: step 6200, loss 7.47414e-05, acc 1\n",
      "2016-03-07T19:39:14.152484: step 6200, loss 0.00438811, acc 1\n",
      "2016-03-07T19:39:14.156323: step 6200, loss 2.74181e-06, acc 1\n",
      "2016-03-07T19:39:14.159810: step 6200, loss 2.09903, acc 0\n",
      "2016-03-07T19:39:14.163456: step 6200, loss 4.16032e-05, acc 1\n",
      "2016-03-07T19:39:14.167013: step 6200, loss 0.000414881, acc 1\n",
      "2016-03-07T19:39:14.170344: step 6200, loss 1.3113e-06, acc 1\n",
      "2016-03-07T19:39:14.174327: step 6200, loss 0.000510801, acc 1\n",
      "2016-03-07T19:39:14.177826: step 6200, loss 4.06495e-05, acc 1\n",
      "2016-03-07T19:39:14.181214: step 6200, loss 8.54694e-05, acc 1\n",
      "2016-03-07T19:39:14.184642: step 6200, loss 0.00869867, acc 1\n",
      "2016-03-07T19:39:14.188195: step 6200, loss 0.000212528, acc 1\n",
      "2016-03-07T19:39:14.191371: step 6200, loss 0.000507703, acc 1\n",
      "2016-03-07T19:39:14.194790: step 6200, loss 0.000491022, acc 1\n",
      "2016-03-07T19:39:14.198218: step 6200, loss 9.45285e-05, acc 1\n",
      "2016-03-07T19:39:14.203780: step 6200, loss 7.52182e-05, acc 1\n",
      "2016-03-07T19:39:14.206943: step 6200, loss 5.91918, acc 0\n",
      "2016-03-07T19:39:14.210280: step 6200, loss 0.001641, acc 1\n",
      "2016-03-07T19:39:14.213901: step 6200, loss 7.33864, acc 0\n",
      "2016-03-07T19:39:14.217319: step 6200, loss 0.000264848, acc 1\n",
      "2016-03-07T19:39:14.221209: step 6200, loss 0.000685695, acc 1\n",
      "2016-03-07T19:39:14.224779: step 6200, loss 0.00131151, acc 1\n",
      "2016-03-07T19:39:14.228447: step 6200, loss 1.31129e-05, acc 1\n",
      "2016-03-07T19:39:14.232061: step 6200, loss 5.2452e-06, acc 1\n",
      "2016-03-07T19:39:14.235864: step 6200, loss 0.00102563, acc 1\n",
      "2016-03-07T19:39:14.239356: step 6200, loss 0.0724024, acc 1\n",
      "2016-03-07T19:39:14.243025: step 6200, loss 7.00508, acc 0\n",
      "2016-03-07T19:39:14.246532: step 6200, loss 0.000791237, acc 1\n",
      "2016-03-07T19:39:14.249937: step 6200, loss 0.000722267, acc 1\n",
      "2016-03-07T19:39:14.254102: step 6200, loss 0.00560142, acc 1\n",
      "2016-03-07T19:39:14.257964: step 6200, loss 0.00597844, acc 1\n",
      "2016-03-07T19:39:14.261705: step 6200, loss 0.000973704, acc 1\n",
      "2016-03-07T19:39:14.265591: step 6200, loss 0.0004772, acc 1\n",
      "2016-03-07T19:39:14.269669: step 6200, loss 8.25363, acc 0\n",
      "2016-03-07T19:39:14.274047: step 6200, loss 10.7871, acc 0\n",
      "2016-03-07T19:39:14.277610: step 6200, loss 0.000100012, acc 1\n",
      "2016-03-07T19:39:14.282110: step 6200, loss 0.00120674, acc 1\n",
      "2016-03-07T19:39:14.285402: step 6200, loss 0.000263418, acc 1\n",
      "2016-03-07T19:39:14.289205: step 6200, loss 3.57628e-07, acc 1\n",
      "2016-03-07T19:39:14.292568: step 6200, loss 0.00318924, acc 1\n",
      "2016-03-07T19:39:14.296583: step 6200, loss 0.000198941, acc 1\n",
      "2016-03-07T19:39:14.299860: step 6200, loss 0.0875887, acc 1\n",
      "2016-03-07T19:39:14.303786: step 6200, loss 5.06627e-05, acc 1\n",
      "2016-03-07T19:39:14.308344: step 6200, loss 0.00733374, acc 1\n",
      "2016-03-07T19:39:14.312019: step 6200, loss 7.27174e-06, acc 1\n",
      "2016-03-07T19:39:14.315283: step 6200, loss 2.86102e-06, acc 1\n",
      "2016-03-07T19:39:14.318620: step 6200, loss 0.00196752, acc 1\n",
      "2016-03-07T19:39:14.322185: step 6200, loss 0.000328249, acc 1\n",
      "2016-03-07T19:39:14.325832: step 6200, loss 7.30695, acc 0\n",
      "2016-03-07T19:39:14.328856: step 6200, loss 0.00448887, acc 1\n",
      "2016-03-07T19:39:14.332311: step 6200, loss 0.000119083, acc 1\n",
      "2016-03-07T19:39:14.335939: step 6200, loss 0.0103596, acc 1\n",
      "2016-03-07T19:39:14.339330: step 6200, loss 0.00024137, acc 1\n",
      "2016-03-07T19:39:14.343006: step 6200, loss 13.9377, acc 0\n",
      "2016-03-07T19:39:14.346994: step 6200, loss 7.35494e-05, acc 1\n",
      "2016-03-07T19:39:14.350659: step 6200, loss 0.00116185, acc 1\n",
      "2016-03-07T19:39:14.354209: step 6200, loss 5.6976, acc 0\n",
      "2016-03-07T19:39:14.358072: step 6200, loss 9.45922, acc 0\n",
      "2016-03-07T19:39:14.363096: step 6200, loss 0.000356968, acc 1\n",
      "2016-03-07T19:39:14.366699: step 6200, loss 0.000956555, acc 1\n",
      "2016-03-07T19:39:14.369987: step 6200, loss 0.00187614, acc 1\n",
      "2016-03-07T19:39:14.373262: step 6200, loss 0.000502341, acc 1\n",
      "2016-03-07T19:39:14.378462: step 6200, loss 0.000179036, acc 1\n",
      "2016-03-07T19:39:14.383145: step 6200, loss 0.00175763, acc 1\n",
      "2016-03-07T19:39:14.386417: step 6200, loss 1.7762e-05, acc 1\n",
      "2016-03-07T19:39:14.389718: step 6200, loss 8.73766e-05, acc 1\n",
      "2016-03-07T19:39:14.393091: step 6200, loss 4.16032e-05, acc 1\n",
      "2016-03-07T19:39:14.397114: step 6200, loss 2.93251e-05, acc 1\n",
      "2016-03-07T19:39:14.400995: step 6200, loss 8.94066e-06, acc 1\n",
      "2016-03-07T19:39:14.404451: step 6200, loss 0.000191551, acc 1\n",
      "2016-03-07T19:39:14.407728: step 6200, loss 0.000128619, acc 1\n",
      "2016-03-07T19:39:14.411298: step 6200, loss 4.48217e-05, acc 1\n",
      "2016-03-07T19:39:14.415874: step 6200, loss 0.000139584, acc 1\n",
      "2016-03-07T19:39:14.419287: step 6200, loss 0.000115269, acc 1\n",
      "2016-03-07T19:39:14.422787: step 6200, loss 11.3904, acc 0\n",
      "2016-03-07T19:39:14.426263: step 6200, loss 3.56429e-05, acc 1\n",
      "2016-03-07T19:39:14.430514: step 6200, loss 3.08747e-05, acc 1\n",
      "2016-03-07T19:39:14.434251: step 6200, loss 0.000548808, acc 1\n",
      "2016-03-07T19:39:14.438782: step 6200, loss 0.00122151, acc 1\n",
      "2016-03-07T19:39:14.441907: step 6200, loss 6.03181e-05, acc 1\n",
      "2016-03-07T19:39:14.445018: step 6200, loss 0.00750567, acc 1\n",
      "2016-03-07T19:39:14.448500: step 6200, loss 0.000114077, acc 1\n",
      "2016-03-07T19:39:14.451735: step 6200, loss 0.00374295, acc 1\n",
      "2016-03-07T19:39:14.455107: step 6200, loss 9.32248, acc 0\n",
      "2016-03-07T19:39:14.459980: step 6200, loss 0.0010036, acc 1\n",
      "2016-03-07T19:39:14.463447: step 6200, loss 0.011856, acc 1\n",
      "2016-03-07T19:39:14.468121: step 6200, loss 4.58945e-05, acc 1\n",
      "2016-03-07T19:39:14.471584: step 6200, loss 6.21952, acc 0\n",
      "2016-03-07T19:39:14.476219: step 6200, loss 2.71793e-05, acc 1\n",
      "2016-03-07T19:39:14.479815: step 6200, loss 0.000249474, acc 1\n",
      "2016-03-07T19:39:14.483186: step 6200, loss 6.09141e-05, acc 1\n",
      "2016-03-07T19:39:14.487631: step 6200, loss 4.40574, acc 0\n",
      "2016-03-07T19:39:14.491557: step 6200, loss 0.000700824, acc 1\n",
      "2016-03-07T19:39:14.497016: step 6200, loss 1.23977e-05, acc 1\n",
      "2016-03-07T19:39:14.500615: step 6200, loss 1.76428e-05, acc 1\n",
      "2016-03-07T19:39:14.504286: step 6200, loss 8.48661, acc 0\n",
      "2016-03-07T19:39:14.508347: step 6200, loss 0.00024709, acc 1\n",
      "2016-03-07T19:39:14.512110: step 6200, loss 3.57627e-06, acc 1\n",
      "2016-03-07T19:39:14.516931: step 6200, loss 0.0361103, acc 1\n",
      "2016-03-07T19:39:14.521457: step 6200, loss 0.000178798, acc 1\n",
      "2016-03-07T19:39:14.525115: step 6200, loss 6.43728e-06, acc 1\n",
      "2016-03-07T19:39:14.528738: step 6200, loss 0.000246971, acc 1\n",
      "2016-03-07T19:39:14.532506: step 6200, loss 0.000383304, acc 1\n",
      "2016-03-07T19:39:14.535933: step 6200, loss 0.00253939, acc 1\n",
      "2016-03-07T19:39:14.539415: step 6200, loss 0.000486017, acc 1\n",
      "2016-03-07T19:39:14.542736: step 6200, loss 1.66893e-06, acc 1\n",
      "2016-03-07T19:39:14.546318: step 6200, loss 5.60282e-06, acc 1\n",
      "2016-03-07T19:39:14.549636: step 6200, loss 3.49277e-05, acc 1\n",
      "2016-03-07T19:39:14.553077: step 6200, loss 3.07555e-05, acc 1\n",
      "2016-03-07T19:39:14.557883: step 6200, loss 0.0351539, acc 1\n",
      "2016-03-07T19:39:14.561443: step 6200, loss 10.3928, acc 0\n",
      "2016-03-07T19:39:14.564851: step 6200, loss 5.05435e-05, acc 1\n",
      "2016-03-07T19:39:14.568589: step 6200, loss 2.78946e-05, acc 1\n",
      "2016-03-07T19:39:14.572952: step 6200, loss 4.16032e-05, acc 1\n",
      "2016-03-07T19:39:14.577062: step 6200, loss 5.68612e-05, acc 1\n",
      "2016-03-07T19:39:14.580299: step 6200, loss 0.0865879, acc 1\n",
      "2016-03-07T19:39:14.583883: step 6200, loss 10.8326, acc 0\n",
      "2016-03-07T19:39:14.587443: step 6200, loss 0.000393552, acc 1\n",
      "2016-03-07T19:39:14.590956: step 6200, loss 0.000596107, acc 1\n",
      "2016-03-07T19:39:14.594749: step 6200, loss 10.8023, acc 0\n",
      "2016-03-07T19:39:14.597994: step 6200, loss 0.007091, acc 1\n",
      "2016-03-07T19:39:14.601394: step 6200, loss 5.78928, acc 0\n",
      "2016-03-07T19:39:14.606389: step 6200, loss 2.98023e-06, acc 1\n",
      "2016-03-07T19:39:14.609519: step 6200, loss 0.000556552, acc 1\n",
      "2016-03-07T19:39:14.613564: step 6200, loss 8.82145e-06, acc 1\n",
      "2016-03-07T19:39:14.618030: step 6200, loss 0.0871672, acc 1\n",
      "2016-03-07T19:39:14.621669: step 6200, loss 9.83317, acc 0\n",
      "2016-03-07T19:39:14.626069: step 6200, loss 0.000109428, acc 1\n",
      "2016-03-07T19:39:14.629632: step 6200, loss 0.006807, acc 1\n",
      "2016-03-07T19:39:14.634278: step 6200, loss 8.74006, acc 0\n",
      "2016-03-07T19:39:14.638046: step 6200, loss 0.000451463, acc 1\n",
      "2016-03-07T19:39:14.641805: step 6200, loss 3.26628e-05, acc 1\n",
      "2016-03-07T19:39:14.645176: step 6200, loss 2.9784, acc 0\n",
      "2016-03-07T19:39:14.648919: step 6200, loss 0.0872847, acc 1\n",
      "2016-03-07T19:39:14.652281: step 6200, loss 9.36941e-05, acc 1\n",
      "2016-03-07T19:39:14.656035: step 6200, loss 6.52054e-05, acc 1\n",
      "2016-03-07T19:39:14.659571: step 6200, loss 7.11654e-05, acc 1\n",
      "2016-03-07T19:39:14.663166: step 6200, loss 2.05038e-05, acc 1\n",
      "2016-03-07T19:39:14.666772: step 6200, loss 0.00023398, acc 1\n",
      "2016-03-07T19:39:14.670661: step 6200, loss 6.81752, acc 0\n",
      "2016-03-07T19:39:14.673627: step 6200, loss 7.54566e-05, acc 1\n",
      "2016-03-07T19:39:14.677786: step 6200, loss 1.20401e-05, acc 1\n",
      "2016-03-07T19:39:14.681261: step 6200, loss 0.00122413, acc 1\n",
      "2016-03-07T19:39:14.684663: step 6200, loss 9.62747, acc 0\n",
      "2016-03-07T19:39:14.688149: step 6200, loss 9.61973e-05, acc 1\n",
      "2016-03-07T19:39:14.691481: step 6200, loss 1.03712e-05, acc 1\n",
      "2016-03-07T19:39:14.694717: step 6200, loss 0.000322409, acc 1\n",
      "2016-03-07T19:39:14.698201: step 6200, loss 5.84124e-06, acc 1\n",
      "2016-03-07T19:39:14.701529: step 6200, loss 0.0017174, acc 1\n",
      "2016-03-07T19:39:14.704912: step 6200, loss 6.22498, acc 0\n",
      "2016-03-07T19:39:14.708822: step 6200, loss 0.000164138, acc 1\n",
      "2016-03-07T19:39:14.713757: step 6200, loss 0.00184663, acc 1\n",
      "2016-03-07T19:39:14.717921: step 6200, loss 6.04373e-05, acc 1\n",
      "2016-03-07T19:39:14.721554: step 6200, loss 0.00280178, acc 1\n",
      "2016-03-07T19:39:14.725097: step 6200, loss 0.00017117, acc 1\n",
      "2016-03-07T19:39:14.729867: step 6200, loss 0.00513224, acc 1\n",
      "2016-03-07T19:39:14.733411: step 6200, loss 0.000311326, acc 1\n",
      "2016-03-07T19:39:14.737258: step 6200, loss 1.41858e-05, acc 1\n",
      "2016-03-07T19:39:14.740275: step 6200, loss 0.000837452, acc 1\n",
      "2016-03-07T19:39:14.743766: step 6200, loss 0.00012838, acc 1\n",
      "2016-03-07T19:39:14.747253: step 6200, loss 7.35314, acc 0\n",
      "2016-03-07T19:39:14.750905: step 6200, loss 8.51041, acc 0\n",
      "2016-03-07T19:39:14.754905: step 6200, loss 0.00247482, acc 1\n",
      "2016-03-07T19:39:14.758252: step 6200, loss 0.000240297, acc 1\n",
      "2016-03-07T19:39:14.762073: step 6200, loss 5.96045e-06, acc 1\n",
      "2016-03-07T19:39:14.765139: step 6200, loss 0.00032372, acc 1\n",
      "2016-03-07T19:39:14.769946: step 6200, loss 0.0020069, acc 1\n",
      "2016-03-07T19:39:14.773087: step 6200, loss 0.000421316, acc 1\n",
      "2016-03-07T19:39:14.776688: step 6200, loss 0.00622003, acc 1\n",
      "2016-03-07T19:39:14.781024: step 6200, loss 2.44376e-05, acc 1\n",
      "2016-03-07T19:39:14.784948: step 6200, loss 0.0112269, acc 1\n",
      "2016-03-07T19:39:14.788789: step 6200, loss 6.63974e-05, acc 1\n",
      "2016-03-07T19:39:14.792174: step 6200, loss 4.41073e-06, acc 1\n",
      "2016-03-07T19:39:14.795580: step 6200, loss 0.000115626, acc 1\n",
      "2016-03-07T19:39:14.798722: step 6200, loss 2.38418e-06, acc 1\n",
      "2016-03-07T19:39:14.802611: step 6200, loss 4.41073e-06, acc 1\n",
      "2016-03-07T19:39:14.806464: step 6200, loss 4.81594e-05, acc 1\n",
      "2016-03-07T19:39:14.810337: step 6200, loss 4.76183, acc 0\n",
      "2016-03-07T19:39:14.813927: step 6200, loss 1.21593e-05, acc 1\n",
      "2016-03-07T19:39:14.817740: step 6200, loss 3.88615e-05, acc 1\n",
      "2016-03-07T19:39:14.821562: step 6200, loss 7.25958e-05, acc 1\n",
      "2016-03-07T19:39:14.824832: step 6200, loss 0.00119853, acc 1\n",
      "2016-03-07T19:39:14.828622: step 6200, loss 2.09806e-05, acc 1\n",
      "2016-03-07T19:39:14.832429: step 6200, loss 10.8306, acc 0\n",
      "2016-03-07T19:39:14.835602: step 6200, loss 0.00061922, acc 1\n",
      "2016-03-07T19:39:14.839387: step 6200, loss 0.00012993, acc 1\n",
      "2016-03-07T19:39:14.842570: step 6200, loss 6.30597e-05, acc 1\n",
      "2016-03-07T19:39:14.846018: step 6200, loss 0.00041083, acc 1\n",
      "2016-03-07T19:39:14.849401: step 6200, loss 0.000253884, acc 1\n",
      "2016-03-07T19:39:14.852597: step 6200, loss 0.0122552, acc 1\n",
      "2016-03-07T19:39:14.856046: step 6200, loss 0.000280579, acc 1\n",
      "2016-03-07T19:39:14.859524: step 6200, loss 0.000159251, acc 1\n",
      "2016-03-07T19:39:14.862924: step 6200, loss 0.000562748, acc 1\n",
      "2016-03-07T19:39:14.866182: step 6200, loss 0.00800922, acc 1\n",
      "2016-03-07T19:39:14.869515: step 6200, loss 0.00450964, acc 1\n",
      "2016-03-07T19:39:14.873078: step 6200, loss 4.29153e-06, acc 1\n",
      "2016-03-07T19:39:14.876382: step 6200, loss 0.000139942, acc 1\n",
      "2016-03-07T19:39:14.880268: step 6200, loss 0.00193159, acc 1\n",
      "2016-03-07T19:39:14.886100: step 6200, loss 0.000278553, acc 1\n",
      "2016-03-07T19:39:14.890002: step 6200, loss 0.130764, acc 1\n",
      "2016-03-07T19:39:14.893579: step 6200, loss 0.0208165, acc 1\n",
      "2016-03-07T19:39:14.896925: step 6200, loss 0.00114602, acc 1\n",
      "2016-03-07T19:39:14.900709: step 6200, loss 0.000162469, acc 1\n",
      "2016-03-07T19:39:14.904351: step 6200, loss 0.000106448, acc 1\n",
      "2016-03-07T19:39:14.907501: step 6200, loss 3.09939e-05, acc 1\n",
      "2016-03-07T19:39:14.911105: step 6200, loss 0.0912119, acc 1\n",
      "2016-03-07T19:39:14.915861: step 6200, loss 0.000214554, acc 1\n",
      "2016-03-07T19:39:14.919861: step 6200, loss 4.78018e-05, acc 1\n",
      "2016-03-07T19:39:14.923066: step 6200, loss 0.000469217, acc 1\n",
      "2016-03-07T19:39:14.927016: step 6200, loss 0.000249116, acc 1\n",
      "2016-03-07T19:39:14.930813: step 6200, loss 1.06096e-05, acc 1\n",
      "2016-03-07T19:39:14.934842: step 6200, loss 7.83174e-05, acc 1\n",
      "2016-03-07T19:39:14.940829: step 6200, loss 10.0743, acc 0\n",
      "2016-03-07T19:39:14.944535: step 6200, loss 1.10864e-05, acc 1\n",
      "2016-03-07T19:39:14.948259: step 6200, loss 3.33785e-06, acc 1\n",
      "2016-03-07T19:39:14.951523: step 6200, loss 0.00178166, acc 1\n",
      "2016-03-07T19:39:14.955231: step 6200, loss 0.00037389, acc 1\n",
      "2016-03-07T19:39:14.958661: step 6200, loss 3.77886e-05, acc 1\n",
      "2016-03-07T19:39:14.962077: step 6200, loss 1.06096e-05, acc 1\n",
      "2016-03-07T19:39:14.965659: step 6200, loss 0.00183164, acc 1\n",
      "2016-03-07T19:39:14.969445: step 6200, loss 0.000240297, acc 1\n",
      "2016-03-07T19:39:14.972879: step 6200, loss 0.000182612, acc 1\n",
      "2016-03-07T19:39:14.976414: step 6200, loss 3.60006e-05, acc 1\n",
      "2016-03-07T19:39:14.979779: step 6200, loss 5.47156e-05, acc 1\n",
      "2016-03-07T19:39:14.983055: step 6200, loss 0.000113481, acc 1\n",
      "2016-03-07T19:39:14.986615: step 6200, loss 0.00221579, acc 1\n",
      "2016-03-07T19:39:14.990696: step 6200, loss 3.98151e-05, acc 1\n",
      "2016-03-07T19:39:14.993878: step 6200, loss 0.000292735, acc 1\n",
      "2016-03-07T19:39:14.997657: step 6200, loss 0.000582049, acc 1\n",
      "2016-03-07T19:39:15.001034: step 6200, loss 0.000191789, acc 1\n",
      "2016-03-07T19:39:15.004537: step 6200, loss 0.00318318, acc 1\n",
      "2016-03-07T19:39:15.007954: step 6200, loss 9.53674e-07, acc 1\n",
      "2016-03-07T19:39:15.011510: step 6200, loss 1.00135e-05, acc 1\n",
      "2016-03-07T19:39:15.015334: step 6200, loss 2.86102e-06, acc 1\n",
      "2016-03-07T19:39:15.019085: step 6200, loss 2.98023e-06, acc 1\n",
      "2016-03-07T19:39:15.022561: step 6200, loss 0.00252548, acc 1\n",
      "2016-03-07T19:39:15.025916: step 6200, loss 0.00106921, acc 1\n",
      "2016-03-07T19:39:15.029409: step 6200, loss 0.00858001, acc 1\n",
      "2016-03-07T19:39:15.032676: step 6200, loss 0.000428704, acc 1\n",
      "2016-03-07T19:39:15.036561: step 6200, loss 9.83428e-05, acc 1\n",
      "2016-03-07T19:39:15.040049: step 6200, loss 3.26628e-05, acc 1\n",
      "2016-03-07T19:39:15.045996: step 6200, loss 0.00338731, acc 1\n",
      "2016-03-07T19:39:15.049728: step 6200, loss 0.0014227, acc 1\n",
      "2016-03-07T19:39:15.052755: step 6200, loss 4.7325e-05, acc 1\n",
      "2016-03-07T19:39:15.056569: step 6200, loss 0.000120871, acc 1\n",
      "2016-03-07T19:39:15.059596: step 6200, loss 7.15256e-07, acc 1\n",
      "2016-03-07T19:39:15.063590: step 6200, loss 6.79491e-06, acc 1\n",
      "2016-03-07T19:39:15.067480: step 6200, loss 6.43728e-06, acc 1\n",
      "2016-03-07T19:39:15.070491: step 6200, loss 1.22785e-05, acc 1\n",
      "2016-03-07T19:39:15.074469: step 6200, loss 0.035234, acc 1\n",
      "2016-03-07T19:39:15.077950: step 6200, loss 6.28481, acc 0\n",
      "2016-03-07T19:39:15.081711: step 6200, loss 1.72339, acc 0\n",
      "2016-03-07T19:39:15.084740: step 6200, loss 0.000108951, acc 1\n",
      "2016-03-07T19:39:15.087892: step 6200, loss 0.000249831, acc 1\n",
      "2016-03-07T19:39:15.091543: step 6200, loss 0.00220104, acc 1\n",
      "2016-03-07T19:39:15.097526: step 6200, loss 1.87157e-05, acc 1\n",
      "2016-03-07T19:39:15.100935: step 6200, loss 3.69548e-06, acc 1\n",
      "2016-03-07T19:39:15.104334: step 6200, loss 6.9022, acc 0\n",
      "2016-03-07T19:39:15.107747: step 6200, loss 3.37357e-05, acc 1\n",
      "2016-03-07T19:39:15.111550: step 6200, loss 6.75964, acc 0\n",
      "2016-03-07T19:39:15.114890: step 6200, loss 7.38932, acc 0\n",
      "2016-03-07T19:39:15.118546: step 6200, loss 0.0083553, acc 1\n",
      "2016-03-07T19:39:15.121480: step 6200, loss 0.000708925, acc 1\n",
      "2016-03-07T19:39:15.124952: step 6200, loss 0.00309833, acc 1\n",
      "2016-03-07T19:39:15.128512: step 6200, loss 0.101516, acc 1\n",
      "2016-03-07T19:39:15.132012: step 6200, loss 0.00811553, acc 1\n",
      "2016-03-07T19:39:15.135310: step 6200, loss 9.75084e-05, acc 1\n",
      "2016-03-07T19:39:15.138803: step 6200, loss 0.00193397, acc 1\n",
      "2016-03-07T19:39:15.142309: step 6200, loss 2.38416e-05, acc 1\n",
      "2016-03-07T19:39:15.147917: step 6200, loss 0.000177248, acc 1\n",
      "2016-03-07T19:39:15.151452: step 6200, loss 1.10864e-05, acc 1\n",
      "2016-03-07T19:39:15.155270: step 6200, loss 0.00251263, acc 1\n",
      "2016-03-07T19:39:15.158796: step 6200, loss 0.0013077, acc 1\n",
      "2016-03-07T19:39:15.162634: step 6200, loss 0.00459923, acc 1\n",
      "2016-03-07T19:39:15.165869: step 6200, loss 0.00161517, acc 1\n",
      "2016-03-07T19:39:15.169660: step 6200, loss 3.95767e-05, acc 1\n",
      "2016-03-07T19:39:15.173095: step 6200, loss 0.000117057, acc 1\n",
      "2016-03-07T19:39:15.177058: step 6200, loss 7.22382e-05, acc 1\n",
      "2016-03-07T19:39:15.180690: step 6200, loss 8.78874, acc 0\n",
      "2016-03-07T19:39:15.184141: step 6200, loss 6.53246e-05, acc 1\n",
      "2016-03-07T19:39:15.187192: step 6200, loss 5.32851e-05, acc 1\n",
      "2016-03-07T19:39:15.191089: step 6200, loss 0.00180439, acc 1\n",
      "2016-03-07T19:39:15.194903: step 6200, loss 0.000181539, acc 1\n",
      "2016-03-07T19:39:15.199217: step 6200, loss 3.50469e-05, acc 1\n",
      "2016-03-07T19:39:15.204440: step 6200, loss 7.12805, acc 0\n",
      "2016-03-07T19:39:15.208105: step 6200, loss 7.08078e-05, acc 1\n",
      "2016-03-07T19:39:15.211148: step 6200, loss 10.9171, acc 0\n",
      "2016-03-07T19:39:15.214831: step 6200, loss 1.06096e-05, acc 1\n",
      "2016-03-07T19:39:15.218540: step 6200, loss 2.0027e-05, acc 1\n",
      "2016-03-07T19:39:15.222861: step 6200, loss 6.91411e-06, acc 1\n",
      "2016-03-07T19:39:15.226737: step 6200, loss 2.28879e-05, acc 1\n",
      "2016-03-07T19:39:15.230508: step 6200, loss 0.000838048, acc 1\n",
      "2016-03-07T19:39:15.234038: step 6200, loss 10.2419, acc 0\n",
      "2016-03-07T19:39:15.237173: step 6200, loss 0.000424653, acc 1\n",
      "2016-03-07T19:39:15.240960: step 6200, loss 0.000103588, acc 1\n",
      "2016-03-07T19:39:15.244222: step 6200, loss 0.00271357, acc 1\n",
      "2016-03-07T19:39:15.247893: step 6200, loss 1.49011e-05, acc 1\n",
      "2016-03-07T19:39:15.253289: step 6200, loss 0.000136009, acc 1\n",
      "2016-03-07T19:39:15.257739: step 6200, loss 0.000498766, acc 1\n",
      "2016-03-07T19:39:15.261250: step 6200, loss 5.96045e-06, acc 1\n",
      "2016-03-07T19:39:15.264865: step 6200, loss 0.000812082, acc 1\n",
      "2016-03-07T19:39:15.268299: step 6200, loss 0.000429538, acc 1\n",
      "2016-03-07T19:39:15.271741: step 6200, loss 0.000103945, acc 1\n",
      "2016-03-07T19:39:15.275260: step 6200, loss 5.4358e-05, acc 1\n",
      "2016-03-07T19:39:15.278837: step 6200, loss 0.000114077, acc 1\n",
      "2016-03-07T19:39:15.282725: step 6200, loss 2.38418e-06, acc 1\n",
      "2016-03-07T19:39:15.286199: step 6200, loss 0.00247993, acc 1\n",
      "2016-03-07T19:39:15.289744: step 6200, loss 0.00132247, acc 1\n",
      "2016-03-07T19:39:15.293203: step 6200, loss 2.58681e-05, acc 1\n",
      "2016-03-07T19:39:15.296407: step 6200, loss 0.00275815, acc 1\n",
      "2016-03-07T19:39:15.299645: step 6200, loss 0.130764, acc 1\n",
      "2016-03-07T19:39:15.306005: step 6200, loss 2.16959e-05, acc 1\n",
      "2016-03-07T19:39:15.309551: step 6200, loss 9.32173e-05, acc 1\n",
      "2016-03-07T19:39:15.313310: step 6200, loss 0.000440024, acc 1\n",
      "2016-03-07T19:39:15.316910: step 6200, loss 0.00679859, acc 1\n",
      "2016-03-07T19:39:15.320319: step 6200, loss 6.19886e-06, acc 1\n",
      "2016-03-07T19:39:15.324071: step 6200, loss 0.0594726, acc 1\n",
      "2016-03-07T19:39:15.327256: step 6200, loss 0.000393194, acc 1\n",
      "2016-03-07T19:39:15.331023: step 6200, loss 0.00134068, acc 1\n",
      "2016-03-07T19:39:15.334733: step 6200, loss 0.000212885, acc 1\n",
      "2016-03-07T19:39:15.338254: step 6200, loss 5.84124e-06, acc 1\n",
      "2016-03-07T19:39:15.341747: step 6200, loss 0.000292497, acc 1\n",
      "2016-03-07T19:39:15.345161: step 6200, loss 3.83847e-05, acc 1\n",
      "2016-03-07T19:39:15.348906: step 6200, loss 0.000256267, acc 1\n",
      "2016-03-07T19:39:15.354014: step 6200, loss 7.58142e-05, acc 1\n",
      "2016-03-07T19:39:15.359810: step 6200, loss 6.05565e-05, acc 1\n",
      "2016-03-07T19:39:15.363594: step 6200, loss 1.07288e-05, acc 1\n",
      "2016-03-07T19:39:15.366933: step 6200, loss 9.07142e-05, acc 1\n",
      "2016-03-07T19:39:15.370573: step 6200, loss 0.0383186, acc 1\n",
      "2016-03-07T19:39:15.374125: step 6200, loss 0.00265448, acc 1\n",
      "2016-03-07T19:39:15.377807: step 6200, loss 0.00329499, acc 1\n",
      "2016-03-07T19:39:15.381912: step 6200, loss 0.000467191, acc 1\n",
      "2016-03-07T19:39:15.385368: step 6200, loss 0.000504605, acc 1\n",
      "2016-03-07T19:39:15.388734: step 6200, loss 7.39095e-06, acc 1\n",
      "2016-03-07T19:39:15.392562: step 6200, loss 0.00109672, acc 1\n",
      "2016-03-07T19:39:15.396578: step 6200, loss 4.29153e-06, acc 1\n",
      "2016-03-07T19:39:15.399678: step 6200, loss 2.19343e-05, acc 1\n",
      "2016-03-07T19:39:15.403565: step 6200, loss 9.53674e-07, acc 1\n",
      "2016-03-07T19:39:15.407302: step 6200, loss 0.000356372, acc 1\n",
      "2016-03-07T19:39:15.412800: step 6200, loss 0.00055405, acc 1\n",
      "2016-03-07T19:39:15.416239: step 6200, loss 3.09939e-05, acc 1\n",
      "2016-03-07T19:39:15.419642: step 6200, loss 1.31129e-05, acc 1\n",
      "2016-03-07T19:39:15.423198: step 6200, loss 1.71639, acc 0\n",
      "2016-03-07T19:39:15.426773: step 6200, loss 5.17355e-05, acc 1\n",
      "2016-03-07T19:39:15.430689: step 6200, loss 0.000111454, acc 1\n",
      "2016-03-07T19:39:15.434413: step 6200, loss 0.00279453, acc 1\n",
      "2016-03-07T19:39:15.437879: step 6200, loss 0.0107419, acc 1\n",
      "2016-03-07T19:39:15.441433: step 6200, loss 2.16959e-05, acc 1\n",
      "2016-03-07T19:39:15.444865: step 6200, loss 3.14708e-05, acc 1\n",
      "2016-03-07T19:39:15.448467: step 6200, loss 5.60268e-05, acc 1\n",
      "2016-03-07T19:39:15.452235: step 6200, loss 0.00023398, acc 1\n",
      "2016-03-07T19:39:15.455841: step 6200, loss 0.00152436, acc 1\n",
      "2016-03-07T19:39:15.459554: step 6200, loss 0.00352298, acc 1\n",
      "2016-03-07T19:39:15.464979: step 6200, loss 0.000147093, acc 1\n",
      "2016-03-07T19:39:15.468533: step 6200, loss 8.57078e-05, acc 1\n",
      "2016-03-07T19:39:15.471961: step 6200, loss 6.06757e-05, acc 1\n",
      "2016-03-07T19:39:15.475505: step 6200, loss 0.00410357, acc 1\n",
      "2016-03-07T19:39:15.479179: step 6200, loss 4.89688, acc 0\n",
      "2016-03-07T19:39:15.482746: step 6200, loss 2.50336e-05, acc 1\n",
      "2016-03-07T19:39:15.486083: step 6200, loss 0.000129572, acc 1\n",
      "2016-03-07T19:39:15.489383: step 6200, loss 0.000476009, acc 1\n",
      "2016-03-07T19:39:15.492974: step 6200, loss 0.000655436, acc 1\n",
      "2016-03-07T19:39:15.496620: step 6200, loss 7.62937e-06, acc 1\n",
      "2016-03-07T19:39:15.500430: step 6200, loss 0.00106659, acc 1\n",
      "2016-03-07T19:39:15.504143: step 6200, loss 0.000203231, acc 1\n",
      "2016-03-07T19:39:15.509032: step 6200, loss 4.63713e-05, acc 1\n",
      "2016-03-07T19:39:15.512504: step 6200, loss 0.0590629, acc 1\n",
      "2016-03-07T19:39:15.517235: step 6200, loss 0.208033, acc 1\n",
      "2016-03-07T19:39:15.521141: step 6200, loss 0.0670819, acc 1\n",
      "2016-03-07T19:39:15.524437: step 6200, loss 0.00284588, acc 1\n",
      "2016-03-07T19:39:15.528062: step 6200, loss 0.000198345, acc 1\n",
      "2016-03-07T19:39:15.531323: step 6200, loss 0.00125615, acc 1\n",
      "2016-03-07T19:39:15.535115: step 6200, loss 0.00021801, acc 1\n",
      "2016-03-07T19:39:15.538239: step 6200, loss 4.50564, acc 0\n",
      "2016-03-07T19:39:15.541981: step 6200, loss 4.17224e-05, acc 1\n",
      "2016-03-07T19:39:15.545705: step 6200, loss 0.00441754, acc 1\n",
      "2016-03-07T19:39:15.549717: step 6200, loss 5.2452e-06, acc 1\n",
      "2016-03-07T19:39:15.553029: step 6200, loss 4.02616, acc 0\n",
      "2016-03-07T19:39:15.557071: step 6200, loss 2.50339e-06, acc 1\n",
      "2016-03-07T19:39:15.560433: step 6200, loss 0.000825779, acc 1\n",
      "2016-03-07T19:39:15.564142: step 6200, loss 1.12056e-05, acc 1\n",
      "2016-03-07T19:39:15.569754: step 6200, loss 0.000116341, acc 1\n",
      "2016-03-07T19:39:15.573216: step 6200, loss 7.89346, acc 0\n",
      "2016-03-07T19:39:15.577652: step 6200, loss 4.64915e-06, acc 1\n",
      "2016-03-07T19:39:15.581140: step 6200, loss 0.000267351, acc 1\n",
      "2016-03-07T19:39:15.584769: step 6200, loss 0.000157582, acc 1\n",
      "2016-03-07T19:39:15.588334: step 6200, loss 5.13779e-05, acc 1\n",
      "2016-03-07T19:39:15.591743: step 6200, loss 0.000480536, acc 1\n",
      "2016-03-07T19:39:15.595525: step 6200, loss 6.4967e-05, acc 1\n",
      "2016-03-07T19:39:15.599362: step 6200, loss 0.000219202, acc 1\n",
      "2016-03-07T19:39:15.603195: step 6200, loss 0.00012838, acc 1\n",
      "2016-03-07T19:39:15.606380: step 6200, loss 1.53779e-05, acc 1\n",
      "2016-03-07T19:39:15.610260: step 6200, loss 0.0143776, acc 1\n",
      "2016-03-07T19:39:15.613821: step 6200, loss 0.00199726, acc 1\n",
      "2016-03-07T19:39:15.617356: step 6200, loss 0.00767615, acc 1\n",
      "2016-03-07T19:39:15.621501: step 6200, loss 4.29153e-06, acc 1\n",
      "2016-03-07T19:39:15.625053: step 6200, loss 0.000821253, acc 1\n",
      "2016-03-07T19:39:15.628141: step 6200, loss 4.69674e-05, acc 1\n",
      "2016-03-07T19:39:15.631054: step 6200, loss 8.82145e-06, acc 1\n",
      "2016-03-07T19:39:15.634984: step 6200, loss 0.000346481, acc 1\n",
      "2016-03-07T19:39:15.638917: step 6200, loss 0.000214554, acc 1\n",
      "2016-03-07T19:39:15.643935: step 6200, loss 8.74958e-05, acc 1\n",
      "2016-03-07T19:39:15.647752: step 6200, loss 0.000428466, acc 1\n",
      "2016-03-07T19:39:15.651357: step 6200, loss 1.34706e-05, acc 1\n",
      "2016-03-07T19:39:15.655936: step 6200, loss 1.01327e-05, acc 1\n",
      "2016-03-07T19:39:15.659461: step 6200, loss 4.94706e-05, acc 1\n",
      "2016-03-07T19:39:15.664228: step 6200, loss 8.0463e-05, acc 1\n",
      "2016-03-07T19:39:15.667569: step 6200, loss 1.31129e-05, acc 1\n",
      "2016-03-07T19:39:15.671032: step 6200, loss 0.00104373, acc 1\n",
      "2016-03-07T19:39:15.675578: step 6200, loss 0.0039022, acc 1\n",
      "2016-03-07T19:39:15.678948: step 6200, loss 2.89674e-05, acc 1\n",
      "2016-03-07T19:39:15.682636: step 6200, loss 0.00189815, acc 1\n",
      "2016-03-07T19:39:15.686441: step 6200, loss 0.000105495, acc 1\n",
      "2016-03-07T19:39:15.689860: step 6200, loss 7.28342e-05, acc 1\n",
      "2016-03-07T19:39:15.693656: step 6200, loss 4.64915e-06, acc 1\n",
      "2016-03-07T19:39:15.697358: step 6200, loss 0.000298336, acc 1\n",
      "2016-03-07T19:39:15.700851: step 6200, loss 11.4342, acc 0\n",
      "2016-03-07T19:39:15.703975: step 6200, loss 0.00264295, acc 1\n",
      "2016-03-07T19:39:15.707971: step 6200, loss 0.000354108, acc 1\n",
      "2016-03-07T19:39:15.711897: step 6200, loss 0.00149151, acc 1\n",
      "2016-03-07T19:39:15.715449: step 6200, loss 0.0013308, acc 1\n",
      "2016-03-07T19:39:15.719130: step 6200, loss 0.00019465, acc 1\n",
      "2016-03-07T19:39:15.722597: step 6200, loss 0.000410949, acc 1\n",
      "2016-03-07T19:39:15.727029: step 6200, loss 5.29275e-05, acc 1\n",
      "2016-03-07T19:39:15.732144: step 6200, loss 0.000480775, acc 1\n",
      "2016-03-07T19:39:15.736031: step 6200, loss 1.38282e-05, acc 1\n",
      "2016-03-07T19:39:15.740013: step 6200, loss 0.007811, acc 1\n",
      "2016-03-07T19:39:15.743726: step 6200, loss 13.6152, acc 0\n",
      "2016-03-07T19:39:15.747346: step 6200, loss 9.41749e-06, acc 1\n",
      "2016-03-07T19:39:15.751035: step 6200, loss 0.000105614, acc 1\n",
      "2016-03-07T19:39:15.754152: step 6200, loss 1.15632e-05, acc 1\n",
      "2016-03-07T19:39:15.757821: step 6200, loss 2.58681e-05, acc 1\n",
      "2016-03-07T19:39:15.761783: step 6200, loss 0.000121109, acc 1\n",
      "2016-03-07T19:39:15.765005: step 6200, loss 4.4213, acc 0\n",
      "2016-03-07T19:39:15.768163: step 6200, loss 4.75634e-05, acc 1\n",
      "2016-03-07T19:39:15.771946: step 6200, loss 0.171721, acc 1\n",
      "2016-03-07T19:39:15.775236: step 6200, loss 3.13144, acc 0\n",
      "2016-03-07T19:39:15.780411: step 6200, loss 1.02519e-05, acc 1\n",
      "2016-03-07T19:39:15.784160: step 6200, loss 7.62937e-06, acc 1\n",
      "2016-03-07T19:39:15.787594: step 6200, loss 3.52853e-05, acc 1\n",
      "2016-03-07T19:39:15.790983: step 6200, loss 0.000241846, acc 1\n",
      "2016-03-07T19:39:15.794592: step 6200, loss 6.91411e-06, acc 1\n",
      "2016-03-07T19:39:15.799001: step 6200, loss 10.3236, acc 0\n",
      "2016-03-07T19:39:15.802744: step 6200, loss 0.000150788, acc 1\n",
      "2016-03-07T19:39:15.806004: step 6200, loss 3.33785e-06, acc 1\n",
      "2016-03-07T19:39:15.809177: step 6200, loss 9.89432e-06, acc 1\n",
      "2016-03-07T19:39:15.812416: step 6200, loss 0.000362331, acc 1\n",
      "2016-03-07T19:39:15.815872: step 6200, loss 2.36032e-05, acc 1\n",
      "2016-03-07T19:39:15.819240: step 6200, loss 9.64357e-05, acc 1\n",
      "2016-03-07T19:39:15.823680: step 6200, loss 5.59832, acc 0\n",
      "2016-03-07T19:39:15.827147: step 6200, loss 8.55886e-05, acc 1\n",
      "2016-03-07T19:39:15.832981: step 6200, loss 0.00101384, acc 1\n",
      "2016-03-07T19:39:15.837666: step 6200, loss 0.00131842, acc 1\n",
      "2016-03-07T19:39:15.841265: step 6200, loss 0.000108713, acc 1\n",
      "2016-03-07T19:39:15.845008: step 6200, loss 2.37224e-05, acc 1\n",
      "2016-03-07T19:39:15.848143: step 6200, loss 0.384247, acc 1\n",
      "2016-03-07T19:39:15.851837: step 6200, loss 9.42901e-05, acc 1\n",
      "2016-03-07T19:39:15.855148: step 6200, loss 0.000441811, acc 1\n",
      "2016-03-07T19:39:15.858466: step 6200, loss 0.000879616, acc 1\n",
      "2016-03-07T19:39:15.862164: step 6200, loss 4.208e-05, acc 1\n",
      "2016-03-07T19:39:15.865571: step 6200, loss 0.000909153, acc 1\n",
      "2016-03-07T19:39:15.869722: step 6200, loss 0.00526933, acc 1\n",
      "2016-03-07T19:39:15.873575: step 6200, loss 0.0050714, acc 1\n",
      "2016-03-07T19:39:15.877314: step 6200, loss 2.01462e-05, acc 1\n",
      "2016-03-07T19:39:15.880776: step 6200, loss 9.67933e-05, acc 1\n",
      "2016-03-07T19:39:15.886148: step 6200, loss 0.000559888, acc 1\n",
      "2016-03-07T19:39:15.889404: step 6200, loss 3.47408, acc 0\n",
      "2016-03-07T19:39:15.893184: step 6200, loss 0.000184757, acc 1\n",
      "2016-03-07T19:39:15.896392: step 6200, loss 9.41709e-05, acc 1\n",
      "2016-03-07T19:39:15.900180: step 6200, loss 7.24766e-05, acc 1\n",
      "2016-03-07T19:39:15.904197: step 6200, loss 7.78406e-05, acc 1\n",
      "2016-03-07T19:39:15.907663: step 6200, loss 0.0306994, acc 1\n",
      "2016-03-07T19:39:15.910774: step 6200, loss 0.00318318, acc 1\n",
      "2016-03-07T19:39:15.914537: step 6200, loss 0.00526127, acc 1\n",
      "2016-03-07T19:39:15.918257: step 6200, loss 5.96046e-07, acc 1\n",
      "2016-03-07T19:39:15.921381: step 6200, loss 6.17485e-05, acc 1\n",
      "2016-03-07T19:39:15.925181: step 6200, loss 0.000156629, acc 1\n",
      "2016-03-07T19:39:15.930429: step 6200, loss 3.2782e-05, acc 1\n",
      "2016-03-07T19:39:15.934181: step 6200, loss 7.39095e-06, acc 1\n",
      "2016-03-07T19:39:15.939524: step 6200, loss 0.00477021, acc 1\n",
      "2016-03-07T19:39:15.943074: step 6200, loss 9.24999, acc 0\n",
      "2016-03-07T19:39:15.946573: step 6200, loss 0.000439786, acc 1\n",
      "2016-03-07T19:39:15.950077: step 6200, loss 9.69125e-05, acc 1\n",
      "2016-03-07T19:39:15.953810: step 6200, loss 7.11366, acc 0\n",
      "2016-03-07T19:39:15.957423: step 6200, loss 0.00195158, acc 1\n",
      "2016-03-07T19:39:15.961206: step 6200, loss 0.000301435, acc 1\n",
      "2016-03-07T19:39:15.964653: step 6200, loss 2.07422e-05, acc 1\n",
      "2016-03-07T19:39:15.968210: step 6200, loss 8.8983, acc 0\n",
      "2016-03-07T19:39:15.971139: step 6200, loss 0.000597537, acc 1\n",
      "2016-03-07T19:39:15.974910: step 6200, loss 1.45434e-05, acc 1\n",
      "2016-03-07T19:39:15.978482: step 6200, loss 13.026, acc 0\n",
      "2016-03-07T19:39:15.982473: step 6200, loss 0.000276527, acc 1\n",
      "2016-03-07T19:39:15.986519: step 6200, loss 2.50339e-06, acc 1\n",
      "2016-03-07T19:39:15.991773: step 6200, loss 0.000522834, acc 1\n",
      "2016-03-07T19:39:15.996641: step 6200, loss 0.00206805, acc 1\n",
      "2016-03-07T19:39:16.000245: step 6200, loss 0.00050806, acc 1\n",
      "2016-03-07T19:39:16.003725: step 6200, loss 0.00061636, acc 1\n",
      "2016-03-07T19:39:16.006972: step 6200, loss 0.00296296, acc 1\n",
      "2016-03-07T19:39:16.010466: step 6200, loss 9.7596, acc 0\n",
      "2016-03-07T19:39:16.014726: step 6200, loss 2.38418e-06, acc 1\n",
      "2016-03-07T19:39:16.018142: step 6200, loss 1.43051e-06, acc 1\n",
      "2016-03-07T19:39:16.021624: step 6200, loss 7.81982e-05, acc 1\n",
      "2016-03-07T19:39:16.024706: step 6200, loss 3.00403e-05, acc 1\n",
      "2016-03-07T19:39:16.028404: step 6200, loss 0.000101442, acc 1\n",
      "2016-03-07T19:39:16.031496: step 6200, loss 5.853e-05, acc 1\n",
      "2016-03-07T19:39:16.035276: step 6200, loss 0.000161635, acc 1\n",
      "2016-03-07T19:39:16.040272: step 6200, loss 5.47156e-05, acc 1\n",
      "2016-03-07T19:39:16.044693: step 6200, loss 1.85965e-05, acc 1\n",
      "2016-03-07T19:39:16.048731: step 6200, loss 0.00641608, acc 1\n",
      "2016-03-07T19:39:16.051916: step 6200, loss 5.7338e-05, acc 1\n",
      "2016-03-07T19:39:16.055581: step 6200, loss 0.000647931, acc 1\n",
      "2016-03-07T19:39:16.058801: step 6200, loss 0.000117295, acc 1\n",
      "2016-03-07T19:39:16.062720: step 6200, loss 7.31918e-05, acc 1\n",
      "2016-03-07T19:39:16.066105: step 6200, loss 0.000377941, acc 1\n",
      "2016-03-07T19:39:16.069873: step 6200, loss 0.000676284, acc 1\n",
      "2016-03-07T19:39:16.073757: step 6200, loss 0.000368051, acc 1\n",
      "2016-03-07T19:39:16.077220: step 6200, loss 6.23445e-05, acc 1\n",
      "2016-03-07T19:39:16.080834: step 6200, loss 9.20053, acc 0\n",
      "2016-03-07T19:39:16.084595: step 6200, loss 4.72058e-05, acc 1\n",
      "2016-03-07T19:39:16.087845: step 6200, loss 0.0111195, acc 1\n",
      "2016-03-07T19:39:16.091278: step 6200, loss 1.47818e-05, acc 1\n",
      "2016-03-07T19:39:16.096827: step 6200, loss 0.000627202, acc 1\n",
      "2016-03-07T19:39:16.100934: step 6200, loss 0.00062196, acc 1\n",
      "2016-03-07T19:39:16.105147: step 6200, loss 3.63582e-05, acc 1\n",
      "2016-03-07T19:39:16.108648: step 6200, loss 9.59342, acc 0\n",
      "2016-03-07T19:39:16.112271: step 6200, loss 0.00561553, acc 1\n",
      "2016-03-07T19:39:16.116266: step 6200, loss 0.0141176, acc 1\n",
      "2016-03-07T19:39:16.119420: step 6200, loss 0.000255195, acc 1\n",
      "2016-03-07T19:39:16.122979: step 6200, loss 0.000708925, acc 1\n",
      "2016-03-07T19:39:16.126637: step 6200, loss 2.28879e-05, acc 1\n",
      "2016-03-07T19:39:16.130292: step 6200, loss 2.27687e-05, acc 1\n",
      "2016-03-07T19:39:16.133738: step 6200, loss 5.11395e-05, acc 1\n",
      "2016-03-07T19:39:16.136556: step 6200, loss 7.55758e-05, acc 1\n",
      "2016-03-07T19:39:16.139680: step 6200, loss 12.2438, acc 0\n",
      "2016-03-07T19:39:16.142913: step 6200, loss 0.0284135, acc 1\n",
      "2016-03-07T19:39:16.147468: step 6200, loss 0.000508537, acc 1\n",
      "2016-03-07T19:39:16.151018: step 6200, loss 1.69276e-05, acc 1\n",
      "2016-03-07T19:39:16.154708: step 6200, loss 4.76836e-06, acc 1\n",
      "2016-03-07T19:39:16.158079: step 6200, loss 6.19886e-06, acc 1\n",
      "2016-03-07T19:39:16.161380: step 6200, loss 0.000195842, acc 1\n",
      "2016-03-07T19:39:16.165218: step 6200, loss 0.0012583, acc 1\n",
      "2016-03-07T19:39:16.168674: step 6200, loss 0.000838286, acc 1\n",
      "2016-03-07T19:39:16.172096: step 6200, loss 0.00131449, acc 1\n",
      "2016-03-07T19:39:16.175940: step 6200, loss 1.09381, acc 0\n",
      "2016-03-07T19:39:16.179358: step 6200, loss 2.20535e-05, acc 1\n",
      "2016-03-07T19:39:16.182467: step 6200, loss 0.0012377, acc 1\n",
      "2016-03-07T19:39:16.186180: step 6200, loss 10.379, acc 0\n",
      "2016-03-07T19:39:16.189846: step 6200, loss 3.75502e-05, acc 1\n",
      "2016-03-07T19:39:16.193208: step 6200, loss 0.00100455, acc 1\n",
      "2016-03-07T19:39:16.197072: step 6200, loss 0.000713213, acc 1\n",
      "2016-03-07T19:39:16.202149: step 6200, loss 0.000382946, acc 1\n",
      "2016-03-07T19:39:16.205635: step 6200, loss 0.00024852, acc 1\n",
      "2016-03-07T19:39:16.209721: step 6200, loss 2.4676e-05, acc 1\n",
      "2016-03-07T19:39:16.213082: step 6200, loss 0.0333994, acc 1\n",
      "2016-03-07T19:39:16.216651: step 6200, loss 3.69548e-06, acc 1\n",
      "2016-03-07T19:39:16.220118: step 6200, loss 6.79491e-06, acc 1\n",
      "2016-03-07T19:39:16.224575: step 6200, loss 8.34462e-06, acc 1\n",
      "2016-03-07T19:39:16.227968: step 6200, loss 0.00157471, acc 1\n",
      "2016-03-07T19:39:16.231400: step 6200, loss 0.00146853, acc 1\n",
      "2016-03-07T19:39:16.235059: step 6200, loss 0.000215507, acc 1\n",
      "2016-03-07T19:39:16.238290: step 6200, loss 0.000275097, acc 1\n",
      "2016-03-07T19:39:16.241857: step 6200, loss 0.000352916, acc 1\n",
      "2016-03-07T19:39:16.245263: step 6200, loss 8.17683, acc 0\n",
      "2016-03-07T19:39:16.248657: step 6200, loss 7.17047, acc 0\n",
      "2016-03-07T19:39:16.253158: step 6200, loss 1.25169e-05, acc 1\n",
      "2016-03-07T19:39:16.256559: step 6200, loss 0.000150908, acc 1\n",
      "2016-03-07T19:39:16.259732: step 6200, loss 0.00949592, acc 1\n",
      "2016-03-07T19:39:16.263327: step 6200, loss 7.70077, acc 0\n",
      "2016-03-07T19:39:16.267102: step 6200, loss 2.92059e-05, acc 1\n",
      "2016-03-07T19:39:16.270997: step 6200, loss 0.00318318, acc 1\n",
      "2016-03-07T19:39:16.274500: step 6200, loss 0.00108291, acc 1\n",
      "2016-03-07T19:39:16.278138: step 6200, loss 0.00107445, acc 1\n",
      "2016-03-07T19:39:16.281180: step 6200, loss 0.0129781, acc 1\n",
      "2016-03-07T19:39:16.284973: step 6200, loss 9.40517e-05, acc 1\n",
      "2016-03-07T19:39:16.288626: step 6200, loss 0.00951954, acc 1\n",
      "2016-03-07T19:39:16.291729: step 6200, loss 0.0834557, acc 1\n",
      "2016-03-07T19:39:16.295165: step 6200, loss 0.000672948, acc 1\n",
      "2016-03-07T19:39:16.298546: step 6200, loss 0.0106232, acc 1\n",
      "2016-03-07T19:39:16.301911: step 6200, loss 0.00152043, acc 1\n",
      "2016-03-07T19:39:16.307339: step 6200, loss 7.99862e-05, acc 1\n",
      "2016-03-07T19:39:16.311038: step 6200, loss 2.83714e-05, acc 1\n",
      "2016-03-07T19:39:16.314683: step 6200, loss 0.00132497, acc 1\n",
      "2016-03-07T19:39:16.318041: step 6200, loss 3.67158e-05, acc 1\n",
      "2016-03-07T19:39:16.321398: step 6200, loss 3.09939e-05, acc 1\n",
      "2016-03-07T19:39:16.324771: step 6200, loss 2.37224e-05, acc 1\n",
      "2016-03-07T19:39:16.328289: step 6200, loss 8.34462e-06, acc 1\n",
      "2016-03-07T19:39:16.331570: step 6200, loss 9.00438, acc 0\n",
      "2016-03-07T19:39:16.334989: step 6200, loss 0.000358279, acc 1\n",
      "2016-03-07T19:39:16.338457: step 6200, loss 1.87157e-05, acc 1\n",
      "2016-03-07T19:39:16.341658: step 6200, loss 1.90733e-05, acc 1\n",
      "2016-03-07T19:39:16.345323: step 6200, loss 0.00282508, acc 1\n",
      "2016-03-07T19:39:16.348784: step 6200, loss 9.73892e-05, acc 1\n",
      "2016-03-07T19:39:16.352097: step 6200, loss 2.7537e-05, acc 1\n",
      "2016-03-07T19:39:16.355682: step 6200, loss 0.00442775, acc 1\n",
      "2016-03-07T19:39:16.359960: step 6200, loss 2.18151e-05, acc 1\n",
      "2016-03-07T19:39:16.363852: step 6200, loss 2.18151e-05, acc 1\n",
      "2016-03-07T19:39:16.368727: step 6200, loss 0.001003, acc 1\n",
      "2016-03-07T19:39:16.372428: step 6200, loss 9.9388, acc 0\n",
      "2016-03-07T19:39:16.376111: step 6200, loss 0.000233385, acc 1\n",
      "2016-03-07T19:39:16.379685: step 6200, loss 0.130764, acc 1\n",
      "2016-03-07T19:39:16.383113: step 6200, loss 0.000333492, acc 1\n",
      "2016-03-07T19:39:16.387023: step 6200, loss 1.80004e-05, acc 1\n",
      "2016-03-07T19:39:16.390648: step 6200, loss 9.47669e-05, acc 1\n",
      "2016-03-07T19:39:16.394418: step 6200, loss 8.68818, acc 0\n",
      "2016-03-07T19:39:16.397671: step 6200, loss 0.00577294, acc 1\n",
      "2016-03-07T19:39:16.401103: step 6200, loss 2.90866e-05, acc 1\n",
      "2016-03-07T19:39:16.404760: step 6200, loss 0.000410234, acc 1\n",
      "2016-03-07T19:39:16.408088: step 6200, loss 0.000117057, acc 1\n",
      "2016-03-07T19:39:16.412785: step 6200, loss 3.95767e-05, acc 1\n",
      "2016-03-07T19:39:16.416322: step 6200, loss 0.000775513, acc 1\n",
      "2016-03-07T19:39:16.419995: step 6200, loss 0.000619339, acc 1\n",
      "2016-03-07T19:39:16.423632: step 6200, loss 0.0373012, acc 1\n",
      "2016-03-07T19:39:16.427154: step 6200, loss 7.89952, acc 0\n",
      "2016-03-07T19:39:16.430727: step 6200, loss 2.31886, acc 0\n",
      "2016-03-07T19:39:16.434233: step 6200, loss 0.000816965, acc 1\n",
      "2016-03-07T19:39:16.437667: step 6200, loss 8.09208, acc 0\n",
      "2016-03-07T19:39:16.441339: step 6200, loss 0.0135661, acc 1\n",
      "2016-03-07T19:39:16.446431: step 6200, loss 1.02519e-05, acc 1\n",
      "2016-03-07T19:39:16.449562: step 6200, loss 13.1513, acc 0\n",
      "2016-03-07T19:39:16.453412: step 6200, loss 4.88771, acc 0\n",
      "2016-03-07T19:39:16.456681: step 6200, loss 0.000820538, acc 1\n",
      "2016-03-07T19:39:16.460228: step 6200, loss 0.000822325, acc 1\n",
      "2016-03-07T19:39:16.465736: step 6200, loss 3.2782e-05, acc 1\n",
      "2016-03-07T19:39:16.469256: step 6200, loss 4.76837e-07, acc 1\n",
      "2016-03-07T19:39:16.473525: step 6200, loss 5.75764e-05, acc 1\n",
      "2016-03-07T19:39:16.477607: step 6200, loss 2.26495e-05, acc 1\n",
      "2016-03-07T19:39:16.481178: step 6200, loss 0.00108565, acc 1\n",
      "2016-03-07T19:39:16.484718: step 6200, loss 12.5099, acc 0\n",
      "2016-03-07T19:39:16.488215: step 6200, loss 7.49798e-05, acc 1\n",
      "2016-03-07T19:39:16.491276: step 6200, loss 8.3443e-05, acc 1\n",
      "2016-03-07T19:39:16.495028: step 6200, loss 0.00110696, acc 1\n",
      "2016-03-07T19:39:16.498399: step 6200, loss 0.000924636, acc 1\n",
      "2016-03-07T19:39:16.501650: step 6200, loss 0.000132552, acc 1\n",
      "2016-03-07T19:39:16.505378: step 6200, loss 3.42125e-05, acc 1\n",
      "2016-03-07T19:39:16.508900: step 6200, loss 0.000133267, acc 1\n",
      "2016-03-07T19:39:16.512742: step 6200, loss 0.12062, acc 1\n",
      "2016-03-07T19:39:16.518305: step 6200, loss 7.42244, acc 0\n",
      "2016-03-07T19:39:16.522010: step 6200, loss 0.00588684, acc 1\n",
      "2016-03-07T19:39:16.525187: step 6200, loss 0.000202754, acc 1\n",
      "2016-03-07T19:39:16.529066: step 6200, loss 0.00162053, acc 1\n",
      "2016-03-07T19:39:16.532511: step 6200, loss 0.000112289, acc 1\n",
      "2016-03-07T19:39:16.535981: step 6200, loss 2.24111e-05, acc 1\n",
      "2016-03-07T19:39:16.539654: step 6200, loss 3.8027e-05, acc 1\n",
      "2016-03-07T19:39:16.544597: step 6200, loss 0.00089105, acc 1\n",
      "2016-03-07T19:39:16.548264: step 6200, loss 0.15239, acc 1\n",
      "2016-03-07T19:39:16.551326: step 6200, loss 0.0582453, acc 1\n",
      "2016-03-07T19:39:16.554607: step 6200, loss 0.000277481, acc 1\n",
      "2016-03-07T19:39:16.558203: step 6200, loss 9.50053e-05, acc 1\n",
      "2016-03-07T19:39:16.562122: step 6200, loss 0.00166623, acc 1\n",
      "2016-03-07T19:39:16.565208: step 6200, loss 0.00011062, acc 1\n",
      "2016-03-07T19:39:16.569257: step 6200, loss 2.86102e-06, acc 1\n",
      "2016-03-07T19:39:16.573165: step 6200, loss 0.000140299, acc 1\n",
      "2016-03-07T19:39:16.577191: step 6200, loss 0.000333134, acc 1\n",
      "2016-03-07T19:39:16.580773: step 6200, loss 0.000433113, acc 1\n",
      "2016-03-07T19:39:16.584327: step 6200, loss 0.000580738, acc 1\n",
      "2016-03-07T19:39:16.587817: step 6200, loss 0.00041083, acc 1\n",
      "2016-03-07T19:39:16.591116: step 6200, loss 0.00964198, acc 1\n",
      "2016-03-07T19:39:16.594457: step 6200, loss 0.000205138, acc 1\n",
      "2016-03-07T19:39:16.598222: step 6200, loss 0.0073961, acc 1\n",
      "2016-03-07T19:39:16.601601: step 6200, loss 3.2186e-05, acc 1\n",
      "2016-03-07T19:39:16.605104: step 6200, loss 0.000395458, acc 1\n",
      "2016-03-07T19:39:16.608589: step 6200, loss 9.53674e-07, acc 1\n",
      "2016-03-07T19:39:16.612403: step 6200, loss 5.555e-05, acc 1\n",
      "2016-03-07T19:39:16.616569: step 6200, loss 0.0279147, acc 1\n",
      "2016-03-07T19:39:16.621427: step 6200, loss 0.000152338, acc 1\n",
      "2016-03-07T19:39:16.624667: step 6200, loss 1.56163e-05, acc 1\n",
      "2016-03-07T19:39:16.628517: step 6200, loss 2.51528e-05, acc 1\n",
      "2016-03-07T19:39:16.632270: step 6200, loss 6.40133e-05, acc 1\n",
      "2016-03-07T19:39:16.635574: step 6200, loss 3.17092e-05, acc 1\n",
      "2016-03-07T19:39:16.639622: step 6200, loss 0.000634469, acc 1\n",
      "2016-03-07T19:39:16.643588: step 6200, loss 0.000941787, acc 1\n",
      "2016-03-07T19:39:16.647306: step 6200, loss 0.000320502, acc 1\n",
      "2016-03-07T19:39:16.650893: step 6200, loss 0.000111454, acc 1\n",
      "2016-03-07T19:39:16.654270: step 6200, loss 0.00193349, acc 1\n",
      "2016-03-07T19:39:16.658211: step 6200, loss 8.54002, acc 0\n",
      "2016-03-07T19:39:16.661779: step 6200, loss 1.1444e-05, acc 1\n",
      "2016-03-07T19:39:16.665056: step 6200, loss 0.0191759, acc 1\n",
      "2016-03-07T19:39:16.668392: step 6200, loss 1.06096e-05, acc 1\n",
      "2016-03-07T19:39:16.672625: step 6200, loss 9.94156e-05, acc 1\n",
      "2016-03-07T19:39:16.676353: step 6200, loss 1.78812e-05, acc 1\n",
      "2016-03-07T19:39:16.679896: step 6200, loss 3.05171e-05, acc 1\n",
      "2016-03-07T19:39:16.683934: step 6200, loss 6.56822e-05, acc 1\n",
      "2016-03-07T19:39:16.687264: step 6200, loss 0.000261153, acc 1\n",
      "2016-03-07T19:39:16.690988: step 6200, loss 0.0210396, acc 1\n",
      "2016-03-07T19:39:16.694890: step 6200, loss 0.000102634, acc 1\n",
      "2016-03-07T19:39:16.698546: step 6200, loss 1.7762e-05, acc 1\n",
      "2016-03-07T19:39:16.702090: step 6200, loss 2.01462e-05, acc 1\n",
      "2016-03-07T19:39:16.705253: step 6200, loss 0.0519726, acc 1\n",
      "2016-03-07T19:39:16.708823: step 6200, loss 5.0875, acc 0\n",
      "2016-03-07T19:39:16.712624: step 6200, loss 4.17232e-06, acc 1\n",
      "2016-03-07T19:39:16.716204: step 6200, loss 6.68742e-05, acc 1\n",
      "2016-03-07T19:39:16.719517: step 6200, loss 3.57621e-05, acc 1\n",
      "2016-03-07T19:39:16.723599: step 6200, loss 0.000195127, acc 1\n",
      "2016-03-07T19:39:16.727373: step 6200, loss 0.0526421, acc 1\n",
      "2016-03-07T19:39:16.730815: step 6200, loss 0.0410547, acc 1\n",
      "2016-03-07T19:39:16.734574: step 6200, loss 1.15632e-05, acc 1\n",
      "2016-03-07T19:39:16.738346: step 6200, loss 0.00094798, acc 1\n",
      "2016-03-07T19:39:16.741663: step 6200, loss 2.50339e-06, acc 1\n",
      "2016-03-07T19:39:16.745373: step 6200, loss 5.03198, acc 0\n",
      "2016-03-07T19:39:16.748842: step 6200, loss 8.94066e-06, acc 1\n",
      "2016-03-07T19:39:16.752561: step 6200, loss 0.000271641, acc 1\n",
      "2016-03-07T19:39:16.755840: step 6200, loss 1.34706e-05, acc 1\n",
      "2016-03-07T19:39:16.759744: step 6200, loss 1.00135e-05, acc 1\n",
      "2016-03-07T19:39:16.763382: step 6200, loss 4.1484e-05, acc 1\n",
      "2016-03-07T19:39:16.766626: step 6200, loss 7.92977, acc 0\n",
      "2016-03-07T19:39:16.770394: step 6200, loss 1.23977e-05, acc 1\n",
      "2016-03-07T19:39:16.774817: step 6200, loss 9.17907e-06, acc 1\n",
      "2016-03-07T19:39:16.778362: step 6200, loss 0.000318238, acc 1\n",
      "2016-03-07T19:39:16.782008: step 6200, loss 1.50203e-05, acc 1\n",
      "2016-03-07T19:39:16.785612: step 6200, loss 9.19061e-05, acc 1\n",
      "2016-03-07T19:39:16.789350: step 6200, loss 0.0525289, acc 1\n",
      "2016-03-07T19:39:16.792927: step 6200, loss 0.0014283, acc 1\n",
      "2016-03-07T19:39:16.796379: step 6200, loss 5.14971e-05, acc 1\n",
      "2016-03-07T19:39:16.799633: step 6200, loss 1.81196e-05, acc 1\n",
      "2016-03-07T19:39:16.803431: step 6200, loss 0.000149001, acc 1\n",
      "2016-03-07T19:39:16.807564: step 6200, loss 3.85039e-05, acc 1\n",
      "2016-03-07T19:39:16.810817: step 6200, loss 6.45287, acc 0\n",
      "2016-03-07T19:39:16.814460: step 6200, loss 0.000216222, acc 1\n",
      "2016-03-07T19:39:16.818147: step 6200, loss 0.000127069, acc 1\n",
      "2016-03-07T19:39:16.820989: step 6200, loss 3.45706e-06, acc 1\n",
      "2016-03-07T19:39:16.824897: step 6200, loss 6.83094, acc 0\n",
      "2016-03-07T19:39:16.830626: step 6200, loss 0.000584908, acc 1\n",
      "2016-03-07T19:39:16.834420: step 6200, loss 6.63974e-05, acc 1\n",
      "2016-03-07T19:39:16.837656: step 6200, loss 1.76428e-05, acc 1\n",
      "2016-03-07T19:39:16.841644: step 6200, loss 0.000632682, acc 1\n",
      "2016-03-07T19:39:16.845189: step 6200, loss 0.000175461, acc 1\n",
      "2016-03-07T19:39:16.848415: step 6200, loss 0.000176653, acc 1\n",
      "2016-03-07T19:39:16.852165: step 6200, loss 5.00678e-06, acc 1\n",
      "True Positives 1\n",
      "True Negatives 6\n",
      "False Positives 1751\n",
      "False Negatives 218\n",
      "Sensitivity: 0.00456621004566\n",
      "Specificity: 0.00341491178145\n"
     ]
    }
   ],
   "source": [
    "    accuracies = []\n",
    "    for y, x in zip(even_y_dev, even_x_dev):\n",
    "        sent = []\n",
    "        for word in x:\n",
    "            sent.append(vocabulary_inv[word])\n",
    "        print(' '.join(sent))\n",
    "        print(\"example\" if y[0] == 0 and y[1] == 1 else \"nonexample\")\n",
    "        dev_step([x], [y], writer=dev_summary_writer)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "    \n",
    "    for y, x in zip(y_dev, x_dev):\n",
    "        accuracies.append(dev_step([x], [y], writer=dev_summary_writer))\n",
    "        \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for a, y in zip(accuracies, y_dev):\n",
    "        if(a == 1.0 and y[0] == 0 and y[1] == 1):\n",
    "            tp += 1\n",
    "        elif(a == 0.0 and y[0] == 0 and y[1] == 1):\n",
    "            fn += 1\n",
    "        elif(a == 1.0 and y[0] == 1 and y[1] == 0):\n",
    "            fp += 1\n",
    "        elif(a == 0.0 and y[0] == 1 and y[1] == 0):\n",
    "            tn +=1 \n",
    "            \n",
    "            \n",
    "    print(\"True Positives %s\" % tp)\n",
    "    print(\"True Negatives %s\" % tn)\n",
    "    print(\"False Positives %s\" % fp)\n",
    "    print(\"False Negatives %s\" % fn)\n",
    "    sensitivity = (tp/(tp+float(fn)))\n",
    "    print(\"Sensitivity: %s\" % sensitivity)\n",
    "    specificity = (tn/(tn+float(fp)))\n",
    "    print(\"Specificity: %s\" % specificity)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
