{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: CNN\n",
    "\n",
    "## Overview: \n",
    "\n",
    "1. Begin by importing and getting the embeddings and word to index mappings we created in [Notebook 1: Embed Words](Notebook_1_Embed_Words.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "from tensorflow.models.rnn.rnn_cell import BasicLSTMCell, LSTMCell \n",
    "import itertools\n",
    "from collections import Counter\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embeddings = None\n",
    "mappings = None\n",
    "rows = None\n",
    "\n",
    "with open(\"word_embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "with open(\"word_mappings.pkl\", \"rb\") as f:\n",
    "    mappings = pickle.load(f)\n",
    "    \n",
    "\n",
    "urlFinder = re.compile('\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*')\n",
    "atNameFinder = re.compile(r'@([A-Za-z0-9_]+)')\n",
    "atNameCounter = 0\n",
    "\n",
    "exclude_punc = set([\n",
    "        \"!\",\n",
    "        \"?\",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \":\",\n",
    "        \";\",\n",
    "        \"'\",\n",
    "        \"\\\"\",\n",
    "        \"‚Äú\",\n",
    "        \"‚Äô\",\n",
    "        \"-\"\n",
    "])\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "x = []\n",
    "y = []\n",
    "_y = []\n",
    "\n",
    "with open('data.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, delimiter=',')   \n",
    "    \n",
    "    for row in reader:\n",
    "        words = []\n",
    "        \n",
    "        for word in row[1] \\\n",
    "            .strip() \\\n",
    "            .replace(\"&amp;\", \"\") \\\n",
    "            .replace(\"&gt;\",\"\") \\\n",
    "            .replace(\"&lt;\", \"\") \\\n",
    "            .lower().split():\n",
    "            \n",
    "            if urlFinder.match(word):\n",
    "                words.append(\"<URL/>\")\n",
    "            else:\n",
    "                word = ''.join(ch for ch in word if ch not in exclude_punc)\n",
    "                words.append(word)\n",
    "        sentences.append(words)\n",
    "        labels.append(([0, 1] if row[0] == \"example\" else [1, 0]))\n",
    "        _y.append(1 if row[0] == \"example\" else 0)\n",
    "\n",
    "\n",
    "sequence_length = max(len(i) for i in sentences)\n",
    "padded_sentences = []\n",
    "for i in range(len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    num_padding = sequence_length - len(sentence)\n",
    "    new_sentence = sentence + [\"<PAD/>\"] * num_padding\n",
    "    padded_sentences.append(new_sentence)\n",
    "    \n",
    " \n",
    "word_counts = Counter(itertools.chain(*padded_sentences))\n",
    "\n",
    "# Mapping from index to word\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "# Mapping from word to index\n",
    "vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "\n",
    "x = np.array([[vocabulary[word] for word in sentence] for sentence in padded_sentences])\n",
    "y = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_data = data[shuffle_indices]\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Writing to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:06:56.787741: step 100, loss 1.20554, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:07:03.289077: step 200, loss 1.15832, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:07:09.929667: step 300, loss 1.11845, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:07:16.682516: step 400, loss 1.05361, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:07:23.168315: step 500, loss 1.01142, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:07:29.639596: step 600, loss 0.963769, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:07:36.117989: step 700, loss 0.906229, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:07:43.098961: step 800, loss 0.863135, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:07:49.481394: step 900, loss 0.826185, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:07:55.984837: step 1000, loss 0.788232, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-1000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:08:02.380327: step 1100, loss 0.757136, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-1100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:08:09.350431: step 1200, loss 0.708539, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-1200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:08:16.192540: step 1300, loss 0.681844, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-1300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:08:23.109342: step 1400, loss 0.659763, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-1400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:08:30.039577: step 1500, loss 0.629232, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-1500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:08:36.670429: step 1600, loss 0.606577, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-1600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:08:43.018576: step 1700, loss 0.583092, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-1700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:08:49.703528: step 1800, loss 0.565555, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-1800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:08:56.016370: step 1900, loss 0.547501, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-1900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:09:02.515624: step 2000, loss 0.532545, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-2000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:09:09.164808: step 2100, loss 0.517671, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-2100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:09:15.638351: step 2200, loss 0.501658, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-2200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:09:22.154060: step 2300, loss 0.488543, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-2300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:09:28.678103: step 2400, loss 0.481143, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-2400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:09:35.417891: step 2500, loss 0.46818, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-2500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:09:41.974864: step 2600, loss 0.461239, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-2600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:09:48.707377: step 2700, loss 0.450734, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-2700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:09:55.351295: step 2800, loss 0.443474, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-2800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:10:02.246645: step 2900, loss 0.436994, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-2900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:10:09.015267: step 3000, loss 0.43384, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-3000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:10:15.506790: step 3100, loss 0.426966, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-3100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:10:21.835728: step 3200, loss 0.423572, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-3200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:10:28.278898: step 3300, loss 0.419531, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-3300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:10:34.624504: step 3400, loss 0.415425, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-3400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:10:41.162704: step 3500, loss 0.4109, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-3500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:10:47.595619: step 3600, loss 0.409578, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-3600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:10:54.242374: step 3700, loss 0.406008, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-3700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:11:00.890212: step 3800, loss 0.404775, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-3800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:11:07.641992: step 3900, loss 0.405962, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-3900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:11:13.998818: step 4000, loss 0.402787, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-4000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:11:20.410019: step 4100, loss 0.401938, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-4100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:11:26.999992: step 4200, loss 0.401222, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-4200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:11:33.400987: step 4300, loss 0.401342, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-4300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:11:39.995901: step 4400, loss 0.403248, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-4400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:11:46.496867: step 4500, loss 0.403492, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-4500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:11:52.916777: step 4600, loss 0.403652, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-4600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:11:59.428415: step 4700, loss 0.403616, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-4700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:12:05.907814: step 4800, loss 0.402647, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-4800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:12:12.607596: step 4900, loss 0.402496, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-4900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:12:19.536816: step 5000, loss 0.407444, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-5000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:12:26.202374: step 5100, loss 0.405585, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-5100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:12:32.549171: step 5200, loss 0.409448, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-5200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:12:39.092449: step 5300, loss 0.407707, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-5300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:12:45.621910: step 5400, loss 0.410702, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-5400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:12:51.904888: step 5500, loss 0.409023, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-5500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:12:58.685571: step 5600, loss 0.417923, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-5600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:13:04.975539: step 5700, loss 0.414725, acc 0.88917\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-5700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:13:11.410036: step 5800, loss 0.415675, acc 0.888664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-5800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:13:17.790324: step 5900, loss 0.41621, acc 0.888664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-5900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:13:24.360407: step 6000, loss 0.417209, acc 0.888664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-6000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:13:30.894141: step 6100, loss 0.419563, acc 0.888664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-6100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2016-03-08T02:13:37.411301: step 6200, loss 0.424993, acc 0.888664\n",
      "\n",
      "Saved model checkpoint to /notebooks/AdHClassification/Expiriment #1.3 - Non-Candidate Tweets Included/runs/1457402809/checkpoints/model-6200\n",
      "\n",
      "\n",
      "Final Evaluations:\n",
      "2016-03-08T02:13:38.158220: step 6200, loss 0.424993, acc 0.888664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 20  #Dimensionality of character embedding (default: 128)\n",
    "filter_sizes =  \"3,4,5\" #\"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "num_filters = 128  #\"Number of filters per filter size (default: 128)\")\n",
    "dropout_keep_prob = 0.5 #\"Dropout keep probability (default: 0.5)\")\n",
    "l2_reg_lambda = 0.0 #\"L2 regularizaion lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64 # \"Batch Size (default: 64)\")\n",
    "num_epochs = 200 #\"Number of training epochs (default: 200)\")\n",
    "evaluate_every = 100  #\"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "checkpoint_every = 100 # \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "allow_soft_placement = True # \"Allow device soft device placement\")\n",
    "log_device_placement = False  #\"Log placement of ops on devices\")\n",
    "display_train_steps = False # toggles output of training step results\n",
    "\n",
    "\n",
    "\n",
    "# Data Preparatopn\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "# Randomly shuffle data\n",
    "sss = StratifiedShuffleSplit(_y, 1, test_size=0.5, random_state=0)\n",
    "for train, test in sss:\n",
    "    x_train = np.random.permutation(x[train])\n",
    "    y_train = np.random.permutation(y[train])\n",
    "\n",
    "    x_dev = np.random.permutation(x[test])\n",
    "    y_dev = np.random.permutation(y[test])\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=len(vocabulary),\n",
    "            embedding_size=embedding_dim,\n",
    "            filter_sizes=map(int, filter_sizes.split(\",\")),\n",
    "            num_filters=num_filters,\n",
    "            l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph_def)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if(display_train_steps):\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return accuracy\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            zip(x_train, y_train), batch_size, num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "        \n",
    "        print(\"\\nFinal Evaluations:\")\n",
    "        dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "  \n",
    "        \n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438, 438)\n",
      "Balanced Evaluation:\n",
      "2016-03-08T02:13:38.265337: step 6200, loss 1.48243, acc 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        pos_x_dev = []\n",
    "        pos_y_dev = []\n",
    "\n",
    "        neg_x_dev = []\n",
    "        neg_y_dev = []\n",
    "\n",
    "        for y, x in zip(y_dev, x_dev):\n",
    "            if(y[0] == 0 and y[1] == 1):\n",
    "                pos_x_dev.append(x)\n",
    "                pos_y_dev.append(y)\n",
    "            else:\n",
    "                neg_x_dev.append(x)\n",
    "                neg_y_dev.append(y)\n",
    "        even_x_dev = np.array(pos_x_dev + neg_x_dev[:len(pos_x_dev)])\n",
    "        even_y_dev = np.array(pos_y_dev + neg_y_dev[:len(pos_y_dev)])\n",
    "        print(len(even_y_dev), len(even_x_dev))\n",
    "\n",
    "        print(\"Balanced Evaluation:\")\n",
    "        dev_step(even_x_dev, even_y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank you for your endorsement @paulteutulsr #bikersfortrump #votetrumpnv video <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.345061: step 6200, loss 2.5211, acc 0\n",
      "\n",
      "\n",
      "in 2014 public advocate @tishjames became the first woman of color to hold citywide office in new york city <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.347263: step 6200, loss 2.80481, acc 0\n",
      "\n",
      "\n",
      "the real winner is the grassroots who propelled us to a victory in iowa and a far stronger outcome in new hampshire than anyone predicted <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.349445: step 6200, loss 4.19407, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump is trafficking 9/11 conspiracy theories i‚Äôm offering detailed plans to defeat isis we need a serious commanderinchief <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.351393: step 6200, loss 2.35102, acc 0\n",
      "\n",
      "\n",
      "if youre in line to caucus do not leave you dont need id to caucus or to register to vote any issues call 7027784336 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.353412: step 6200, loss 2.98166, acc 0\n",
      "\n",
      "\n",
      "in 2015 @simone_biles leapt into record books as the worlds most decorated woman gymnast <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.355436: step 6200, loss 3.59743, acc 0\n",
      "\n",
      "\n",
      "@tdltdltdltdl marco cruz and ted rubio (easy to get the two politicians confused) looked like desperate panicked dc insiders tonight <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.357219: step 6200, loss 2.75299, acc 0\n",
      "\n",
      "\n",
      "i very much look forward to tomorrow‚Äôs debate in new hampshire‚Äîso many things to say so much at stake it will be an incredible evening <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.359496: step 6200, loss 3.23717, acc 0\n",
      "\n",
      "\n",
      "if were going to get into labels it wasnt very progressive to vote against the brady bill five times and immigration reform #demdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.361456: step 6200, loss 4.17941, acc 0\n",
      "\n",
      "\n",
      "@hillaryclinton –æ–∫ <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.363341: step 6200, loss 2.01927, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump common core is to children as a 10ft high bridge is to a trucker with a very large truck heading toward it total disaster <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.365210: step 6200, loss 3.90662, acc 0\n",
      "\n",
      "\n",
      "wondering if youre eligible to caucus in nevada help us win nevada #americatogether <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.366907: step 6200, loss 2.34488, acc 0\n",
      "\n",
      "\n",
      "we can #abolishtheirs with my simple flat tax rt if youre in #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.368608: step 6200, loss 3.84334, acc 0\n",
      "\n",
      "\n",
      "see you soon senator <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.370459: step 6200, loss 1.15126, acc 0\n",
      "\n",
      "\n",
      "find your caucus location by texting caucus to 82623 #americatogether <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.372112: step 6200, loss 2.29471, acc 0\n",
      "\n",
      "\n",
      "@berniesanders spending all his time apologizing to blm while the trump is getting 25% of the black vote <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.373684: step 6200, loss 2.38485, acc 0\n",
      "\n",
      "\n",
      "the children of flint deserve bright futures‚Äîand today those children need our help please chip in if youre able <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.375433: step 6200, loss 3.31818, acc 0\n",
      "\n",
      "\n",
      "@hillaryclinton #thetruthisoutthere #disclosure #therockefellerinitiative <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.377075: step 6200, loss 3.74458, acc 0\n",
      "\n",
      "\n",
      "your next president chris christie  @charliebakerma watch <URL/> #fitn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.378888: step 6200, loss 3.57914, acc 0\n",
      "\n",
      "\n",
      "look real hard‚Äîyou still wont find anyone supporting equal pay paid leave or reproductive rights at #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.380606: step 6200, loss 2.8, acc 0\n",
      "\n",
      "\n",
      "tomorrow were starting in rock hill then on to florence lexington for town halls rsvp to attend <URL/> #scformarco <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.382641: step 6200, loss 2.22808, acc 0\n",
      "\n",
      "\n",
      "great to meet everyone while having breakfast @chezvachon this morning #fitn #votetrumpnh <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.384403: step 6200, loss 3.49761, acc 0\n",
      "\n",
      "\n",
      "great poll thank you north carolina #votetrumpnc on 3/15 trump 36% cruz 18% rubio 18% carson 10% kasich 7% via @surveyusa <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.386208: step 6200, loss 3.32082, acc 0\n",
      "\n",
      "\n",
      "so proud to have congressman jim clyburn on this team <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.388188: step 6200, loss 3.12576, acc 0\n",
      "\n",
      "\n",
      "its been less than a week since a mass shooting tore apart a community and now another this has to end praying for hesston ks h <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.390029: step 6200, loss 2.67342, acc 0\n",
      "\n",
      "\n",
      "i will unite this country around common purpose because i did it as governor of florida #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.391948: step 6200, loss 4.33694, acc 0\n",
      "\n",
      "\n",
      "43 has arrived follow along here ‚Üí <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.395074: step 6200, loss 0.836195, acc 0\n",
      "\n",
      "\n",
      "the more people get to know bernie what he stands for and his consistency the more people support him <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.396995: step 6200, loss 2.79624, acc 0\n",
      "\n",
      "\n",
      "live on the @marklevinshow now tune in #cruzcrew <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.398990: step 6200, loss 2.17383, acc 0\n",
      "\n",
      "\n",
      "i took on wall street and opposed deregulation i now believe that we need to break up financial institutions that are too big to fail <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.400798: step 6200, loss 3.22889, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump your on tv tonight channel 4 9 oclock the mad world of donald trump <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.402658: step 6200, loss 3.49103, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump please no one likes common core its also not fair to those who grew up not using it and are now forced to do so <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.404415: step 6200, loss 3.64465, acc 0\n",
      "\n",
      "\n",
      "i am so honored and proud to have your endorsement your support and your friendship @dloesch <URL/> #caucusforcruz <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.406142: step 6200, loss 2.17975, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump voting for hilary is like allowing an accused pedophile to take a kindergarten teaching position #nowayhilary <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.407977: step 6200, loss 3.09831, acc 0\n",
      "\n",
      "\n",
      "#christie2016 #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.409996: step 6200, loss 1.78457, acc 0\n",
      "\n",
      "\n",
      "school choice leads to healthy competition among schools to provide the best educational atmosphere for all students to learn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.411565: step 6200, loss 3.34713, acc 0\n",
      "\n",
      "\n",
      "join us saturday night for the south carolina primary watch party #scprimary #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.413306: step 6200, loss 3.03793, acc 0\n",
      "\n",
      "\n",
      "‚Äúto all my supporters out there‚Äîsome may have doubted us but we never doubted each other this one‚Äôs for you‚Äù ‚Äîhillary in nv <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.415105: step 6200, loss 3.27771, acc 0\n",
      "\n",
      "\n",
      "my conversation with @empowertexans is beginning now watch here <URL/> #cruzcrew <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.416770: step 6200, loss 2.03597, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump @puttster71 @bobvanderplaats fuck you trumpüá∫üá∏üëé <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.418488: step 6200, loss 2.19366, acc 0\n",
      "\n",
      "\n",
      "we‚Äôre in bedford this morning for a rally with my good friends @charliebakerma and @larryhogan watch it live <URL/> #fitn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.420324: step 6200, loss 3.84931, acc 0\n",
      "\n",
      "\n",
      "best of luck to the broncos and the panthers in the #superbowl tonight looking forward to a great game <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.422239: step 6200, loss 2.90348, acc 0\n",
      "\n",
      "\n",
      "@tedcruz @glennbeck @realdonaldtrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.424180: step 6200, loss 2.04794, acc 0\n",
      "\n",
      "\n",
      "no questions on this at #demdebate but a reminder women deserve a champion in the white house <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.425864: step 6200, loss 4.16328, acc 0\n",
      "\n",
      "\n",
      "@berniesanders thanks for shedding light on the plight of our 50000 homeless veterans on any given night it is a shame #wearebernie <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.427463: step 6200, loss 3.26971, acc 0\n",
      "\n",
      "\n",
      "@marcorubio <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.429471: step 6200, loss 1.88508, acc 0\n",
      "\n",
      "\n",
      "new hampshire i am asking for your vote this is my closing argument <URL/> #fitn #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.431209: step 6200, loss 2.40884, acc 0\n",
      "\n",
      "\n",
      "we live in a world with serious foreign policy challenges we need serious leadership #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.432868: step 6200, loss 3.98375, acc 0\n",
      "\n",
      "\n",
      "@hillaryclinton you are held accountable for your lack of actions and deleting emails classified emails your bullying sex scandals pathet <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.435013: step 6200, loss 2.67568, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump @xxflame @bobvanderplaats so childishnobody likes you ted they all wanna be cool and rich like mecan you grow up <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.436876: step 6200, loss 1.75256, acc 0\n",
      "\n",
      "\n",
      "the fiscal irresponsibility of our government must stop proud to unveil my plan to reduce wasteful spending <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.438717: step 6200, loss 4.3681, acc 0\n",
      "\n",
      "\n",
      "im sorry @marcorubio being senator is a fine job but <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.440351: step 6200, loss 1.23139, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump donald i think if we want our military strong we need to hire more idealists and inventors in star wars attack of the <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.442089: step 6200, loss 2.37594, acc 0\n",
      "\n",
      "\n",
      "sadly in the greatest democracy in the world the federal government no longer serves america it serves itself <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.443837: step 6200, loss 3.35549, acc 0\n",
      "\n",
      "\n",
      "if i‚Äôm elected president we will utterly and completely destroy isis #crconvention <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.446863: step 6200, loss 4.19394, acc 0\n",
      "\n",
      "\n",
      "@joenbc latest umass tracking poll nh gop trump 35 (+1) rubio 14 (1) cruz 13 (1) jeb 10 (+2) kasich 10 (+2) <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.448689: step 6200, loss 3.20755, acc 0\n",
      "\n",
      "\n",
      "new hampshire  lets do this together #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.450523: step 6200, loss 2.97392, acc 0\n",
      "\n",
      "\n",
      "on the radio with @seanhannity now listen here #cruzcrew <URL/> #caucusforcruz <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.452388: step 6200, loss 0.76851, acc 0\n",
      "\n",
      "\n",
      "@potus is doing his job by nominating a new supreme court justice tell senate republicans to do theirs <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.454206: step 6200, loss 1.85185, acc 0\n",
      "\n",
      "\n",
      "a vote in 2002 is not a plan to defeat isis in 2016 #demdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.456007: step 6200, loss 4.01072, acc 0\n",
      "\n",
      "\n",
      "@hillaryclinton @billclinton <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.457571: step 6200, loss 1.66654, acc 0\n",
      "\n",
      "\n",
      "join us tonight in manchester for the first of our new hampshire swing <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.459104: step 6200, loss 2.4896, acc 0\n",
      "\n",
      "\n",
      "new ad my conservative record of accomplishments vs @marcorubio‚Äôs record of doing nothing in the senate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.460937: step 6200, loss 2.53523, acc 0\n",
      "\n",
      "\n",
      "the likelihood is that wall street gets away with a lot more illegal behavior than we know of #fitn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.462799: step 6200, loss 2.19584, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump only donald j trump make america win again #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.464521: step 6200, loss 2.36154, acc 0\n",
      "\n",
      "\n",
      "@chrischristie what do you want me to do go down there with a mop u cannot handle nj how can you handle the usa #dropoutoftheracestupid <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.466027: step 6200, loss 3.51421, acc 0\n",
      "\n",
      "\n",
      "sad to hear another #borinqueneer francisco torregrosa died before receiving congressional gold medal <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.467753: step 6200, loss 3.6523, acc 0\n",
      "\n",
      "\n",
      "watching tonights #gopdebate join the #cruzcrew <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.469460: step 6200, loss 1.61603, acc 0\n",
      "\n",
      "\n",
      "@brandonsawyer84 @realdonaldtrump will rule #southcarolinaprimary <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.471307: step 6200, loss 3.69925, acc 0\n",
      "\n",
      "\n",
      "our next town hall is about to begin watch live here <URL/> #fitn #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.473353: step 6200, loss 2.20461, acc 0\n",
      "\n",
      "\n",
      "donald trump has spent a career sticking it to working americans <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.475078: step 6200, loss 3.38882, acc 0\n",
      "\n",
      "\n",
      "once again we have made history you the grassroots continue to defy the pundits and produce extraordinary results <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.476755: step 6200, loss 3.50343, acc 0\n",
      "\n",
      "\n",
      "@tedcruz @glennbeck @realdonaldtrump at this point i find both kelly trump to be utterly disgusting arrogant buffoons kelly is a hack <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.478633: step 6200, loss 2.95765, acc 0\n",
      "\n",
      "\n",
      "why would the people of florida vote for marco rubio when he defrauded them by agreeing to represent them as their senator and then quit <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.480638: step 6200, loss 2.31339, acc 0\n",
      "\n",
      "\n",
      "@berniesanders theres too many homeless people its really sad too bad the 1% doesnt pay their fair share of taxes #feelthebern <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.482587: step 6200, loss 2.57988, acc 0\n",
      "\n",
      "\n",
      "there are no buyers for the worthless @nydailynews but little mort zuckerman is frantically looking it is bleeding red ink  a total loser <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.484597: step 6200, loss 2.67524, acc 0\n",
      "\n",
      "\n",
      "surprisingly american taxpayers already pay enough to fund national health insurance we just dont get it <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.486284: step 6200, loss 3.3195, acc 0\n",
      "\n",
      "\n",
      "in case you missed the #gopdebate last night see all my responses here <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.488149: step 6200, loss 1.2852, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump @lindasuhler @megynkelly he is just trying to get out of debate because he knows he is not a good debater <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.489970: step 6200, loss 3.0873, acc 0\n",
      "\n",
      "\n",
      "we can and must do more to honor the commitment to serve the men and women who have served our great nation <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.491671: step 6200, loss 3.67939, acc 0\n",
      "\n",
      "\n",
      "proud to receive this award from @frcaction <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.493734: step 6200, loss 3.93325, acc 0\n",
      "\n",
      "\n",
      "this is it your last chance to say youre with hillary before the new hampshire primary commit today <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.495883: step 6200, loss 3.60103, acc 0\n",
      "\n",
      "\n",
      "yesterday i unveiled my plan to rebuild our military read it here <URL/> #goptownhall <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.499291: step 6200, loss 3.29145, acc 0\n",
      "\n",
      "\n",
      "when it comes to my conservative record of accomplishments the numbers speak for themselves <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.501328: step 6200, loss 2.53692, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump @puttster71 @bobvanderplaats we need you register and vote for trumplets make america great again  <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.503449: step 6200, loss 2.52708, acc 0\n",
      "\n",
      "\n",
      "@jebbush unfortunately jebby boy you just cant seem to put together a campaign theme that excites america so youll end up on trash heap <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.505408: step 6200, loss 2.88305, acc 0\n",
      "\n",
      "\n",
      "in texas we defended the ten commandments monument that stands on the capitol grounds we went to #scotus and we won 54 #crconvention <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.507079: step 6200, loss 1.9717, acc 0\n",
      "\n",
      "\n",
      "@berniesanders said yes i will raise tax on everyone <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.508747: step 6200, loss 4.03592, acc 0\n",
      "\n",
      "\n",
      "tomorrow join me and @governorperry in mount pleasant sc where well unveil my plan to rebuild our military <URL/> #fits <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.510531: step 6200, loss 3.76742, acc 0\n",
      "\n",
      "\n",
      "@tedcruz knows his electorate he spent the last four years burning bridges to run for president #ambition <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.512167: step 6200, loss 1.50479, acc 0\n",
      "\n",
      "\n",
      "@wehearthomes was great meeting him <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.514115: step 6200, loss 3.36388, acc 0\n",
      "\n",
      "\n",
      "elections have consequences the president has a responsibility to nominate a new justice and the senate has a responsibility to vote <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.515913: step 6200, loss 1.9007, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump is the religious guy when hes the first to have strip clubs in his casinos that is found in 2 corinthians @glennbeck <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.517750: step 6200, loss 2.86022, acc 0\n",
      "\n",
      "\n",
      "teubert said he plans to caucus for christie bc of his experience as governor his focus on national security <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.519443: step 6200, loss 2.78868, acc 0\n",
      "\n",
      "\n",
      "the stakes of this next election couldnt be higher #choosecruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.521096: step 6200, loss 3.71852, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump @mariabartiromo @morningsmaria @foxbusiness  you got me on your side mr trump change is coming with a meaningful purpose <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.522739: step 6200, loss 2.83081, acc 0\n",
      "\n",
      "\n",
      "last summer sandra bland lost her life in police custody now her mother is channeling her grief into action <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.524475: step 6200, loss 2.77158, acc 0\n",
      "\n",
      "\n",
      "nevada if you‚Äôre in line and have a question or need help at your caucus location call 17025508008 #nvdemscaucus <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.525989: step 6200, loss 2.44378, acc 0\n",
      "\n",
      "\n",
      "@tedcruz @glennbeck @realdonaldtrump ted cruz doesnt have the constitution in his bones his bones where born in canada eh <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.527536: step 6200, loss 2.80324, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump give me strength today for i am taking finals <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.529271: step 6200, loss 1.64702, acc 0\n",
      "\n",
      "\n",
      "caught a few minutes of a basketball game in daniel island today rooting for the cheetah tigers <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.531070: step 6200, loss 3.43584, acc 0\n",
      "\n",
      "\n",
      "todays your day south carolina rt to say youre with us #scprimary #scformarco <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.532858: step 6200, loss 3.13622, acc 0\n",
      "\n",
      "\n",
      "all human life is worthy of the protection of our laws add your name if you agree <URL/> #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.534667: step 6200, loss 3.323, acc 0\n",
      "\n",
      "\n",
      "may the god of hope fill you with all joy and peace as you trust in him so that you may overflow with hope by the power of the holy spirit\n",
      "example\n",
      "2016-03-08T02:13:38.536414: step 6200, loss 2.62229, acc 0\n",
      "\n",
      "\n",
      "about to be on the radio with @radiotalkermike listen live <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.538222: step 6200, loss 2.60045, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump megan kelly no trump you are liberal con you should not be pres of a republic #iowacaucus #trump2016 #ccot #tcot <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.539897: step 6200, loss 2.34687, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump theocracy <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.541709: step 6200, loss 2.59057, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.543320: step 6200, loss 2.53943, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump yes thank you <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.545275: step 6200, loss 3.23682, acc 0\n",
      "\n",
      "\n",
      "history made #iowacaucus <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.546965: step 6200, loss 3.6486, acc 0\n",
      "\n",
      "\n",
      "on with @ingrahamangle right now listen live <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.548559: step 6200, loss 3.35545, acc 0\n",
      "\n",
      "\n",
      "help push us to victory in nevada super tuesday and beyond #cruzcrew <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.551577: step 6200, loss 2.90327, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump falwell is an asshole  birds of a feather you know the rest <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.553277: step 6200, loss 2.82339, acc 0\n",
      "\n",
      "\n",
      "@chrischristie @njdotcom @news12nj <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.555268: step 6200, loss 2.96509, acc 0\n",
      "\n",
      "\n",
      "thank you to all of the incredible volunteers behind the scenes in iowa #caucusfortrump <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.557114: step 6200, loss 3.32378, acc 0\n",
      "\n",
      "\n",
      "we must increase the min wage to $15 which will increase the wages of nearly 60 percent of latinos #votetogether <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.558755: step 6200, loss 3.06137, acc 0\n",
      "\n",
      "\n",
      "get ready to #caucusforcruz tomorrow <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.560706: step 6200, loss 3.1418, acc 0\n",
      "\n",
      "\n",
      "this is it‚Äîthe #iowacaucus is today if you‚Äôre standing with hillary rt to say #imwithher <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.562494: step 6200, loss 2.38728, acc 0\n",
      "\n",
      "\n",
      "20 most anticipated hotel openings of 2016 trump international hotel washington dc <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.564225: step 6200, loss 1.75171, acc 0\n",
      "\n",
      "\n",
      "ted cruz only talks tough on immigration now because he did so badly in sc he is in favor of amnesty and weak on illegal immigration <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.566224: step 6200, loss 3.39025, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump now do john rocker <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.567974: step 6200, loss 1.5086, acc 0\n",
      "\n",
      "\n",
      "@chrischristie <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.569609: step 6200, loss 2.37492, acc 0\n",
      "\n",
      "\n",
      "obamacare saved this woman when she needed emergency surgery lets build on its progress not start over <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.571402: step 6200, loss 2.87584, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump muslims are not the enemies authoritarian fascists are <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.572931: step 6200, loss 3.8661, acc 0\n",
      "\n",
      "\n",
      "nevada bernie cant do this alone caucus tomorrow at 11am #americatogether <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.574669: step 6200, loss 2.93505, acc 0\n",
      "\n",
      "\n",
      "@chrischristie lmao you really dont care about your own state its becoming a joke at this point <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.576415: step 6200, loss 2.77607, acc 0\n",
      "\n",
      "\n",
      "ive defunded planned parenthood every year for the last 6 years #thechristierecord <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.578005: step 6200, loss 1.48352, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump you were born to be real <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.579727: step 6200, loss 2.25753, acc 0\n",
      "\n",
      "\n",
      "thank you nevada #trump2016 #makeamericagreatagain @snapchat username realdonaldtrump <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.581479: step 6200, loss 2.26858, acc 0\n",
      "\n",
      "\n",
      "i would consult with top military minds to create proactive ways to seek out and destroy #isis read my plan here <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.583064: step 6200, loss 2.98582, acc 0\n",
      "\n",
      "\n",
      "@berniesanders 75% of the abortions are black babies i guess you an hillary are racist over 99 thousand black babies last year aborted <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.584685: step 6200, loss 3.92237, acc 0\n",
      "\n",
      "\n",
      "@jojo2foxy trump is unstoppablehe is what common sense americans want whether republican independent or a reagan democrat <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.586412: step 6200, loss 3.08387, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump @xxflame @bobvanderplaats wrong again donald <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.588237: step 6200, loss 3.42298, acc 0\n",
      "\n",
      "\n",
      "@tedcruz @glennbeck @realdonaldtrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.590118: step 6200, loss 2.04794, acc 0\n",
      "\n",
      "\n",
      "super pacs funded by billionaires buy elections ordinary people don‚Äôt vote we have an economic and political crisis in this country <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.591760: step 6200, loss 3.52035, acc 0\n",
      "\n",
      "\n",
      "a message to the great people of new hampshire on this important day #votetrumpnh video <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.593502: step 6200, loss 3.62762, acc 0\n",
      "\n",
      "\n",
      "on the way to the #gopdebate with my wonderful wife @melaniatrump <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.595329: step 6200, loss 3.34235, acc 0\n",
      "\n",
      "\n",
      "the polls are now showing that i am the best to win the general election states that are never in play for repubs will be won by me great <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.596964: step 6200, loss 3.22549, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump dishonest @foxnews wont show their own poll for iowa now that they found one showing cruz doing better still losing <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.598596: step 6200, loss 1.95172, acc 0\n",
      "\n",
      "\n",
      "i am thrilled to earn the support of congressman @timhuelskamp <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.600166: step 6200, loss 1.9558, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump hurray good for you hope d bless <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.603021: step 6200, loss 2.85509, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump man up and go to the debate on thursday <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.604786: step 6200, loss 2.91337, acc 0\n",
      "\n",
      "\n",
      "before social security was signed into law nearly half of our senior citizens lived in poverty <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.606479: step 6200, loss 4.60997, acc 0\n",
      "\n",
      "\n",
      "a day on the campaign trail with @msvivicafox and @imangelabassett <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.608092: step 6200, loss 2.39478, acc 0\n",
      "\n",
      "\n",
      "why did chris christie get down on one knee in nh today via @bostonglobe #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.609587: step 6200, loss 3.23387, acc 0\n",
      "\n",
      "\n",
      "#cruzcrew tune in to the #gopdebate at 9 pm et on @foxnews <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.611134: step 6200, loss 2.81561, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump @puttster71 @bobvanderplaats go trump he is the only one to fix things <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.612715: step 6200, loss 2.3733, acc 0\n",
      "\n",
      "\n",
      "you me @glennbeck and phil robertson in iowa saturday and sunday more info <URL/> #iacaucus <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.614410: step 6200, loss 2.43493, acc 0\n",
      "\n",
      "\n",
      "@scotmreed no matter what liar sleazeball @rickwtyler says about @tedcruz in tennessee we have early voting and @realdonaldtrump has won <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.615926: step 6200, loss 2.9907, acc 0\n",
      "\n",
      "\n",
      "watching the #gopdebate you could watch the next one in person by entering our debate contest <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.617623: step 6200, loss 2.78897, acc 0\n",
      "\n",
      "\n",
      "glad to join @chucktodd and @savannahguthrie this morning to speak about tonights #iacaucus <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.619412: step 6200, loss 3.96759, acc 0\n",
      "\n",
      "\n",
      "@salmhay1 thanks sally appreciate the support <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.621252: step 6200, loss 2.53141, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump actually the founding fathers set up our system to include gridlock to prevent excessive legislation no kumbaya for them <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.623296: step 6200, loss 2.80979, acc 0\n",
      "\n",
      "\n",
      "racial inequality isn‚Äôt just a symptom of economic inequality <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.625032: step 6200, loss 3.65553, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump clones the gunship could be made into reality ball turrets were made before but i put mini guns but also make it <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.626667: step 6200, loss 2.73084, acc 0\n",
      "\n",
      "\n",
      "our south carolina team is making some last minute calls before tonight‚Äôs rally rsvp now <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.628689: step 6200, loss 2.89439, acc 0\n",
      "\n",
      "\n",
      "@tedcruz @glennbeck @realdonaldtrump i dont trust alcoholics esp the crying ones and i dont like gold salesmen fdr confiscated it <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.630345: step 6200, loss 2.50991, acc 0\n",
      "\n",
      "\n",
      "we had great crowds all day today in new hampshire thanks to everyone who came out to show your support <URL/> #fitn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.632039: step 6200, loss 3.55104, acc 0\n",
      "\n",
      "\n",
      "@tedcruz @glennbeck @realdonaldtrump except our lying eyes and ears say differently it is about beck (and megyn) rage @ trump is personal <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.633577: step 6200, loss 2.55011, acc 0\n",
      "\n",
      "\n",
      "millions of $s of false ads paid for by lobbyistsspecial interests of cheater @sentedcruz and sleepy @jebbush are now running in sc <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.635252: step 6200, loss 2.53784, acc 0\n",
      "\n",
      "\n",
      "great morning in new hampshire with our dedicated volunteers working hard lets do this #christie2016 #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.636810: step 6200, loss 3.06737, acc 0\n",
      "\n",
      "\n",
      "the mainstream media is fixated on nonsensical issues and pointless bickering none of which does any good for our nation <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.638628: step 6200, loss 2.18266, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump and he is no longer one of the most respected religious leaders <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.640140: step 6200, loss 2.02311, acc 0\n",
      "\n",
      "\n",
      "icymi were liveblogging the #gopdebate go check it out <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.641817: step 6200, loss 3.14503, acc 0\n",
      "\n",
      "\n",
      "proud to have the strongest national organization and the support of these state leaders <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.643313: step 6200, loss 1.60047, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump some of the states such as mine are already ending it thank god <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.644858: step 6200, loss 4.16408, acc 0\n",
      "\n",
      "\n",
      "thank you to 6yearold camille from sarasota florida for the artwork and more importantly the prayers <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.646564: step 6200, loss 2.85908, acc 0\n",
      "\n",
      "\n",
      "the us needs to lead the international community in fighting climate change to maintain our economic strength and global security <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.648319: step 6200, loss 2.21634, acc 0\n",
      "\n",
      "\n",
      "an unbelievable night in iowa with our great veterans we raised $600000000 while the politicians talked #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.649939: step 6200, loss 3.21875, acc 0\n",
      "\n",
      "\n",
      "hillary is handing over her snapchat account to @billclinton today don‚Äôt miss it <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.651516: step 6200, loss 2.23404, acc 0\n",
      "\n",
      "\n",
      "@tedcruz listening to glenn beck discussing your more than admirable career thus far from mrbecks lips 2 gods ears <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.654552: step 6200, loss 3.02491, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump trump 2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.656211: step 6200, loss 2.60184, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump go donald would love to have u in savannah ga <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.657836: step 6200, loss 3.84989, acc 0\n",
      "\n",
      "\n",
      "this is what were fighting for in nevada <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.659767: step 6200, loss 3.46857, acc 0\n",
      "\n",
      "\n",
      "#choosecruz <URL/> #fitn #nhpolitics <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.661396: step 6200, loss 1.94721, acc 0\n",
      "\n",
      "\n",
      "a message to the great people of iowa on this historic day find your caucus site here <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.662878: step 6200, loss 3.01855, acc 0\n",
      "\n",
      "\n",
      "for forty years @realdonaldtrump has been funding liberal democratic politicians #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.664340: step 6200, loss 2.12661, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump @sarahpalinusa @hillaryclinton @gma @gstephanopoulos @savannahguthrie @meetthepress @chucktodd @facethenation @cbsnews @nbc <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.666047: step 6200, loss 3.61922, acc 0\n",
      "\n",
      "\n",
      "our opponents are funding their campaigns with checks from billionaires and wall street not us #bernieinsc <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.667790: step 6200, loss 2.84425, acc 0\n",
      "\n",
      "\n",
      "@haloonefortrump @foxnews takes #yuge hit on advertising revenues by screwing with @realdonaldtrump rofl <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.669461: step 6200, loss 1.95776, acc 0\n",
      "\n",
      "\n",
      "ill talk with @seanhannity on @foxnews tonight  tune in at 10 pm et <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.671062: step 6200, loss 2.82083, acc 0\n",
      "\n",
      "\n",
      "the senate‚Äôs duty is to advise and consent you know what the senate is advising right now <URL/> #scotus <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.672561: step 6200, loss 1.86858, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump yesterday jeffers now falwell jr we r gonna do this if we vote take your friends neighbors have a voting party #iacaucus <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.674338: step 6200, loss 3.76651, acc 0\n",
      "\n",
      "\n",
      "lets roll up our sleeves and get to work dismantling the schooltoprison pipelinewe need a cradletocollege pipeline ‚Äîhillary <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.675774: step 6200, loss 3.20651, acc 0\n",
      "\n",
      "\n",
      "@tperkins is a man of incredible principle and faith i am honored to have his blessing and endorsement for 2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.677392: step 6200, loss 2.00907, acc 0\n",
      "\n",
      "\n",
      "@hillaryclinton whats your involvement in #therockefellerinitiative #thetruthisoutthere #disclosure <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.679214: step 6200, loss 3.32601, acc 0\n",
      "\n",
      "\n",
      "@tedcruz @glennbeck @realdonaldtrump oh the one the government has violated almost every term since the 1st presidential term #null #void <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.681089: step 6200, loss 2.60316, acc 0\n",
      "\n",
      "\n",
      "tennessee gop poll <URL/> trump 327% cruz 165% carson 66% rubio 53% christie 24% jeb 16% <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.682905: step 6200, loss 2.12337, acc 0\n",
      "\n",
      "\n",
      "it‚Äôs time to close this dangerous loophole in our gun laws <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.684531: step 6200, loss 4.28883, acc 0\n",
      "\n",
      "\n",
      "get your jeb bumper sticker here ‚Üí <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.686156: step 6200, loss 2.14616, acc 0\n",
      "\n",
      "\n",
      "@berniesanders ‚Äì the candidate for main street not wall street #feelthebern #bernie2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.687906: step 6200, loss 1.90204, acc 0\n",
      "\n",
      "\n",
      "democrats win when the voter turnout is high when people come together and we reject division register to vote <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.689467: step 6200, loss 3.1668, acc 0\n",
      "\n",
      "\n",
      "poverty is free enterprise not reaching people <URL/> #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.691035: step 6200, loss 2.8499, acc 0\n",
      "\n",
      "\n",
      "i will promote peace and prosperity through the power of we the people and restore america‚Äôs role as the world‚Äôs preeminent global leader <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.692618: step 6200, loss 3.42441, acc 0\n",
      "\n",
      "\n",
      "i look forward to signing the #auditthefed bill into law as president #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.694344: step 6200, loss 3.53398, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.696012: step 6200, loss 2.53943, acc 0\n",
      "\n",
      "\n",
      "@berniesanders @carolcnn @cspanwj @cnnashleigh @vets4bernie #feelthebern @hillaryclinton @newshour <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.697496: step 6200, loss 2.73518, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump #scaredoftoughquestions im no @megankelly fan but it sounds like your scared of her @realdonaldtrump <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.698973: step 6200, loss 2.71939, acc 0\n",
      "\n",
      "\n",
      "all of us democrats have a responsibility to make sure a republican doesn‚Äôt win and rip away all the progress weve made ‚Äîhillary in co <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.700422: step 6200, loss 2.43493, acc 0\n",
      "\n",
      "\n",
      "@hillaryclinton <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.701974: step 6200, loss 2.2437, acc 0\n",
      "\n",
      "\n",
      "the new hampshire #cruztovictory visits weare <URL/> #fitn #nhpolitics <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.703523: step 6200, loss 2.35911, acc 0\n",
      "\n",
      "\n",
      "thanks @meagy19 helps when you know what you stand for cc @marcorubio @tedcruz <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.706406: step 6200, loss 2.14004, acc 0\n",
      "\n",
      "\n",
      "remember cruz and bush gave us roberts who upheld #obamacare twice i am the only one who will #makeamericagreatagain <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.708030: step 6200, loss 2.74483, acc 0\n",
      "\n",
      "\n",
      "you voted for it <URL/> #demtownhall <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.709724: step 6200, loss 2.55842, acc 0\n",
      "\n",
      "\n",
      "more proof that women need a president who will stop attacks on womens health in their tracks <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.711472: step 6200, loss 4.08486, acc 0\n",
      "\n",
      "\n",
      "@by @tedcruz good i want to see the court decide what i was always told natural born means  born on american soil <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.713232: step 6200, loss 3.20011, acc 0\n",
      "\n",
      "\n",
      "new hampshire  help us #cruztovictory find your polling place now <URL/> #fitn #nhpolitics <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.714752: step 6200, loss 1.89481, acc 0\n",
      "\n",
      "\n",
      "we need endtoend reform in our criminal justice system‚Äînot halfmeasures but a full commitment with real followthrough ‚Äîhillary <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.716381: step 6200, loss 3.62171, acc 0\n",
      "\n",
      "\n",
      "these are just a few of the women leading our political revolution in new hampshire #fitn <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.717925: step 6200, loss 3.87944, acc 0\n",
      "\n",
      "\n",
      "thank you @kayleighmcenany for your nice words  great knowledge and style we are doing really well in south carolina @cnn @donlemon <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.719467: step 6200, loss 2.87152, acc 0\n",
      "\n",
      "\n",
      "@ddpick18 @realdonaldtrump this texan will be voting trump march 1st cruz is a fake texan <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.721057: step 6200, loss 2.70328, acc 0\n",
      "\n",
      "\n",
      "@potus may be satisfied with 07% growth but with the right leadership i know we can achieve 4% economic growth <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.722817: step 6200, loss 2.77879, acc 0\n",
      "\n",
      "\n",
      "@tedcruz is not even eligible to run for president therefore u should vote 4 @realdonaldtrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.724383: step 6200, loss 2.8378, acc 0\n",
      "\n",
      "\n",
      "3 days until nh heads to the polls here‚Äôs one thing you can do right now to help hillary‚Äîno matter where you live <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.725935: step 6200, loss 2.92266, acc 0\n",
      "\n",
      "\n",
      "@jebbush @hillaryclinton @danmericacnn she has failed at everything she has done #notreadyforhillary <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.727748: step 6200, loss 2.2566, acc 0\n",
      "\n",
      "\n",
      "the political revolution continues in nevada but we can‚Äôt do it without you if we show up on caucus day we can win‚Äîit‚Äôs that simple <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.729293: step 6200, loss 3.16768, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump thank god america has allowed functional illiterates to leave schools for decades education has ceased to exist <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.731071: step 6200, loss 2.10663, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump üí™ üëç üëå üëä üëè go trump 2016 ‚ÄºÔ∏è üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏ <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.732833: step 6200, loss 2.59655, acc 0\n",
      "\n",
      "\n",
      "@realdonaldtrump how its not a national mandate to begin with i do hope you know that <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "example\n",
      "2016-03-08T02:13:38.734643: step 6200, loss 1.99261, acc 0\n",
      "\n",
      "\n",
      "we will have legislation to enact comprehensive immigration reform before congress within my first 100 days in office <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.736355: step 6200, loss 0.0645804, acc 1\n",
      "\n",
      "\n",
      "nobody who is actually prolife could stand on a national stage and say planned parenthood does wonderful things #goptownhall <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.737848: step 6200, loss 0.111667, acc 1\n",
      "\n",
      "\n",
      "icymi the clintons when they were in office weren‚Äôt exactly friends to immigrants <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.739430: step 6200, loss 0.136887, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump deport me daddy <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.741209: step 6200, loss 0.0972526, acc 1\n",
      "\n",
      "\n",
      "just a reminder <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.743156: step 6200, loss 0.1616, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump showing class grace calls me a soft weak little baby hope he doesnt try to eat me <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.744737: step 6200, loss 0.0618077, acc 1\n",
      "\n",
      "\n",
      "black families are denied mortgages nearly 3x as often as white families we need to tear down barriers of systemic racism that still exist <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.746340: step 6200, loss 0.0778001, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump who one of the most respected thats like saying your one the most respected businessman not <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.747931: step 6200, loss 0.170669, acc 1\n",
      "\n",
      "\n",
      "#cruztovictory in new hampshire <URL/> #fitn #nhpolitics <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.749398: step 6200, loss 0.18691, acc 1\n",
      "\n",
      "\n",
      "got a letter in the mail from our future president maggie hope i can count on your mom‚Äôs vote <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.750927: step 6200, loss 0.0944384, acc 1\n",
      "\n",
      "\n",
      "south carolina has a critical choice to make not just the presidency but also the supreme court hang in the balance #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.752435: step 6200, loss 0.109298, acc 1\n",
      "\n",
      "\n",
      "we need to make college affordable we also need to lift the enormous burden of student debt that millions carry right now #demtownhall <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.754154: step 6200, loss 0.114607, acc 1\n",
      "\n",
      "\n",
      "failed presidential candidate @mittromney was made to look like a fool by senator harry reid didnt release his tax returns until 9/21/12 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.755915: step 6200, loss 0.072699, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump @laurencristmann @sharp_trident @megynkelly using kelly again is being provocative debate is about the candidates not fox <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.758916: step 6200, loss 0.171537, acc 1\n",
      "\n",
      "\n",
      "@splashpoint50 @realdonaldtrump our retrumplican trump support group of 9500 members say 97% will not watch the debate tonight <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.760695: step 6200, loss 0.14384, acc 1\n",
      "\n",
      "\n",
      "watch how i know @realdonaldtrump‚Äã‚Äôs supreme court justices would be liberals  <URL/> #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.762349: step 6200, loss 0.111715, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump @xxflame @bobvanderplaats gooooooo trump all the way  <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.764349: step 6200, loss 0.258207, acc 1\n",
      "\n",
      "\n",
      "dropped by the red arrow in milford nh their fresh blueberry muffins look too good to pass up <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.766123: step 6200, loss 0.161759, acc 1\n",
      "\n",
      "\n",
      "when we guarantee health care as a right you take a huge bite out of poverty <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.767813: step 6200, loss 0.0680539, acc 1\n",
      "\n",
      "\n",
      "a pre #gopdebate prayer <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.769533: step 6200, loss 0.151556, acc 1\n",
      "\n",
      "\n",
      "honored to speak at @clpforums #faithfamilyforum an amazing crowd in attendance thank you for the warm welcome <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.771215: step 6200, loss 0.0900926, acc 1\n",
      "\n",
      "\n",
      "todays third stop londonderry new hampshire thank you #fitn #votetrumpnh <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.772891: step 6200, loss 0.110058, acc 1\n",
      "\n",
      "\n",
      "thank you nashville for joining us this morning get ready to #choosecruz on super tuesday <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.774552: step 6200, loss 0.0635685, acc 1\n",
      "\n",
      "\n",
      "@swimmomjj hey its not over yet im fighting to the finish to start ive got 10 town halls in the next 3 days <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.776194: step 6200, loss 0.0894209, acc 1\n",
      "\n",
      "\n",
      "rt if you agree we need a president who knows what he believes and is willing to say it on day one and not at the end of his term #gopdebate\n",
      "nonexample\n",
      "2016-03-08T02:13:38.777951: step 6200, loss 0.088258, acc 1\n",
      "\n",
      "\n",
      "i want you to imagine eight years from now the minimum wage is a living wage #nhprimary <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.779702: step 6200, loss 0.157081, acc 1\n",
      "\n",
      "\n",
      "dr james dobson i know ted he‚Äôs a christian family man of the utmost integrity listen to what else he said  <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.781391: step 6200, loss 0.149363, acc 1\n",
      "\n",
      "\n",
      "@floydpatriot thanks for stopping by appreciate you taking the time this morning <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.783094: step 6200, loss 0.169623, acc 1\n",
      "\n",
      "\n",
      "new video once again i was the only candidate on the #gopdebate stage to take on @realdonaldtrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.784733: step 6200, loss 0.0741036, acc 1\n",
      "\n",
      "\n",
      "granite staters vote in 4 days this new hampshire snowstorm wont stop our volunteers <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.786428: step 6200, loss 0.0590365, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump donald will give his all he will not let us down <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.788177: step 6200, loss 0.0819033, acc 1\n",
      "\n",
      "\n",
      "if youre with hillary now is the time to stand up and say so if you can pitch in to show her you‚Äôre with her <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.789911: step 6200, loss 0.0959015, acc 1\n",
      "\n",
      "\n",
      "tomorrow‚Äôs south carolina events will have a very special guest rsvp now <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.791630: step 6200, loss 0.15621, acc 1\n",
      "\n",
      "\n",
      "the #gopdebate has one thing right if you want to protect and build on the progress president obama has made hillarys your woman <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.793467: step 6200, loss 0.1859, acc 1\n",
      "\n",
      "\n",
      "ill be speaking soon about social security high drug prices and medicare at #seniorsdecide watch live here <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.795093: step 6200, loss 0.111804, acc 1\n",
      "\n",
      "\n",
      "young people in this country know exactly what theyre doing they know we need big change not just small steps <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.796884: step 6200, loss 0.117277, acc 1\n",
      "\n",
      "\n",
      "@joenbc trump kasich jeb and christie all had good nights <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.798779: step 6200, loss 0.204807, acc 1\n",
      "\n",
      "\n",
      "watch  <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.800927: step 6200, loss 0.171865, acc 1\n",
      "\n",
      "\n",
      "these voters from salem new hampshire explain why theyre supporting our campaign <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.803098: step 6200, loss 0.111845, acc 1\n",
      "\n",
      "\n",
      "excited to be visiting south carolina tomorrow starting with a town hall with @justinbamberg in the morning #scforbernie <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.804975: step 6200, loss 0.147827, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrumps ‚Äúsolutions‚Äù need to be challenged and so far i‚Äôm the only candidate who has watch <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.806575: step 6200, loss 0.0928756, acc 1\n",
      "\n",
      "\n",
      "i was asked about healthcare by anderson cooper have been consistent i will repeal all of #obamacare including the mandate period <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.808128: step 6200, loss 0.141721, acc 1\n",
      "\n",
      "\n",
      "‚Äúdo not grow weary doing good do not get discouraged do not give up‚Äù ‚Äîhillary in flint <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.810736: step 6200, loss 0.0866205, acc 1\n",
      "\n",
      "\n",
      "heart to heart in newton ia <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.812422: step 6200, loss 0.198891, acc 1\n",
      "\n",
      "\n",
      "in ohio and across the country republicans are once again attacking womens health we wont stand for this <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.814251: step 6200, loss 0.057809, acc 1\n",
      "\n",
      "\n",
      "@chrischristie please bring your mop clean up the mess you made in new jersey #tellingitlikeitis #fundnjpension <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.816130: step 6200, loss 0.0970289, acc 1\n",
      "\n",
      "\n",
      "tonight is the last #gopdebate before voting begins i‚Äôm asking for your support chip in $1 to say you‚Äôre with me <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.817929: step 6200, loss 0.131355, acc 1\n",
      "\n",
      "\n",
      "dont just listen to what candidates say ask who has walked the walk who has stood up and fought #caucusforcruz <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.819812: step 6200, loss 0.0763848, acc 1\n",
      "\n",
      "\n",
      "2% economic growth means declining incomes fewer opportunities for future generations my plan for 4% growth ‚Üí <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.821627: step 6200, loss 0.0789326, acc 1\n",
      "\n",
      "\n",
      "there is a reason it has been longstanding policy that we don‚Äôt negotiate with terrorists or pay ransoms #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.823397: step 6200, loss 0.0703103, acc 1\n",
      "\n",
      "\n",
      "questions or issues call our hotline #americatogether <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.825088: step 6200, loss 0.25733, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump this the big endorsement about as surprising as @sarahpalinusa and far less entertaining gather all ye snakes to the pit <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.826835: step 6200, loss 0.074163, acc 1\n",
      "\n",
      "\n",
      "new hampshire i am tested and i am ready to be your commander in chief #fitn <URL/> #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.828615: step 6200, loss 0.132197, acc 1\n",
      "\n",
      "\n",
      "closing statement on tonights #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.830277: step 6200, loss 0.0974298, acc 1\n",
      "\n",
      "\n",
      "@tedcruz @glennbeck following a flawed politician is one thing but clinging to a failed human like beck for advice is just pathetic <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.831947: step 6200, loss 0.123778, acc 1\n",
      "\n",
      "\n",
      "#choosecruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.833719: step 6200, loss 0.144159, acc 1\n",
      "\n",
      "\n",
      "tune in now <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.835519: step 6200, loss 0.18678, acc 1\n",
      "\n",
      "\n",
      "thank you congressman @dgvaladao for talking with our nevada team caucusgoers this weekend <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.837414: step 6200, loss 0.106023, acc 1\n",
      "\n",
      "\n",
      "5 days to #iacaucus here are a few ways your five bucks help hillary win in iowa <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.839074: step 6200, loss 0.0652115, acc 1\n",
      "\n",
      "\n",
      "thank you for a great night south carolina donate now to help us continue the momentum <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.840795: step 6200, loss 0.127708, acc 1\n",
      "\n",
      "\n",
      "justice scalia‚Äôs passing underscores the enormous gravity of this election #scotus is now hanging in the balance #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.842608: step 6200, loss 0.084131, acc 1\n",
      "\n",
      "\n",
      "we are going to get the government out of profiteering on student loans that is going to change <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.844438: step 6200, loss 0.0831803, acc 1\n",
      "\n",
      "\n",
      "@chrischristie <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.846212: step 6200, loss 0.141199, acc 1\n",
      "\n",
      "\n",
      "#makeamericagreatagain #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.847907: step 6200, loss 0.354824, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump how about *one* endorsement from sane person your insanity stable is already full <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.849785: step 6200, loss 0.101164, acc 1\n",
      "\n",
      "\n",
      "#iacaucus #caucusfortrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.851710: step 6200, loss 0.0916171, acc 1\n",
      "\n",
      "\n",
      "just got back from tampa it was an amazing evening with an even more amazing crowd  fantastic people will be in south carolina tomorrow <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.853546: step 6200, loss 0.0956722, acc 1\n",
      "\n",
      "\n",
      "@gigglemitz @geraldorivera @foxnews damn fox is really acting like a baby today i guess trump got under their skin  pretty biased <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.855257: step 6200, loss 0.149939, acc 1\n",
      "\n",
      "\n",
      "@morning_joe @mikebarnicle on @realdonaldtrump he finished 2nd but he made the turn successfully like a pro <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.856989: step 6200, loss 0.1876, acc 1\n",
      "\n",
      "\n",
      "@jebbush #jebbush2016 #commoncore #commonignorance an ignorant society is easier for the #elites to rule #truth <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.858885: step 6200, loss 0.104897, acc 1\n",
      "\n",
      "\n",
      "you arent going to accomplish what we need for working families as long as big money interests control the us congress #demdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.860606: step 6200, loss 0.0866476, acc 1\n",
      "\n",
      "\n",
      "@hillaryclinton #benghazi is an issue because your lack of conscience caused numerous unpardonable american deaths <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.863687: step 6200, loss 0.119657, acc 1\n",
      "\n",
      "\n",
      "i want to talk about how we break down barriers that disproportionately affect african americans build ladders of opportunity ‚Äîhillary <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.865666: step 6200, loss 0.079773, acc 1\n",
      "\n",
      "\n",
      "couldnt agree more @iamsteveharvey its time to act on gun violence thank you for telling these stories today <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.867507: step 6200, loss 0.160036, acc 1\n",
      "\n",
      "\n",
      "‚úîÔ∏è lifesaving cancer screenings ‚úîÔ∏è birth control ‚úîÔ∏è sex education pretty wonderful <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.869224: step 6200, loss 0.106551, acc 1\n",
      "\n",
      "\n",
      "one of five people in this country that get a prescription from a doctor cannot afford to fill that prescription that‚Äôs wrong #bernieinmn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.870984: step 6200, loss 0.0659814, acc 1\n",
      "\n",
      "\n",
      "if i am elected president we will have a #fullrepeal of obamacare #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.872679: step 6200, loss 0.09519, acc 1\n",
      "\n",
      "\n",
      "some may have doubted us but we never doubted each other <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.874571: step 6200, loss 0.150006, acc 1\n",
      "\n",
      "\n",
      "thank you america #trump2016 via @drudge_report <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.876379: step 6200, loss 0.154237, acc 1\n",
      "\n",
      "\n",
      "debate moderators squeeze dr ben carson on time <URL/> via @breitbartnews <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.878079: step 6200, loss 0.0813774, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump im on 5 th avenue where are you donny <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.879986: step 6200, loss 0.0499604, acc 1\n",
      "\n",
      "\n",
      "you must be registered republican by february 16th to vote trump in the florida primary <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.881825: step 6200, loss 0.0596346, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump will u sign a binding contract with america to follow through on your promises <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.883530: step 6200, loss 0.0811155, acc 1\n",
      "\n",
      "\n",
      "@berniesanders fuckin a speak truth to power <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.885257: step 6200, loss 0.0997146, acc 1\n",
      "\n",
      "\n",
      "just a reminder <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.887052: step 6200, loss 0.1616, acc 1\n",
      "\n",
      "\n",
      "rt to remind your friends to #caucusforcruz at 7pm <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.888843: step 6200, loss 0.175595, acc 1\n",
      "\n",
      "\n",
      "a physicist explains #marcomentum <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.890419: step 6200, loss 0.0596889, acc 1\n",
      "\n",
      "\n",
      "i will always stand with the american people against the bipartisan corruption of washington <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.892263: step 6200, loss 0.135179, acc 1\n",
      "\n",
      "\n",
      "we are not going to turn over the conservative movement to a con artist <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.894155: step 6200, loss 0.0684084, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump on who who surrounds himself with @katrinapierson @sarahpalinusa roy cohn and this guy <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.895974: step 6200, loss 0.0588498, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump trump 2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.897901: step 6200, loss 0.119684, acc 1\n",
      "\n",
      "\n",
      "it is so important to audit the federal reserve and yet ted cruz missed the vote on the bill that would allow this to be done <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.900280: step 6200, loss 0.0745517, acc 1\n",
      "\n",
      "\n",
      "its time for a president who will stand up to runaway government and fight washington special interests <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.901931: step 6200, loss 0.0860528, acc 1\n",
      "\n",
      "\n",
      "when ted cruz quits the race and the field begins to clear i will get most of his votes  no problem <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.903719: step 6200, loss 0.0754299, acc 1\n",
      "\n",
      "\n",
      "@nicholas_s_m thanks for your help really appreciate it <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.905484: step 6200, loss 0.276464, acc 1\n",
      "\n",
      "\n",
      "nobody else will challenge donalds domain i did i will #gopdebate #eminentdomain <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.907186: step 6200, loss 0.250432, acc 1\n",
      "\n",
      "\n",
      "make sure you get on the trump line and are not mislead by the cruz people they are bad be careful <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.908787: step 6200, loss 0.0923671, acc 1\n",
      "\n",
      "\n",
      "an honor to connect with our brave service men and women at the vfw in aiken sc out veterans deserve the best <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.910578: step 6200, loss 0.0716951, acc 1\n",
      "\n",
      "\n",
      "two town halls down two to go (for today) #iacaucus #christie2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.912358: step 6200, loss 0.267178, acc 1\n",
      "\n",
      "\n",
      "support a proven conservative for president #caucusforcruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.915283: step 6200, loss 0.0993647, acc 1\n",
      "\n",
      "\n",
      "i love being in south carolina we are leading big in all of the state polls  saturday is a big day make america great again <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.917170: step 6200, loss 0.105503, acc 1\n",
      "\n",
      "\n",
      "i know hillary will fight for the issues most important to me and to so many women and families ‚Äî@sengillibrand <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.918941: step 6200, loss 0.216604, acc 1\n",
      "\n",
      "\n",
      "thank you @lancesilver1 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.920767: step 6200, loss 0.163807, acc 1\n",
      "\n",
      "\n",
      "thank you <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.922662: step 6200, loss 0.117221, acc 1\n",
      "\n",
      "\n",
      "primary day is here new hampshire text where to 47246 to find your polling location hillary‚Äôs counting on you <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.924518: step 6200, loss 0.126831, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump is a hypocrite hes prolife then says he could shoot someone and still not lose voters #dumptrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.926259: step 6200, loss 0.122226, acc 1\n",
      "\n",
      "\n",
      "today is the #iacaucus rt to say #imwithher and make sure your friends and family know where to caucus at 630 pm <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.928146: step 6200, loss 0.167715, acc 1\n",
      "\n",
      "\n",
      "@hillaryclinton <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.930026: step 6200, loss 0.156335, acc 1\n",
      "\n",
      "\n",
      "a quote was read from a parody account last night on msnbc re jeb <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.931636: step 6200, loss 0.113487, acc 1\n",
      "\n",
      "\n",
      "i guess firstterm senators stick together cc @barackobama <URL/> #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.933345: step 6200, loss 0.096066, acc 1\n",
      "\n",
      "\n",
      "@berniesanders the cost of war doesnt end until every vet receives best health care <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.935110: step 6200, loss 0.163549, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump üëçüëçüá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏ <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.936719: step 6200, loss 0.1006, acc 1\n",
      "\n",
      "\n",
      "its irresponsible that my opponent justifies sending children back to terribly violent countries as a way to send a message <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.938370: step 6200, loss 0.080649, acc 1\n",
      "\n",
      "\n",
      "5 more days until the iowa caucus and the cold isn‚Äôt stopping our hard working group of volunteers <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.940352: step 6200, loss 0.130683, acc 1\n",
      "\n",
      "\n",
      "great twitter poll and i wasnt even there thank you #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.942050: step 6200, loss 0.117939, acc 1\n",
      "\n",
      "\n",
      "nothing will get better if we keep sending people to washington who will say or do anything to get elected <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.943837: step 6200, loss 0.0775857, acc 1\n",
      "\n",
      "\n",
      "we‚Äôve leveled the playing field so billionaires are no longer able to buy our candidates and elections #nhprimary <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.945551: step 6200, loss 0.0575866, acc 1\n",
      "\n",
      "\n",
      "@jebbush move from spiritual genocide to intellectual genocide to economic social and cultural genocide to civilizational genocide <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.947348: step 6200, loss 0.169073, acc 1\n",
      "\n",
      "\n",
      "the us is less safe from @potuss weak leadingfrombehind foreign policy i believe in peace through strength <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.949018: step 6200, loss 0.0943062, acc 1\n",
      "\n",
      "\n",
      "this election is about trust #caucusforcruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.950697: step 6200, loss 0.102896, acc 1\n",
      "\n",
      "\n",
      "while bernie rallied in downtown greenville volunteers kept calls going strong join us <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.952384: step 6200, loss 0.0824859, acc 1\n",
      "\n",
      "\n",
      "meeting and greeting caucus goers in linn county #caucusforcruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.954056: step 6200, loss 0.0804886, acc 1\n",
      "\n",
      "\n",
      "caucus day is here iowa text caucus to 47246 to find your caucus location and make a plan to get there by 630 pm shes counting on you <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.955904: step 6200, loss 0.0858341, acc 1\n",
      "\n",
      "\n",
      "salem nh bring your friends family and neighbors to #choosecruz on 2/9 <URL/> #fitn #nhpolitics <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.957636: step 6200, loss 0.171689, acc 1\n",
      "\n",
      "\n",
      "this election is going to be a turning point #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.959450: step 6200, loss 0.0936559, acc 1\n",
      "\n",
      "\n",
      "it never ends <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.961320: step 6200, loss 0.184535, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.963135: step 6200, loss 0.125079, acc 1\n",
      "\n",
      "\n",
      "proud to welcome yet another conservative leader to our team  senator @deanheller <URL/> #nvformarco <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.964784: step 6200, loss 0.0954032, acc 1\n",
      "\n",
      "\n",
      "if anyone needed a reminder of how important it is to take back the senate hold onto the white house‚Äîlook at the supreme court ‚Äîhillary <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.967701: step 6200, loss 0.126266, acc 1\n",
      "\n",
      "\n",
      "commit to the lord whatever you do and he will establish your plans  proverbs 163 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.969585: step 6200, loss 0.116802, acc 1\n",
      "\n",
      "\n",
      "our nation faces grave national security threats we must act decisively to protect american citizens from terrorists at home and abroad <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.971330: step 6200, loss 0.0973833, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump with cruz you lose #trump2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.973154: step 6200, loss 0.212929, acc 1\n",
      "\n",
      "\n",
      "donald trump and planned parenthood watch  <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.975013: step 6200, loss 0.088285, acc 1\n",
      "\n",
      "\n",
      "in order to keep america safe we must have the best trained and most equipped military in the world my plan <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.976650: step 6200, loss 0.0513464, acc 1\n",
      "\n",
      "\n",
      "time is running out to join us in sc tomorrow were stopping in mt pleasant aiken chapin rsvp <URL/> #scformarco <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.978389: step 6200, loss 0.0949574, acc 1\n",
      "\n",
      "\n",
      "handing over my twitter account to @teammarco  tune into the #gopdebate at 9pm et on @cbs follow our liveblog <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.980177: step 6200, loss 0.0766963, acc 1\n",
      "\n",
      "\n",
      "flashback sanders endorses jesse jackson for president in ‚Äò88 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.981943: step 6200, loss 0.118576, acc 1\n",
      "\n",
      "\n",
      "watch erica garners powerful endorsement of our campaign <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.983782: step 6200, loss 0.277794, acc 1\n",
      "\n",
      "\n",
      "i‚Äôll be talking with @seanhannity on @foxnews tonight at 10 pm et hope you‚Äôll tune in <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.985450: step 6200, loss 0.114599, acc 1\n",
      "\n",
      "\n",
      "because of leadership strength and determination 2 florida schools divided by a river a rivalry reunited watch <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.987175: step 6200, loss 0.108549, acc 1\n",
      "\n",
      "\n",
      "killer mike and nina turner visiting stroys barber shop in columbia sc to talk to voters <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.989007: step 6200, loss 0.0945964, acc 1\n",
      "\n",
      "\n",
      "track two from the album #scripted in essence <URL/> cc @marcorubio <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.990654: step 6200, loss 0.0942365, acc 1\n",
      "\n",
      "\n",
      "its not tough to take an elderly womans home @realdonaldtrump #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.992345: step 6200, loss 0.113905, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump dont forget tomorrow is #bellletstalk day <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.993991: step 6200, loss 0.179015, acc 1\n",
      "\n",
      "\n",
      "the line waiting outside to get in to our henderson nv rally with @glennbeck #nvgopcaucus <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.995728: step 6200, loss 0.0775184, acc 1\n",
      "\n",
      "\n",
      "@berniesanders goes full socialist we will raise taxes  yes we will <URL/> via @youtube <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.997525: step 6200, loss 0.0678515, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump @megynkelly if you cant handle kelly how are you gonna handle the world just curious take her on again dont run <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:38.999483: step 6200, loss 0.0565739, acc 1\n",
      "\n",
      "\n",
      "@hillaryclinton i dont know why youre not capitalising more on the fact that you could be the first female president <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.001252: step 6200, loss 0.105346, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump have you thought about ben carson as your vp you should <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.003003: step 6200, loss 0.254215, acc 1\n",
      "\n",
      "\n",
      "thank you iowa <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.004643: step 6200, loss 0.129918, acc 1\n",
      "\n",
      "\n",
      "funny that jeb() didnt want help from his family in his failed campaign and didnt even want to use his last namethen mommy now brother <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.006510: step 6200, loss 0.127671, acc 1\n",
      "\n",
      "\n",
      "tune in for the #goptownhall tonight at 800 pm (est) on cnn looking forward to presenting my solutions to the problems facing our nation <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.008310: step 6200, loss 0.0971944, acc 1\n",
      "\n",
      "\n",
      "@hillaryclinton and @realdonaldtrump will get that job done üá∫üá∏ <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.010132: step 6200, loss 0.15919, acc 1\n",
      "\n",
      "\n",
      "@hillaryclinton @berniesanders @chriscuomo @msnbc @donlemon <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.011907: step 6200, loss 0.363096, acc 1\n",
      "\n",
      "\n",
      "at a time when senior poverty is increasing we must expand social security benefits not cut them <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.013637: step 6200, loss 0.0588169, acc 1\n",
      "\n",
      "\n",
      "candy is making phone calls for tonights #iacaucus from our iowa hq #bc2dc16 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.015369: step 6200, loss 0.114764, acc 1\n",
      "\n",
      "\n",
      "our health care plan is the only plan that would provide care for all americans regardless of income <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.017018: step 6200, loss 0.111548, acc 1\n",
      "\n",
      "\n",
      "florida and ohio today is your last chance to register to vote in the primary visit <URL/> to register now <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.019860: step 6200, loss 0.0944715, acc 1\n",
      "\n",
      "\n",
      "@potus is doing his job to fill the vacancy on the supreme court @marcorubio @tedcruz and gop senators should do theirs #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.021494: step 6200, loss 0.223681, acc 1\n",
      "\n",
      "\n",
      "missouris voter registration deadline is today and its an open primary visit <URL/> for details <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.023309: step 6200, loss 0.214781, acc 1\n",
      "\n",
      "\n",
      "we need someone with a proven record to take our case to the american people join us <URL/> #gopdebate <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.025084: step 6200, loss 0.0654868, acc 1\n",
      "\n",
      "\n",
      "@hillaryclinton 8 minutes ago thousands of cuban refugees crossing the border <URL/> ‚Ä¶ <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.026751: step 6200, loss 0.0573323, acc 1\n",
      "\n",
      "\n",
      "enjoyed the opportunity to join @winterjamtour this afternoon #winterjam <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.028634: step 6200, loss 0.0643756, acc 1\n",
      "\n",
      "\n",
      "everybody is laughing at jeb bushspent $100 million and is at bottom of pack a pathetic figure <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.030527: step 6200, loss 0.127601, acc 1\n",
      "\n",
      "\n",
      "rt if youre tuning in to tonights #gopdebate on @foxnews #cruzcrew #caucusforcruz #iacaucus <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.032388: step 6200, loss 0.117609, acc 1\n",
      "\n",
      "\n",
      "an amazing crowd of supporters at our town hall in virginia city nevada thank you to all who were in attendance <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.034578: step 6200, loss 0.0814936, acc 1\n",
      "\n",
      "\n",
      "heading to baton rouge louisiana for a speech expecting a very large crowd see you soon #trump2016 #makeamericagreatagain <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.036393: step 6200, loss 0.0986244, acc 1\n",
      "\n",
      "\n",
      "my sister doro spent yesterday campaigning ‚Äî and taking selfies ‚Äî with our south carolina volunteers <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.038241: step 6200, loss 0.0895445, acc 1\n",
      "\n",
      "\n",
      "rsvp now for our rally in oklahoma city on friday dont miss your chance to join us before march 1st <URL/> #okformarco <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.040092: step 6200, loss 0.0739638, acc 1\n",
      "\n",
      "\n",
      "new hampshire  thank you for welcoming us for your votes for the hours you gave us on phones and the doors you knocked <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.041774: step 6200, loss 0.164264, acc 1\n",
      "\n",
      "\n",
      "watch jebs response on how hell destroy isis #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.043651: step 6200, loss 0.0647534, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump quit talking about yourself in the third person you fucking weirdo <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.045368: step 6200, loss 0.0725152, acc 1\n",
      "\n",
      "\n",
      "if you cant afford to take care of our veterans then dont send them to war #demdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.047221: step 6200, loss 0.175434, acc 1\n",
      "\n",
      "\n",
      "24 hour contest win tickets to the #gopdebate in greenville enter now  <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.048840: step 6200, loss 0.109937, acc 1\n",
      "\n",
      "\n",
      "in 1968 shirley chisholm became the first african american woman elected to congress #blackhistorymonth <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.050642: step 6200, loss 0.219545, acc 1\n",
      "\n",
      "\n",
      "@hyman7718 thank you honored to have your support stephanie <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.052522: step 6200, loss 0.173502, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump did you hear the one about another negative zero endorsing rubio it must be his cute little puppy growl charm <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.054307: step 6200, loss 0.0703662, acc 1\n",
      "\n",
      "\n",
      "the polls are now open in new hampshire find your polling location here <URL/> #fitn #nhprimary <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.056138: step 6200, loss 0.155463, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump a homophobe endorsing a bigot shocker <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.057590: step 6200, loss 0.277111, acc 1\n",
      "\n",
      "\n",
      "wall street may have an endless supply of money but we have millions of working families standing together demanding real changes <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.059344: step 6200, loss 0.146507, acc 1\n",
      "\n",
      "\n",
      "on the questions of life and marriage a record matters #goptownhall <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.060995: step 6200, loss 0.161766, acc 1\n",
      "\n",
      "\n",
      "@tedcruz @teamtedcruz #cruzcrew #cruztovictory hay @realdonaldtrump what have you done for usa lately @theblaze <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.062438: step 6200, loss 0.17667, acc 1\n",
      "\n",
      "\n",
      "@realbencarson @dmregister #votecarson #bornintheusa #tcot #ccot #iowa #bc2dc16 #iowa #gopdebate #integritymatters <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.064145: step 6200, loss 0.0659782, acc 1\n",
      "\n",
      "\n",
      "@hillaryclinton @realdonaldtrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.065657: step 6200, loss 0.167238, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump @xxflame @bobvanderplaats or hes more liked than trump in every demographic <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.067385: step 6200, loss 0.201421, acc 1\n",
      "\n",
      "\n",
      "only a few hours left nh i am asking for your vote find your polling location here <URL/> #fitn #christie2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.069042: step 6200, loss 0.17165, acc 1\n",
      "\n",
      "\n",
      "@chrischristie <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.071882: step 6200, loss 0.141199, acc 1\n",
      "\n",
      "\n",
      "@bensasse looks more like a gym rat than a us senator how the hell did he ever get elected @greta <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.073654: step 6200, loss 0.0964736, acc 1\n",
      "\n",
      "\n",
      "@marcorubio not a chance way to busy supporting #berniesanders <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.075527: step 6200, loss 0.134347, acc 1\n",
      "\n",
      "\n",
      "@tedcruz @glennbeck @realdonaldtrump trump 2016 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.077140: step 6200, loss 0.24854, acc 1\n",
      "\n",
      "\n",
      "tune in #bernieinreno live here <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.078731: step 6200, loss 0.109938, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump thank you this is the main reason i will vote for you my kids need to get back to the basics <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.080458: step 6200, loss 0.0793069, acc 1\n",
      "\n",
      "\n",
      "@hillaryclinton @billclinton @jebbush @realdonaldtrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.082023: step 6200, loss 0.151932, acc 1\n",
      "\n",
      "\n",
      "i urge sec clinton to join me in saying loudly and clearly that we will never cut social security <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.083576: step 6200, loss 0.094006, acc 1\n",
      "\n",
      "\n",
      "this looks to me like colorado is prepared to make a political revolution #bernieindenver <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.085280: step 6200, loss 0.0660521, acc 1\n",
      "\n",
      "\n",
      "thank you <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.087072: step 6200, loss 0.117221, acc 1\n",
      "\n",
      "\n",
      "#makeamericagreatagain <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.088861: step 6200, loss 0.220544, acc 1\n",
      "\n",
      "\n",
      "we are building together is a political revolution that will bring tens of millions of people together join us <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.090522: step 6200, loss 0.0771081, acc 1\n",
      "\n",
      "\n",
      "4 days until the iowa caucus we‚Äôre so grateful to have @djscotter on our team thanks for all you do <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.092255: step 6200, loss 0.0779963, acc 1\n",
      "\n",
      "\n",
      "as president i will work tirelessly to seek out and destroy global terrorism #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.094082: step 6200, loss 0.0803993, acc 1\n",
      "\n",
      "\n",
      "during last night‚Äôs #gopdebate i called out @realdonaldtrump he is not a true conservative watch <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.095761: step 6200, loss 0.0840314, acc 1\n",
      "\n",
      "\n",
      "@hillaryclinton u yelled @ the person who asked u about ur lying u r mean <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.097713: step 6200, loss 0.0929564, acc 1\n",
      "\n",
      "\n",
      "there‚Äôs a great story behind this unusual campaign stop üé≥ <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.099511: step 6200, loss 0.0704607, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.101126: step 6200, loss 0.125079, acc 1\n",
      "\n",
      "\n",
      "i will end common core its a disaster <URL/> #makeamericagreatagain #trump2016 <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.102942: step 6200, loss 0.155303, acc 1\n",
      "\n",
      "\n",
      "@tedcruz @glennbeck @realdonaldtrump you cant claim to defend cotus while your offending it #fail <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.104799: step 6200, loss 0.0977906, acc 1\n",
      "\n",
      "\n",
      "@jebbush @marcorubio considering the polls seems to me the commercial money could be better spent like going after the leaders in the polls <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.106551: step 6200, loss 0.0983219, acc 1\n",
      "\n",
      "\n",
      "@berniesanders the fact that we cant get adequate healthcare to our veterans is also a national disgrace <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.108438: step 6200, loss 0.127537, acc 1\n",
      "\n",
      "\n",
      "i believe in the teddy roosevelt philosophy #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.110273: step 6200, loss 0.103904, acc 1\n",
      "\n",
      "\n",
      "behind the scenes *right* after our win in nevada <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.112153: step 6200, loss 0.0910706, acc 1\n",
      "\n",
      "\n",
      "iowa has the chance to do something extraordinary to pull this country back #caucusforcruz <URL/> <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.113825: step 6200, loss 0.0640285, acc 1\n",
      "\n",
      "\n",
      "watch and rt  <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.115630: step 6200, loss 0.244747, acc 1\n",
      "\n",
      "\n",
      "rt if you agree we must allow our soldiers to do their jobs #gopdebate <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.117309: step 6200, loss 0.100491, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump sir you withstand the barrage frm th liarshaters like th champion that you ar stillto see these attacks is infuriating <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.119128: step 6200, loss 0.0691437, acc 1\n",
      "\n",
      "\n",
      "woah big turnout for our #gotvforbernie rally at @uofnh <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.120834: step 6200, loss 0.0765482, acc 1\n",
      "\n",
      "\n",
      "i believe that we need a political revolution the same old same old just isnt going to do it we have to be bold #bernieinmn <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.123676: step 6200, loss 0.106189, acc 1\n",
      "\n",
      "\n",
      "@realdonaldtrump yeah‚Ä¶this guy <URL/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.125366: step 6200, loss 0.186472, acc 1\n",
      "\n",
      "\n",
      "@jebbush @mschultz16 <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.126869: step 6200, loss 0.237223, acc 1\n",
      "\n",
      "\n",
      "thank you @opinionamerica1 really appreciate it <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n",
      "nonexample\n",
      "2016-03-08T02:13:39.128425: step 6200, loss 0.0800488, acc 1\n",
      "\n",
      "\n",
      "2016-03-08T02:13:39.132432: step 6200, loss 0.0645804, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.134117: step 6200, loss 2.5211, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.135905: step 6200, loss 0.111667, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.137659: step 6200, loss 0.136887, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.139272: step 6200, loss 0.0972526, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.141059: step 6200, loss 0.1616, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.142689: step 6200, loss 0.0618077, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.144476: step 6200, loss 0.0778001, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.146068: step 6200, loss 0.170669, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.147787: step 6200, loss 0.18691, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.149443: step 6200, loss 0.0944384, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.151020: step 6200, loss 0.109298, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.152900: step 6200, loss 0.114607, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.154547: step 6200, loss 0.072699, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.156170: step 6200, loss 0.171537, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.157999: step 6200, loss 0.14384, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.159944: step 6200, loss 0.111715, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.161574: step 6200, loss 0.258207, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.163197: step 6200, loss 0.161759, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.164879: step 6200, loss 0.0680539, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.166501: step 6200, loss 0.151556, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.168592: step 6200, loss 0.0900926, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.170162: step 6200, loss 2.80481, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.172040: step 6200, loss 0.110058, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.174600: step 6200, loss 4.19407, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.176283: step 6200, loss 0.0635685, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.178000: step 6200, loss 0.0894209, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.179756: step 6200, loss 0.088258, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.181216: step 6200, loss 0.157081, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.182979: step 6200, loss 0.149363, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.184679: step 6200, loss 0.169623, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.186244: step 6200, loss 0.0741036, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.187693: step 6200, loss 2.35102, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.189302: step 6200, loss 0.0590365, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.190964: step 6200, loss 0.0819033, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.192497: step 6200, loss 0.0959015, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.194092: step 6200, loss 0.15621, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.195620: step 6200, loss 2.98166, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.197240: step 6200, loss 0.1859, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.198913: step 6200, loss 0.111804, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.200463: step 6200, loss 0.117277, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.202195: step 6200, loss 0.204807, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.203953: step 6200, loss 0.171865, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.205619: step 6200, loss 0.111845, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.206993: step 6200, loss 0.147827, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.208580: step 6200, loss 0.0928756, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.209956: step 6200, loss 0.141721, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.211437: step 6200, loss 0.0866205, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.213035: step 6200, loss 0.198891, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.214619: step 6200, loss 0.057809, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.216239: step 6200, loss 0.0970289, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.217947: step 6200, loss 0.131355, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.219626: step 6200, loss 0.0763848, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.221249: step 6200, loss 0.0789326, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.222712: step 6200, loss 0.0703103, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.224330: step 6200, loss 0.25733, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.226717: step 6200, loss 0.074163, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.228443: step 6200, loss 0.132197, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.229990: step 6200, loss 0.0974298, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.231645: step 6200, loss 0.123778, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.233120: step 6200, loss 0.144159, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.234705: step 6200, loss 0.18678, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.236126: step 6200, loss 0.106023, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.237746: step 6200, loss 0.0652115, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.239142: step 6200, loss 0.127708, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.240737: step 6200, loss 0.084131, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.242322: step 6200, loss 0.0831803, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.243863: step 6200, loss 0.141199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.245337: step 6200, loss 3.59743, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.246898: step 6200, loss 0.354824, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.248626: step 6200, loss 0.101164, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.250063: step 6200, loss 0.0916171, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.251771: step 6200, loss 0.0956722, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.253449: step 6200, loss 0.149939, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.255008: step 6200, loss 0.1876, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.256471: step 6200, loss 0.104897, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.258130: step 6200, loss 0.0866476, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.259609: step 6200, loss 0.119657, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.261266: step 6200, loss 0.079773, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.262921: step 6200, loss 0.160036, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.264510: step 6200, loss 0.106551, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.266229: step 6200, loss 0.0659814, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.267873: step 6200, loss 0.09519, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.269605: step 6200, loss 0.150006, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.271096: step 6200, loss 0.154237, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.272653: step 6200, loss 0.0813774, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.274267: step 6200, loss 0.0499604, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.275847: step 6200, loss 0.0596346, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.278146: step 6200, loss 2.75299, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.279860: step 6200, loss 3.23717, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.281426: step 6200, loss 0.0811155, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.282865: step 6200, loss 0.0997146, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.284569: step 6200, loss 0.1616, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.286203: step 6200, loss 0.175595, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.288040: step 6200, loss 0.0596889, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.289544: step 6200, loss 0.135179, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.290966: step 6200, loss 0.0684084, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.292754: step 6200, loss 0.0588498, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.294439: step 6200, loss 0.119684, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.296013: step 6200, loss 0.0745517, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.297759: step 6200, loss 0.0860528, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.299533: step 6200, loss 0.0754299, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.301164: step 6200, loss 0.276464, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.302929: step 6200, loss 0.250432, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.305026: step 6200, loss 0.0923671, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.306777: step 6200, loss 0.0716951, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.308530: step 6200, loss 0.267178, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.310179: step 6200, loss 0.0993647, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.311865: step 6200, loss 0.105503, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.313281: step 6200, loss 0.216604, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.314984: step 6200, loss 0.163807, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.316708: step 6200, loss 4.17941, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.318074: step 6200, loss 0.117221, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.319562: step 6200, loss 0.126831, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.321194: step 6200, loss 0.122226, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.322857: step 6200, loss 0.167715, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.324424: step 6200, loss 0.156335, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.325984: step 6200, loss 0.113487, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.327590: step 6200, loss 0.096066, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.329970: step 6200, loss 0.163549, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.331605: step 6200, loss 0.1006, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.333352: step 6200, loss 0.080649, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.335105: step 6200, loss 0.130683, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.336653: step 6200, loss 0.117939, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.338209: step 6200, loss 0.0775857, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.339910: step 6200, loss 2.01927, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.341429: step 6200, loss 0.0575866, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.342924: step 6200, loss 0.169073, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.344616: step 6200, loss 3.90662, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.346211: step 6200, loss 0.0943062, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.347690: step 6200, loss 0.102896, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.349315: step 6200, loss 0.0824859, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.350977: step 6200, loss 0.0804886, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.352560: step 6200, loss 0.0858341, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.354269: step 6200, loss 0.171689, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.355928: step 6200, loss 0.0936559, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.357612: step 6200, loss 0.184535, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.359205: step 6200, loss 0.125079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.360922: step 6200, loss 0.0954032, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.362318: step 6200, loss 0.126266, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.363893: step 6200, loss 0.116802, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.365322: step 6200, loss 0.0973833, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.366910: step 6200, loss 0.212929, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.368587: step 6200, loss 0.088285, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.370259: step 6200, loss 0.0513464, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.372079: step 6200, loss 0.0949574, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.373846: step 6200, loss 0.0766963, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.375399: step 6200, loss 0.118576, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.377229: step 6200, loss 0.277794, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.379115: step 6200, loss 0.114599, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.382807: step 6200, loss 0.108549, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.385014: step 6200, loss 2.34488, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.386923: step 6200, loss 0.0945964, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.388890: step 6200, loss 0.0942365, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.391051: step 6200, loss 0.113905, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.392960: step 6200, loss 0.179015, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.395171: step 6200, loss 0.0775184, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.397187: step 6200, loss 0.0678515, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.398937: step 6200, loss 0.0565739, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.400871: step 6200, loss 0.105346, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.403126: step 6200, loss 3.84334, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.405047: step 6200, loss 0.254215, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.407178: step 6200, loss 0.129918, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.409181: step 6200, loss 0.127671, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.411069: step 6200, loss 1.15126, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.413078: step 6200, loss 0.0971944, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.415077: step 6200, loss 0.15919, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.417142: step 6200, loss 0.363096, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.419080: step 6200, loss 0.0588169, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.421339: step 6200, loss 0.114764, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.423563: step 6200, loss 0.111548, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.425477: step 6200, loss 0.0944715, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.427610: step 6200, loss 0.223681, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.429674: step 6200, loss 0.214781, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.431775: step 6200, loss 0.0654868, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.435079: step 6200, loss 0.0573323, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.437404: step 6200, loss 0.0643756, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.440023: step 6200, loss 0.127601, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.442280: step 6200, loss 0.117609, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.444564: step 6200, loss 0.0814936, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.446984: step 6200, loss 0.0986244, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.449193: step 6200, loss 0.0895445, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.451396: step 6200, loss 0.0739638, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.453584: step 6200, loss 0.164264, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.455911: step 6200, loss 0.0647534, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.458077: step 6200, loss 0.0725152, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.460117: step 6200, loss 2.29471, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.462125: step 6200, loss 0.175434, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.464501: step 6200, loss 0.109937, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.466981: step 6200, loss 0.219545, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.469252: step 6200, loss 0.173502, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.471215: step 6200, loss 0.0703662, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.473459: step 6200, loss 0.155463, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.475315: step 6200, loss 0.277111, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.477487: step 6200, loss 0.146507, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.479714: step 6200, loss 2.38485, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.481764: step 6200, loss 3.31818, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.484026: step 6200, loss 3.74458, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.488098: step 6200, loss 0.161766, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.490215: step 6200, loss 0.17667, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.492608: step 6200, loss 3.57914, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.494509: step 6200, loss 0.0659782, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.496874: step 6200, loss 0.167238, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.499053: step 6200, loss 0.201421, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.501228: step 6200, loss 0.17165, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.503490: step 6200, loss 0.141199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.505815: step 6200, loss 0.0964736, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.507861: step 6200, loss 0.134347, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.509796: step 6200, loss 0.24854, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.511861: step 6200, loss 0.109938, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.513752: step 6200, loss 0.0793069, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.515775: step 6200, loss 0.151932, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.518117: step 6200, loss 0.094006, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.520326: step 6200, loss 0.0660521, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.522250: step 6200, loss 0.117221, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.524295: step 6200, loss 0.220544, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.526080: step 6200, loss 0.0771081, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.527803: step 6200, loss 0.0779963, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.529472: step 6200, loss 0.0803993, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.531367: step 6200, loss 0.0840314, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.533233: step 6200, loss 0.0929564, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.534960: step 6200, loss 0.0704607, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.537581: step 6200, loss 0.125079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.539529: step 6200, loss 0.155303, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.541179: step 6200, loss 0.0977906, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.542917: step 6200, loss 0.0983219, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.544744: step 6200, loss 0.127537, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.546443: step 6200, loss 0.103904, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.548375: step 6200, loss 2.8, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.550286: step 6200, loss 0.0910706, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.552127: step 6200, loss 0.0640285, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.553927: step 6200, loss 0.244747, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.555644: step 6200, loss 0.100491, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.557364: step 6200, loss 0.0691437, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.559198: step 6200, loss 0.0765482, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.561034: step 6200, loss 0.106189, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.562729: step 6200, loss 0.186472, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.564619: step 6200, loss 0.237223, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.566354: step 6200, loss 0.0800488, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.568082: step 6200, loss 0.105697, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.569720: step 6200, loss 0.107779, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.571615: step 6200, loss 0.0975419, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.573740: step 6200, loss 0.0788999, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.575549: step 6200, loss 0.0906006, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.577376: step 6200, loss 0.105892, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.579134: step 6200, loss 0.124995, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.580656: step 6200, loss 0.112791, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.582452: step 6200, loss 2.22808, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.584122: step 6200, loss 0.134248, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.585919: step 6200, loss 0.1035, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.588604: step 6200, loss 0.096436, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.590302: step 6200, loss 0.0976795, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.592000: step 6200, loss 3.49761, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.593661: step 6200, loss 0.119392, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.595283: step 6200, loss 0.129532, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.596938: step 6200, loss 0.0765161, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.598613: step 6200, loss 0.213691, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.600517: step 6200, loss 0.103979, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.602184: step 6200, loss 0.131839, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.604008: step 6200, loss 0.0963491, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.605838: step 6200, loss 0.174641, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.607628: step 6200, loss 0.151596, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.609538: step 6200, loss 3.32082, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.611426: step 6200, loss 0.0904406, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.613412: step 6200, loss 0.203639, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.615314: step 6200, loss 3.12576, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.617039: step 6200, loss 0.141199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.618961: step 6200, loss 0.144494, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.620811: step 6200, loss 0.0620139, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.622340: step 6200, loss 0.0642833, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.624000: step 6200, loss 0.245415, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.625806: step 6200, loss 0.0992176, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.627663: step 6200, loss 0.144956, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.629456: step 6200, loss 0.091997, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.631231: step 6200, loss 0.0724748, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.633075: step 6200, loss 2.67342, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.634841: step 6200, loss 4.33694, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.636698: step 6200, loss 0.141725, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.639450: step 6200, loss 0.0759646, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.641338: step 6200, loss 0.836195, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.643029: step 6200, loss 0.111851, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.644883: step 6200, loss 0.216659, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.646554: step 6200, loss 0.166729, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.648414: step 6200, loss 0.274542, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.650314: step 6200, loss 0.0784872, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.652162: step 6200, loss 0.113422, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.653996: step 6200, loss 0.117737, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.655799: step 6200, loss 0.110819, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.657689: step 6200, loss 0.120951, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.659469: step 6200, loss 0.0745244, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.661271: step 6200, loss 0.116208, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.663130: step 6200, loss 0.10081, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.665004: step 6200, loss 0.121921, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.666693: step 6200, loss 0.109651, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.668617: step 6200, loss 0.147962, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.670364: step 6200, loss 0.0990089, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.672170: step 6200, loss 2.79624, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.673888: step 6200, loss 0.0608572, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.675682: step 6200, loss 0.0664345, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.677522: step 6200, loss 0.311854, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.679342: step 6200, loss 0.134407, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.681231: step 6200, loss 0.136842, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.683088: step 6200, loss 0.0929921, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.684945: step 6200, loss 2.17383, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.686597: step 6200, loss 0.200771, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.688431: step 6200, loss 0.289114, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.690977: step 6200, loss 0.0719627, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.692663: step 6200, loss 0.140142, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.694324: step 6200, loss 0.129802, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.695937: step 6200, loss 3.22889, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.697584: step 6200, loss 0.196015, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.699417: step 6200, loss 0.105229, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.701235: step 6200, loss 0.120639, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.702944: step 6200, loss 0.132372, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.704731: step 6200, loss 0.123393, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.706353: step 6200, loss 0.207099, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.707956: step 6200, loss 0.0823736, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.709936: step 6200, loss 0.100432, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.711598: step 6200, loss 3.49103, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.713426: step 6200, loss 0.1058, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.715211: step 6200, loss 0.154362, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.717034: step 6200, loss 0.223483, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.718649: step 6200, loss 0.317876, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.720377: step 6200, loss 0.20815, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.722272: step 6200, loss 0.139855, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.724129: step 6200, loss 0.147628, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.725871: step 6200, loss 0.0977252, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.727617: step 6200, loss 0.0858016, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.729430: step 6200, loss 0.110903, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.731335: step 6200, loss 0.235119, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.733128: step 6200, loss 0.389571, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.734841: step 6200, loss 0.074626, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.736507: step 6200, loss 0.260379, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.738368: step 6200, loss 0.230801, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.740037: step 6200, loss 0.239298, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.742413: step 6200, loss 0.0993491, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.744067: step 6200, loss 0.0594066, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.745773: step 6200, loss 0.0710099, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.747548: step 6200, loss 0.0835421, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.749149: step 6200, loss 0.0928849, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.750726: step 6200, loss 0.136927, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.752436: step 6200, loss 0.206746, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.754122: step 6200, loss 0.125498, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.755761: step 6200, loss 0.0872496, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.757449: step 6200, loss 0.153475, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.759094: step 6200, loss 0.0940835, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.760907: step 6200, loss 0.105343, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.762556: step 6200, loss 0.105518, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.764248: step 6200, loss 0.168035, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.766021: step 6200, loss 0.188836, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.767895: step 6200, loss 3.64465, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.769720: step 6200, loss 0.111352, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.771503: step 6200, loss 0.0807692, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.773246: step 6200, loss 0.154425, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.775046: step 6200, loss 2.17975, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.776638: step 6200, loss 0.120476, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.778292: step 6200, loss 0.121154, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.779950: step 6200, loss 0.193236, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.782079: step 6200, loss 0.0685185, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.784078: step 6200, loss 0.12306, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.786217: step 6200, loss 0.12927, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.788241: step 6200, loss 0.0774821, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.790693: step 6200, loss 0.0716145, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.794533: step 6200, loss 0.0794801, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.796794: step 6200, loss 0.0665655, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.798948: step 6200, loss 0.280455, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.801016: step 6200, loss 0.17888, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.803207: step 6200, loss 0.0617964, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.805177: step 6200, loss 0.329607, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.807623: step 6200, loss 0.0974659, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.809795: step 6200, loss 0.0767137, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.811815: step 6200, loss 0.339961, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.813649: step 6200, loss 0.108321, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.815716: step 6200, loss 0.0675127, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.817840: step 6200, loss 0.110742, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.820044: step 6200, loss 0.152844, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.822198: step 6200, loss 0.161867, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.824264: step 6200, loss 3.09831, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.826422: step 6200, loss 0.113205, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.828502: step 6200, loss 0.220064, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.830420: step 6200, loss 0.0774586, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.832342: step 6200, loss 0.0804743, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.834368: step 6200, loss 0.10608, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.836576: step 6200, loss 0.364359, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.838916: step 6200, loss 0.0673304, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.840911: step 6200, loss 0.0881447, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.843042: step 6200, loss 0.145829, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.847561: step 6200, loss 0.156335, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.849811: step 6200, loss 0.124005, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.852113: step 6200, loss 0.187293, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.854284: step 6200, loss 0.183517, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.856506: step 6200, loss 0.317876, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.858709: step 6200, loss 0.127241, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.860903: step 6200, loss 0.128494, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.863127: step 6200, loss 0.084066, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.865584: step 6200, loss 0.0779019, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.867950: step 6200, loss 0.14872, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.870395: step 6200, loss 1.78457, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.872625: step 6200, loss 0.0586306, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.874887: step 6200, loss 0.0892953, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.877272: step 6200, loss 0.110535, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.879265: step 6200, loss 0.101058, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.881164: step 6200, loss 0.126884, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.883106: step 6200, loss 0.072914, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.885138: step 6200, loss 0.057459, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.887236: step 6200, loss 3.34713, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.889523: step 6200, loss 0.0914677, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.891527: step 6200, loss 0.12347, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.893619: step 6200, loss 0.146791, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.895583: step 6200, loss 0.108803, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.898315: step 6200, loss 0.101667, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.900319: step 6200, loss 0.113198, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.902192: step 6200, loss 0.0675296, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.904002: step 6200, loss 3.03793, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.905873: step 6200, loss 3.27771, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.907856: step 6200, loss 0.117218, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.909875: step 6200, loss 0.0746143, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.911722: step 6200, loss 0.107782, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.913649: step 6200, loss 0.112005, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.915770: step 6200, loss 0.0816748, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.917555: step 6200, loss 0.0738922, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.919524: step 6200, loss 0.163072, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.921476: step 6200, loss 0.0799588, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.923470: step 6200, loss 0.0871756, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.925397: step 6200, loss 0.116334, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.927277: step 6200, loss 2.03597, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.929259: step 6200, loss 0.0631953, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.931206: step 6200, loss 0.0840317, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.933196: step 6200, loss 0.0862663, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.935105: step 6200, loss 0.136297, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.937143: step 6200, loss 0.0810554, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.938809: step 6200, loss 2.19366, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.940732: step 6200, loss 0.0968601, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.942621: step 6200, loss 0.116364, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.944529: step 6200, loss 0.195641, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.946572: step 6200, loss 0.0864798, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.949418: step 6200, loss 0.15676, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.951903: step 6200, loss 0.135749, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.953898: step 6200, loss 0.156125, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.955925: step 6200, loss 0.121567, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.957956: step 6200, loss 0.0768624, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.959654: step 6200, loss 0.0886264, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.961700: step 6200, loss 0.101696, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.963919: step 6200, loss 3.84931, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.965989: step 6200, loss 0.124094, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.968072: step 6200, loss 0.0914939, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.970090: step 6200, loss 0.108622, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.972069: step 6200, loss 0.165724, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.974234: step 6200, loss 2.90348, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.976428: step 6200, loss 0.106158, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.978682: step 6200, loss 0.092127, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.980831: step 6200, loss 0.105127, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.982805: step 6200, loss 0.0922751, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.984880: step 6200, loss 0.0622033, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.986637: step 6200, loss 0.106742, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.988562: step 6200, loss 2.04794, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:39.990745: step 6200, loss 0.0974182, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.992653: step 6200, loss 0.0974458, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.994745: step 6200, loss 0.153027, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.996944: step 6200, loss 0.0710369, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:39.998673: step 6200, loss 0.150118, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.001321: step 6200, loss 0.191487, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.003296: step 6200, loss 0.0592797, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.005241: step 6200, loss 0.112712, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.007121: step 6200, loss 0.080793, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.008930: step 6200, loss 0.0710838, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.010570: step 6200, loss 0.0688099, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.012254: step 6200, loss 0.0791817, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.013678: step 6200, loss 0.085485, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.015067: step 6200, loss 4.16328, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.016649: step 6200, loss 0.0581158, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.018939: step 6200, loss 0.0550388, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.020978: step 6200, loss 0.0881589, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.022918: step 6200, loss 0.11892, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.024994: step 6200, loss 3.26971, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.026924: step 6200, loss 0.0692336, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.028779: step 6200, loss 0.0695372, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.030496: step 6200, loss 0.0703507, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.032259: step 6200, loss 0.0856455, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.033944: step 6200, loss 0.162534, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.035556: step 6200, loss 0.228397, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.037404: step 6200, loss 0.153868, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.039139: step 6200, loss 0.0946238, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.040668: step 6200, loss 0.0527153, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.042229: step 6200, loss 0.0799014, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.044102: step 6200, loss 0.0794325, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.045828: step 6200, loss 0.0937147, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.047672: step 6200, loss 0.140513, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.049244: step 6200, loss 0.115774, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.053090: step 6200, loss 0.196843, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.054925: step 6200, loss 0.0655636, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.056651: step 6200, loss 1.88508, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.058637: step 6200, loss 0.0771118, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.060594: step 6200, loss 2.40884, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.062810: step 6200, loss 0.149699, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.064509: step 6200, loss 0.229427, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.066396: step 6200, loss 0.144459, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.068131: step 6200, loss 0.0655058, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.069899: step 6200, loss 0.118526, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.071545: step 6200, loss 0.116232, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.073221: step 6200, loss 0.0869737, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.075004: step 6200, loss 0.110878, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.076794: step 6200, loss 3.98375, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.078271: step 6200, loss 0.165618, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.080357: step 6200, loss 2.67568, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.082702: step 6200, loss 0.188045, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.084534: step 6200, loss 0.174505, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.086710: step 6200, loss 0.0744971, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.088751: step 6200, loss 1.75256, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.090612: step 6200, loss 0.0671795, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.092275: step 6200, loss 0.137642, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.094384: step 6200, loss 0.0947325, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.096199: step 6200, loss 0.0832199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.097921: step 6200, loss 4.3681, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.099472: step 6200, loss 0.110294, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.101159: step 6200, loss 0.072692, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.104870: step 6200, loss 0.156335, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.106581: step 6200, loss 0.0691879, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.108416: step 6200, loss 0.0913083, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.110167: step 6200, loss 1.23139, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.111800: step 6200, loss 0.0907314, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.113592: step 6200, loss 0.0646957, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.115213: step 6200, loss 0.0845768, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.116643: step 6200, loss 0.0989924, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.118305: step 6200, loss 0.094033, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.119855: step 6200, loss 0.0959287, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.121659: step 6200, loss 2.37594, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.123691: step 6200, loss 0.0785772, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.125726: step 6200, loss 0.166226, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.127583: step 6200, loss 0.0861221, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.129612: step 6200, loss 0.102466, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.131485: step 6200, loss 0.0597646, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.133675: step 6200, loss 0.113039, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.135603: step 6200, loss 0.0733535, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.137651: step 6200, loss 0.0869155, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.139203: step 6200, loss 0.0693244, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.140917: step 6200, loss 0.0847926, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.142542: step 6200, loss 0.0603272, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.144136: step 6200, loss 0.142462, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.145613: step 6200, loss 0.0758917, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.147245: step 6200, loss 0.13044, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.149032: step 6200, loss 0.11941, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.150413: step 6200, loss 0.10249, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.151858: step 6200, loss 0.141199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.153525: step 6200, loss 0.0670303, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.157222: step 6200, loss 0.0832809, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.159347: step 6200, loss 0.0783464, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.162086: step 6200, loss 0.100123, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.164074: step 6200, loss 0.243596, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.165729: step 6200, loss 0.0794259, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.168006: step 6200, loss 0.107948, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.170301: step 6200, loss 0.146091, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.172240: step 6200, loss 0.0604589, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.174417: step 6200, loss 0.0969421, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.176242: step 6200, loss 3.35549, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.178109: step 6200, loss 0.300666, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.179702: step 6200, loss 4.19394, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.181228: step 6200, loss 0.102896, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.182908: step 6200, loss 0.0810971, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.184402: step 6200, loss 0.0941373, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.186023: step 6200, loss 0.0873, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.187600: step 6200, loss 0.102374, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.189463: step 6200, loss 0.122943, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.191063: step 6200, loss 0.0646832, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.192810: step 6200, loss 0.202503, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.194867: step 6200, loss 0.0693894, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.197019: step 6200, loss 3.20755, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.198852: step 6200, loss 0.117221, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.200473: step 6200, loss 2.97392, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.202206: step 6200, loss 0.0699213, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.203819: step 6200, loss 0.128516, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.205981: step 6200, loss 0.76851, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.209825: step 6200, loss 0.0603026, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.211731: step 6200, loss 0.222841, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.213832: step 6200, loss 0.0503535, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.215930: step 6200, loss 1.85185, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.217711: step 6200, loss 0.0816791, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.219523: step 6200, loss 0.20315, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.221152: step 6200, loss 0.144641, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.223022: step 6200, loss 4.01072, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.224776: step 6200, loss 0.132622, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.226384: step 6200, loss 0.18828, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.228046: step 6200, loss 0.129774, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.229973: step 6200, loss 0.069501, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.231602: step 6200, loss 0.0858775, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.233306: step 6200, loss 1.66654, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.235057: step 6200, loss 0.135451, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.236903: step 6200, loss 0.0803837, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.238771: step 6200, loss 0.0878398, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.240572: step 6200, loss 0.11328, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.242263: step 6200, loss 0.248442, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.243794: step 6200, loss 0.107244, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.245697: step 6200, loss 2.4896, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.247611: step 6200, loss 0.0619479, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.249804: step 6200, loss 0.157145, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.251466: step 6200, loss 0.158084, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.253040: step 6200, loss 2.53523, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.254886: step 6200, loss 0.0917621, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.256518: step 6200, loss 0.103913, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.258309: step 6200, loss 0.123852, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.261126: step 6200, loss 0.302811, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.263444: step 6200, loss 0.161404, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.265384: step 6200, loss 0.098315, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.267020: step 6200, loss 0.0647905, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.268512: step 6200, loss 0.0920251, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.270094: step 6200, loss 0.0473911, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.271842: step 6200, loss 0.0744108, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.273627: step 6200, loss 0.0721512, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.275366: step 6200, loss 2.19584, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.276997: step 6200, loss 0.0877145, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.278576: step 6200, loss 0.06762, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.280339: step 6200, loss 0.0736964, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.281994: step 6200, loss 0.16666, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.283701: step 6200, loss 0.176328, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.285405: step 6200, loss 0.0797249, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.287156: step 6200, loss 0.0556884, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.288682: step 6200, loss 2.36154, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.290330: step 6200, loss 3.51421, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.292601: step 6200, loss 0.0952806, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.295007: step 6200, loss 0.0501963, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.297405: step 6200, loss 0.104518, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.299590: step 6200, loss 3.6523, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.301779: step 6200, loss 0.513524, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.303697: step 6200, loss 0.233296, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.305835: step 6200, loss 0.203936, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.307650: step 6200, loss 0.160766, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.309698: step 6200, loss 0.139717, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.313692: step 6200, loss 1.61603, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.315614: step 6200, loss 0.102118, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.317399: step 6200, loss 0.10218, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.319233: step 6200, loss 0.0707703, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.321457: step 6200, loss 0.141199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.323189: step 6200, loss 0.139223, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.325308: step 6200, loss 0.186731, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.327433: step 6200, loss 3.69925, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.329447: step 6200, loss 0.191684, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.331576: step 6200, loss 0.117578, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.333549: step 6200, loss 0.244217, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.335819: step 6200, loss 0.0706145, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.338151: step 6200, loss 0.0999217, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.340090: step 6200, loss 0.102424, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.342108: step 6200, loss 0.0721706, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.343764: step 6200, loss 0.113014, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.345381: step 6200, loss 0.111629, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.346960: step 6200, loss 0.073465, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.348687: step 6200, loss 0.0835626, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.350322: step 6200, loss 0.0783807, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.351834: step 6200, loss 0.0897557, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.353848: step 6200, loss 0.0662421, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.355603: step 6200, loss 0.102963, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.357526: step 6200, loss 0.139433, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.359104: step 6200, loss 0.136236, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.361054: step 6200, loss 0.117003, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.365285: step 6200, loss 0.141199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.367262: step 6200, loss 0.0662645, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.369337: step 6200, loss 0.37185, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.370950: step 6200, loss 0.181877, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.372562: step 6200, loss 0.0887526, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.374327: step 6200, loss 2.20461, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.376012: step 6200, loss 3.38882, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.377805: step 6200, loss 0.11386, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.379631: step 6200, loss 0.0838455, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.381548: step 6200, loss 0.105654, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.383920: step 6200, loss 0.104923, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.385737: step 6200, loss 0.158276, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.387798: step 6200, loss 0.1683, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.389737: step 6200, loss 0.110845, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.391472: step 6200, loss 0.207558, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.393088: step 6200, loss 0.0860081, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.394782: step 6200, loss 0.125691, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.396445: step 6200, loss 3.50343, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.398376: step 6200, loss 0.0755015, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.400017: step 6200, loss 0.0828468, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.401612: step 6200, loss 0.134502, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.403474: step 6200, loss 0.0689462, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.405114: step 6200, loss 0.0961748, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.406750: step 6200, loss 0.0692418, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.408437: step 6200, loss 0.0762148, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.410105: step 6200, loss 0.1091, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.411840: step 6200, loss 2.95765, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.415073: step 6200, loss 0.109731, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.416963: step 6200, loss 0.122609, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.419141: step 6200, loss 0.216549, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.421162: step 6200, loss 0.0903565, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.422926: step 6200, loss 0.054846, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.424593: step 6200, loss 2.31339, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.426331: step 6200, loss 0.0920107, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.428057: step 6200, loss 0.069932, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.429756: step 6200, loss 2.57988, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.431342: step 6200, loss 0.100726, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.433383: step 6200, loss 0.0854587, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.435147: step 6200, loss 0.121847, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.436862: step 6200, loss 0.268214, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.438507: step 6200, loss 0.111264, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.440208: step 6200, loss 0.0793393, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.442146: step 6200, loss 0.0726254, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.443743: step 6200, loss 0.0651338, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.445222: step 6200, loss 2.67524, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.446983: step 6200, loss 0.134908, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.448668: step 6200, loss 0.182425, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.450451: step 6200, loss 0.111948, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.452263: step 6200, loss 0.0739212, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.454072: step 6200, loss 0.162745, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.455580: step 6200, loss 0.104401, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.457158: step 6200, loss 0.0716512, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.458997: step 6200, loss 0.0891368, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.460678: step 6200, loss 0.0562047, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.462227: step 6200, loss 0.0735508, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.464225: step 6200, loss 0.103251, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.466682: step 6200, loss 0.0687921, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.468280: step 6200, loss 0.204485, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.470041: step 6200, loss 0.128671, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.471927: step 6200, loss 0.0937246, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.473665: step 6200, loss 0.173933, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.475746: step 6200, loss 0.0955013, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.477794: step 6200, loss 0.0898805, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.479666: step 6200, loss 3.3195, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.481526: step 6200, loss 0.143723, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.483146: step 6200, loss 0.189414, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.484728: step 6200, loss 0.101136, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.486468: step 6200, loss 0.0912582, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.488180: step 6200, loss 0.127707, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.490102: step 6200, loss 0.0833169, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.491811: step 6200, loss 0.110206, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.493253: step 6200, loss 1.2852, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.494940: step 6200, loss 0.0982877, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.497184: step 6200, loss 0.106066, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.499118: step 6200, loss 0.102808, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.501121: step 6200, loss 3.0873, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.503016: step 6200, loss 0.232912, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.504800: step 6200, loss 3.67939, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.506663: step 6200, loss 0.14577, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.508516: step 6200, loss 0.125079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.510283: step 6200, loss 3.93325, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.512009: step 6200, loss 0.190698, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.513545: step 6200, loss 0.0819327, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.515399: step 6200, loss 0.0833146, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.518933: step 6200, loss 0.150304, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.520952: step 6200, loss 3.60103, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.522856: step 6200, loss 0.0872303, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.524543: step 6200, loss 0.209325, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.526343: step 6200, loss 3.29145, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.528347: step 6200, loss 0.137856, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.530223: step 6200, loss 0.139985, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.531827: step 6200, loss 2.53692, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.533687: step 6200, loss 0.0716457, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.535592: step 6200, loss 0.317876, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.537377: step 6200, loss 0.0567701, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.539103: step 6200, loss 0.0980881, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.541171: step 6200, loss 0.099863, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.543000: step 6200, loss 0.0818694, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.544946: step 6200, loss 0.265191, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.546898: step 6200, loss 0.0928432, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.548809: step 6200, loss 0.0663893, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.550844: step 6200, loss 0.105876, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.552828: step 6200, loss 0.107391, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.554542: step 6200, loss 0.113482, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.556447: step 6200, loss 0.140506, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.558025: step 6200, loss 0.229872, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.559878: step 6200, loss 0.0668198, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.561918: step 6200, loss 0.157883, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.563949: step 6200, loss 0.0850027, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.566000: step 6200, loss 0.056755, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.567927: step 6200, loss 0.118176, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.572060: step 6200, loss 0.114981, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.573919: step 6200, loss 0.0745711, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.575666: step 6200, loss 0.285253, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.577497: step 6200, loss 0.159328, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.579149: step 6200, loss 0.101458, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.580811: step 6200, loss 0.208601, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.582438: step 6200, loss 0.11947, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.584006: step 6200, loss 0.0595356, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.585924: step 6200, loss 2.52708, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.587533: step 6200, loss 0.113228, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.589064: step 6200, loss 0.176638, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.590506: step 6200, loss 0.100944, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.592350: step 6200, loss 0.171279, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.594132: step 6200, loss 0.152782, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.595709: step 6200, loss 0.161552, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.597522: step 6200, loss 0.0586954, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.599362: step 6200, loss 0.0697373, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.601313: step 6200, loss 0.10922, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.602869: step 6200, loss 0.0586076, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.604706: step 6200, loss 0.0466624, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.606274: step 6200, loss 0.125916, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.608074: step 6200, loss 2.88305, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.609585: step 6200, loss 0.0632925, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.611409: step 6200, loss 0.106149, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.613055: step 6200, loss 0.0964895, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.615036: step 6200, loss 0.0979588, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.617209: step 6200, loss 0.149903, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.619225: step 6200, loss 0.113195, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.621221: step 6200, loss 0.0941366, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.624799: step 6200, loss 0.0881727, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.627116: step 6200, loss 0.108943, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.628932: step 6200, loss 0.0847013, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.631059: step 6200, loss 0.1389, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.632940: step 6200, loss 0.15154, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.634862: step 6200, loss 0.102119, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.636770: step 6200, loss 0.0627539, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.638500: step 6200, loss 0.132379, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.640608: step 6200, loss 0.0640907, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.642504: step 6200, loss 0.133749, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.644352: step 6200, loss 0.0649364, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.646393: step 6200, loss 1.9717, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.648748: step 6200, loss 0.189611, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.650845: step 6200, loss 0.115572, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.653017: step 6200, loss 0.124789, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.655316: step 6200, loss 0.136394, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.657038: step 6200, loss 0.0839567, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.658705: step 6200, loss 0.0866167, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.660718: step 6200, loss 0.122323, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.662597: step 6200, loss 0.166165, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.664561: step 6200, loss 0.0708407, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.666202: step 6200, loss 0.127697, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.668110: step 6200, loss 0.264059, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.669865: step 6200, loss 0.1862, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.671460: step 6200, loss 0.105292, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.673369: step 6200, loss 0.0701455, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.675951: step 6200, loss 0.0543915, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.677955: step 6200, loss 0.0968667, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.679446: step 6200, loss 0.0825183, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.681357: step 6200, loss 0.163537, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.683221: step 6200, loss 0.0555808, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.684722: step 6200, loss 0.112036, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.686140: step 6200, loss 0.211729, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.688049: step 6200, loss 0.0754551, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.690039: step 6200, loss 0.0869694, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.691874: step 6200, loss 4.03592, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.693973: step 6200, loss 0.125, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.695804: step 6200, loss 0.0666817, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.697746: step 6200, loss 0.0691246, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.700089: step 6200, loss 0.473021, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.701984: step 6200, loss 0.123782, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.703939: step 6200, loss 3.76742, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.705953: step 6200, loss 0.109686, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.708180: step 6200, loss 1.50479, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.710303: step 6200, loss 0.196507, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.712446: step 6200, loss 0.0709925, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.714145: step 6200, loss 0.0838089, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.715942: step 6200, loss 0.0512735, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.717851: step 6200, loss 0.296093, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.719574: step 6200, loss 0.119229, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.721321: step 6200, loss 0.250433, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.723228: step 6200, loss 0.07068, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.724781: step 6200, loss 0.0815544, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.728351: step 6200, loss 0.121609, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.730357: step 6200, loss 0.199403, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.732089: step 6200, loss 0.105552, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.733995: step 6200, loss 0.0956039, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.736079: step 6200, loss 3.36388, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.737916: step 6200, loss 0.129175, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.739876: step 6200, loss 0.188655, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.741927: step 6200, loss 0.0632715, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.743504: step 6200, loss 0.126061, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.745531: step 6200, loss 0.129912, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.747669: step 6200, loss 0.230881, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.749316: step 6200, loss 0.141199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.751348: step 6200, loss 0.0882441, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.753250: step 6200, loss 0.0827428, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.755089: step 6200, loss 0.0885196, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.756935: step 6200, loss 0.080073, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.759042: step 6200, loss 0.155427, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.761080: step 6200, loss 0.0574493, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.762816: step 6200, loss 0.0875218, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.764520: step 6200, loss 0.142715, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.766183: step 6200, loss 0.1674, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.767673: step 6200, loss 0.0837987, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.769471: step 6200, loss 0.0872214, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.771309: step 6200, loss 0.0723935, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.772830: step 6200, loss 0.0926532, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.774584: step 6200, loss 1.9007, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.776531: step 6200, loss 0.265192, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.779911: step 6200, loss 2.86022, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.782421: step 6200, loss 0.201994, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.784532: step 6200, loss 0.156335, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.786434: step 6200, loss 2.78868, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.788496: step 6200, loss 0.116555, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.790219: step 6200, loss 0.0671446, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.791929: step 6200, loss 0.114768, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.793426: step 6200, loss 0.0645764, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.795070: step 6200, loss 3.71852, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.796744: step 6200, loss 0.0668163, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.798401: step 6200, loss 0.125252, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.800080: step 6200, loss 0.0989213, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.801936: step 6200, loss 0.132921, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.803702: step 6200, loss 0.124061, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.805584: step 6200, loss 0.0663051, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.807123: step 6200, loss 0.0926277, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.808860: step 6200, loss 0.0939417, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.810248: step 6200, loss 0.199998, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.811947: step 6200, loss 0.222707, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.813525: step 6200, loss 0.0827603, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.815309: step 6200, loss 0.103931, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.816941: step 6200, loss 0.0921909, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.818751: step 6200, loss 0.0863819, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.820379: step 6200, loss 0.0900021, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.822077: step 6200, loss 0.131735, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.823854: step 6200, loss 0.0997423, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.825786: step 6200, loss 0.0767627, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.828043: step 6200, loss 0.0617243, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.830864: step 6200, loss 0.114832, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.832615: step 6200, loss 0.173353, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.834252: step 6200, loss 0.169392, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.836232: step 6200, loss 0.208723, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.838054: step 6200, loss 0.149982, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.839931: step 6200, loss 0.0607788, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.841480: step 6200, loss 0.127655, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.843135: step 6200, loss 0.220544, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.844927: step 6200, loss 0.159276, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.846448: step 6200, loss 0.0978598, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.847921: step 6200, loss 0.0847404, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.849441: step 6200, loss 0.10172, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.851116: step 6200, loss 0.109903, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.852485: step 6200, loss 0.109306, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.854160: step 6200, loss 0.0922633, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.855864: step 6200, loss 0.331405, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.857626: step 6200, loss 0.127335, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.859037: step 6200, loss 0.130826, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.861241: step 6200, loss 0.0885475, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.863221: step 6200, loss 0.208723, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.864967: step 6200, loss 0.112281, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.867021: step 6200, loss 0.0949941, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.869086: step 6200, loss 0.0877219, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.871385: step 6200, loss 2.83081, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.873518: step 6200, loss 2.77158, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.875693: step 6200, loss 0.0577778, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.877458: step 6200, loss 0.136311, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.879706: step 6200, loss 0.0637335, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.883661: step 6200, loss 0.225625, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.885244: step 6200, loss 0.205769, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.886981: step 6200, loss 0.112925, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.888466: step 6200, loss 0.156657, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.890217: step 6200, loss 0.19932, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.891845: step 6200, loss 0.108991, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.893332: step 6200, loss 0.0873182, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.895264: step 6200, loss 0.0590704, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.897361: step 6200, loss 0.0854879, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.899038: step 6200, loss 0.0734663, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.900553: step 6200, loss 0.186958, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.902094: step 6200, loss 0.118656, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.903836: step 6200, loss 0.117221, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.905467: step 6200, loss 0.0722625, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.907499: step 6200, loss 0.0648537, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.909576: step 6200, loss 0.150882, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.911551: step 6200, loss 2.44378, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.913430: step 6200, loss 2.80324, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.915578: step 6200, loss 0.0875184, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.917740: step 6200, loss 1.64702, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.919975: step 6200, loss 0.12367, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.922045: step 6200, loss 0.0682685, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.924234: step 6200, loss 0.108404, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.926071: step 6200, loss 0.147185, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.928080: step 6200, loss 0.104984, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.929942: step 6200, loss 0.104646, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.931965: step 6200, loss 0.124551, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.936111: step 6200, loss 0.262651, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.938327: step 6200, loss 0.0964377, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.940424: step 6200, loss 0.284589, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.942367: step 6200, loss 0.131734, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.944099: step 6200, loss 0.12175, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.945854: step 6200, loss 0.157263, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.947653: step 6200, loss 3.43584, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.949638: step 6200, loss 3.13622, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.951682: step 6200, loss 0.0752347, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.953653: step 6200, loss 3.323, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.955533: step 6200, loss 0.168925, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.957294: step 6200, loss 0.121273, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.959182: step 6200, loss 0.103033, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.961104: step 6200, loss 0.112154, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.963306: step 6200, loss 0.0739769, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.964982: step 6200, loss 0.0611966, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.966674: step 6200, loss 0.140715, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.968591: step 6200, loss 2.62229, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:40.970423: step 6200, loss 0.100814, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.972295: step 6200, loss 0.0952433, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.974144: step 6200, loss 0.0835969, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.975894: step 6200, loss 0.0720244, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.977729: step 6200, loss 0.0743039, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.979677: step 6200, loss 0.117783, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.981777: step 6200, loss 0.331555, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.983478: step 6200, loss 0.151319, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.987665: step 6200, loss 0.0626573, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.989699: step 6200, loss 0.0755489, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.991643: step 6200, loss 0.0538337, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.993582: step 6200, loss 0.154853, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.995412: step 6200, loss 0.125079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.997344: step 6200, loss 0.170529, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:40.999364: step 6200, loss 0.0725724, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.001785: step 6200, loss 0.0575474, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.003509: step 6200, loss 0.0545801, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.005297: step 6200, loss 0.0797104, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.007074: step 6200, loss 0.0934406, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.008946: step 6200, loss 0.139457, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.010803: step 6200, loss 0.1893, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.012660: step 6200, loss 0.0940835, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.014900: step 6200, loss 0.0545606, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.016685: step 6200, loss 0.144977, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.018282: step 6200, loss 0.117958, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.020070: step 6200, loss 0.0720979, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.021738: step 6200, loss 0.140572, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.024013: step 6200, loss 0.0822529, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.026009: step 6200, loss 0.0773187, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.027678: step 6200, loss 0.231063, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.029593: step 6200, loss 2.60045, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.031508: step 6200, loss 0.0818233, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.033258: step 6200, loss 0.0662834, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.035385: step 6200, loss 2.34687, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.039335: step 6200, loss 0.0842823, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.041046: step 6200, loss 0.125879, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.042411: step 6200, loss 0.0643007, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.044215: step 6200, loss 0.0626238, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.045918: step 6200, loss 2.59057, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.047560: step 6200, loss 0.107842, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.049596: step 6200, loss 0.0958399, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.051698: step 6200, loss 0.0888705, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.053208: step 6200, loss 0.1031, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.054918: step 6200, loss 0.125079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.056692: step 6200, loss 2.53943, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.058719: step 6200, loss 0.076493, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.060369: step 6200, loss 0.120012, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.062129: step 6200, loss 0.113536, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.063827: step 6200, loss 0.0862392, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.065541: step 6200, loss 0.0690795, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.067337: step 6200, loss 0.088221, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.069335: step 6200, loss 3.23682, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.071346: step 6200, loss 0.105, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.073452: step 6200, loss 0.0581422, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.075534: step 6200, loss 3.6486, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.077723: step 6200, loss 0.0693229, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.079970: step 6200, loss 0.0666455, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.082210: step 6200, loss 0.103088, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.084058: step 6200, loss 0.143468, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.085819: step 6200, loss 0.0710061, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.087837: step 6200, loss 0.0657051, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.091857: step 6200, loss 0.116113, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.093782: step 6200, loss 0.104907, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.095549: step 6200, loss 0.11795, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.097547: step 6200, loss 0.215918, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.099400: step 6200, loss 0.133945, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.101387: step 6200, loss 0.354824, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.103481: step 6200, loss 0.0769804, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.105580: step 6200, loss 0.154425, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.107860: step 6200, loss 0.0567943, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.110111: step 6200, loss 0.0806912, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.112036: step 6200, loss 0.100321, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.113980: step 6200, loss 3.35545, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.115667: step 6200, loss 2.90327, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.117697: step 6200, loss 0.122093, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.119519: step 6200, loss 0.0870144, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.121235: step 6200, loss 0.0999864, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.123047: step 6200, loss 0.117221, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.125038: step 6200, loss 0.0676545, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.127246: step 6200, loss 0.0970043, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.129298: step 6200, loss 2.82339, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.131205: step 6200, loss 0.104713, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.132950: step 6200, loss 0.13949, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.134816: step 6200, loss 0.0917146, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.136366: step 6200, loss 0.100037, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.138245: step 6200, loss 0.145589, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.139925: step 6200, loss 0.103491, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.142596: step 6200, loss 2.96509, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.144487: step 6200, loss 0.0841077, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.146250: step 6200, loss 0.0824046, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.148157: step 6200, loss 0.0941701, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.149996: step 6200, loss 0.101873, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.151734: step 6200, loss 0.143497, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.153454: step 6200, loss 0.0805825, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.155190: step 6200, loss 0.317876, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.156934: step 6200, loss 0.201642, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.159212: step 6200, loss 0.106382, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.161363: step 6200, loss 0.0961559, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.163645: step 6200, loss 3.32378, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.165713: step 6200, loss 0.171377, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.167976: step 6200, loss 0.103459, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.170163: step 6200, loss 0.181146, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.172117: step 6200, loss 0.0701037, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.174455: step 6200, loss 0.0966703, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.176482: step 6200, loss 0.401684, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.178490: step 6200, loss 0.171308, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.180495: step 6200, loss 3.06137, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.182470: step 6200, loss 0.204848, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.184577: step 6200, loss 0.188777, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.186444: step 6200, loss 3.1418, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.188095: step 6200, loss 0.1219, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.189889: step 6200, loss 0.156335, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.191889: step 6200, loss 0.0768406, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.195281: step 6200, loss 0.0823464, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.197483: step 6200, loss 0.208515, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.199722: step 6200, loss 0.0838833, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.201638: step 6200, loss 2.38728, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.203718: step 6200, loss 0.136103, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.205915: step 6200, loss 0.0857384, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.207882: step 6200, loss 1.75171, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.209849: step 6200, loss 0.174881, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.211741: step 6200, loss 0.0670325, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.213778: step 6200, loss 0.137201, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.215700: step 6200, loss 0.10136, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.217717: step 6200, loss 0.0790513, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.219670: step 6200, loss 0.055402, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.221735: step 6200, loss 0.248441, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.223798: step 6200, loss 3.39025, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.225732: step 6200, loss 0.0705617, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.227649: step 6200, loss 0.136762, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.229418: step 6200, loss 0.1294, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.231175: step 6200, loss 0.135706, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.233042: step 6200, loss 0.108055, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.235046: step 6200, loss 0.0732396, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.236988: step 6200, loss 1.5086, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.238759: step 6200, loss 0.165668, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.240527: step 6200, loss 0.134685, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.242501: step 6200, loss 0.0606125, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.244426: step 6200, loss 0.0818317, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.247117: step 6200, loss 0.11764, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.249563: step 6200, loss 0.183903, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.251732: step 6200, loss 0.123781, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.253897: step 6200, loss 0.101233, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.256111: step 6200, loss 0.118608, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.258312: step 6200, loss 0.18525, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.260253: step 6200, loss 0.0568266, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.262446: step 6200, loss 0.122514, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.264527: step 6200, loss 2.37492, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.266519: step 6200, loss 0.176563, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.268497: step 6200, loss 0.110834, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.270360: step 6200, loss 0.134641, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.272253: step 6200, loss 0.113037, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.274340: step 6200, loss 0.270497, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.276036: step 6200, loss 0.0682061, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.278181: step 6200, loss 0.209325, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.280483: step 6200, loss 0.104683, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.282457: step 6200, loss 0.108939, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.284580: step 6200, loss 0.300581, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.286734: step 6200, loss 0.168423, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.288680: step 6200, loss 0.0669402, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.290688: step 6200, loss 0.0924492, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.292627: step 6200, loss 0.101029, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.294652: step 6200, loss 0.154796, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.296696: step 6200, loss 0.148724, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.300923: step 6200, loss 0.0593339, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.303916: step 6200, loss 0.464966, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.306779: step 6200, loss 0.0962984, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.308983: step 6200, loss 2.87584, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.311418: step 6200, loss 0.0694342, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.313665: step 6200, loss 0.201939, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.315978: step 6200, loss 0.0854941, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.318175: step 6200, loss 0.118571, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.320382: step 6200, loss 0.087731, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.322655: step 6200, loss 0.120963, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.324851: step 6200, loss 0.112588, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.326952: step 6200, loss 0.124103, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.329325: step 6200, loss 0.0712776, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.331522: step 6200, loss 0.0704464, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.333574: step 6200, loss 3.8661, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.335610: step 6200, loss 0.0707243, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.337911: step 6200, loss 0.20408, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.340183: step 6200, loss 0.0524996, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.342388: step 6200, loss 0.192195, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.344433: step 6200, loss 0.0981359, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.346425: step 6200, loss 0.0516955, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.348287: step 6200, loss 0.0944162, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.351544: step 6200, loss 0.200199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.353669: step 6200, loss 0.170868, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.355954: step 6200, loss 0.0942892, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.358243: step 6200, loss 0.139415, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.360354: step 6200, loss 0.16275, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.362204: step 6200, loss 2.93505, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.364358: step 6200, loss 0.0640637, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.366364: step 6200, loss 0.123455, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.368209: step 6200, loss 0.187595, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.370264: step 6200, loss 0.208723, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.372391: step 6200, loss 0.176509, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.374644: step 6200, loss 2.77607, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.376878: step 6200, loss 0.0764496, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.378964: step 6200, loss 1.48352, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.380989: step 6200, loss 0.0655012, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.382999: step 6200, loss 0.0972535, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.385257: step 6200, loss 0.14196, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.387244: step 6200, loss 0.0874519, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.389424: step 6200, loss 0.115039, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.391480: step 6200, loss 0.28167, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.393798: step 6200, loss 0.151601, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.395837: step 6200, loss 0.078279, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.397794: step 6200, loss 0.101987, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.399709: step 6200, loss 0.0848645, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.404139: step 6200, loss 0.139112, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.406175: step 6200, loss 0.141199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.408110: step 6200, loss 0.272332, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.409986: step 6200, loss 0.114196, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.411994: step 6200, loss 0.12053, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.413765: step 6200, loss 0.0670992, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.415794: step 6200, loss 0.110903, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.417886: step 6200, loss 0.069345, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.420113: step 6200, loss 0.0798725, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.422115: step 6200, loss 0.226231, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.424183: step 6200, loss 0.0771187, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.426312: step 6200, loss 0.103602, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.428247: step 6200, loss 0.649832, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.430176: step 6200, loss 0.152742, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.432252: step 6200, loss 0.063549, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.434362: step 6200, loss 2.25753, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.436537: step 6200, loss 0.0887052, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.439129: step 6200, loss 0.183307, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.441495: step 6200, loss 0.0799662, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.443671: step 6200, loss 0.113896, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.445932: step 6200, loss 0.100439, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.448074: step 6200, loss 0.092717, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.449993: step 6200, loss 0.10844, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.452261: step 6200, loss 0.0678343, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.455284: step 6200, loss 0.213384, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.457898: step 6200, loss 0.138632, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.460126: step 6200, loss 0.144159, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.462387: step 6200, loss 0.125218, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.465164: step 6200, loss 0.110453, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.467570: step 6200, loss 0.0975105, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.469813: step 6200, loss 2.26858, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.472240: step 6200, loss 0.310262, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.474284: step 6200, loss 0.10556, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.476379: step 6200, loss 0.211517, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.478438: step 6200, loss 0.148387, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.480396: step 6200, loss 0.0654227, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.482305: step 6200, loss 0.1418, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.484363: step 6200, loss 0.108907, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.486442: step 6200, loss 0.117221, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.488424: step 6200, loss 0.201645, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.490433: step 6200, loss 0.161914, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.492497: step 6200, loss 0.121905, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.494541: step 6200, loss 0.07372, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.496721: step 6200, loss 0.0959475, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.498899: step 6200, loss 0.0517701, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.501027: step 6200, loss 0.101136, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.502925: step 6200, loss 0.0772252, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.507387: step 6200, loss 0.0863224, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.509767: step 6200, loss 0.0747013, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.511998: step 6200, loss 0.177266, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.514115: step 6200, loss 0.210964, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.516020: step 6200, loss 2.98582, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.518081: step 6200, loss 0.112624, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.520169: step 6200, loss 0.0980452, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.522272: step 6200, loss 3.92237, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.524284: step 6200, loss 0.173777, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.526326: step 6200, loss 3.08387, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.528265: step 6200, loss 0.275722, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.530416: step 6200, loss 3.42298, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.532477: step 6200, loss 2.04794, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.534543: step 6200, loss 0.192844, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.536335: step 6200, loss 0.155644, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.538448: step 6200, loss 0.10541, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.540680: step 6200, loss 0.14174, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.542867: step 6200, loss 0.316465, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.544860: step 6200, loss 0.216305, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.546878: step 6200, loss 0.103, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.549050: step 6200, loss 0.109959, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.551165: step 6200, loss 3.52035, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.553195: step 6200, loss 0.107972, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.555043: step 6200, loss 0.115457, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.556979: step 6200, loss 0.14796, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.558986: step 6200, loss 0.114896, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.562695: step 6200, loss 0.244852, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.564681: step 6200, loss 3.62762, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.566716: step 6200, loss 0.112376, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.568872: step 6200, loss 0.0745279, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.570949: step 6200, loss 0.0784006, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.572816: step 6200, loss 0.0663547, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.574947: step 6200, loss 0.0657276, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.577345: step 6200, loss 0.156348, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.579538: step 6200, loss 0.150352, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.581668: step 6200, loss 0.0653189, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.583721: step 6200, loss 0.0669831, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.585740: step 6200, loss 0.0721916, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.587931: step 6200, loss 3.34235, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.589970: step 6200, loss 0.0632067, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.591966: step 6200, loss 0.0728872, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.594015: step 6200, loss 0.0545038, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.597273: step 6200, loss 0.16117, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.599670: step 6200, loss 0.317876, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.601820: step 6200, loss 3.22549, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.603795: step 6200, loss 0.142633, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.605899: step 6200, loss 0.103502, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.607871: step 6200, loss 0.12122, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.609914: step 6200, loss 0.201131, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.614059: step 6200, loss 0.0694276, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.616079: step 6200, loss 1.95172, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.618094: step 6200, loss 0.0792095, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.620167: step 6200, loss 0.0762431, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.622297: step 6200, loss 0.0715899, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.624249: step 6200, loss 0.181069, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.626270: step 6200, loss 1.9558, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.628036: step 6200, loss 0.0933111, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.629661: step 6200, loss 0.241269, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.631461: step 6200, loss 2.85509, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.633355: step 6200, loss 0.0893926, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.635211: step 6200, loss 0.143601, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.637146: step 6200, loss 0.183517, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.639232: step 6200, loss 0.0658609, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.641496: step 6200, loss 0.0666067, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.643528: step 6200, loss 0.158831, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.645476: step 6200, loss 0.0577448, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.647687: step 6200, loss 0.771553, acc 0\n",
      "2016-03-08T02:13:41.649744: step 6200, loss 0.0587139, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.651810: step 6200, loss 0.343217, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.654034: step 6200, loss 0.125079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.656077: step 6200, loss 0.139422, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.658105: step 6200, loss 0.0796465, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.660129: step 6200, loss 0.109722, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.662070: step 6200, loss 2.91337, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.665046: step 6200, loss 0.15589, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.666939: step 6200, loss 0.120252, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.668979: step 6200, loss 0.0814914, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.670861: step 6200, loss 0.114547, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.672629: step 6200, loss 4.60997, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.674564: step 6200, loss 0.273426, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.676878: step 6200, loss 0.16024, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.679266: step 6200, loss 0.0827456, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.681171: step 6200, loss 0.0752487, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.683022: step 6200, loss 2.39478, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.685084: step 6200, loss 0.154882, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.686998: step 6200, loss 0.208723, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.689088: step 6200, loss 0.161339, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.691471: step 6200, loss 0.137129, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.693521: step 6200, loss 0.0697327, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.695526: step 6200, loss 0.0618012, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.697778: step 6200, loss 0.0815466, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.699721: step 6200, loss 3.23387, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.701529: step 6200, loss 0.323363, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.703607: step 6200, loss 0.0921748, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.705627: step 6200, loss 0.0711977, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.707669: step 6200, loss 0.0992907, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.709662: step 6200, loss 0.109928, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.711356: step 6200, loss 0.12192, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.713294: step 6200, loss 0.108608, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.717235: step 6200, loss 0.138483, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.718824: step 6200, loss 2.81561, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.720561: step 6200, loss 0.144594, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.722579: step 6200, loss 0.287306, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.724302: step 6200, loss 0.0535697, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.726249: step 6200, loss 0.121633, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.728269: step 6200, loss 0.119957, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.730234: step 6200, loss 0.183143, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.732204: step 6200, loss 0.0990085, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.734715: step 6200, loss 0.114093, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.736835: step 6200, loss 0.191873, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.739100: step 6200, loss 0.118532, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.741137: step 6200, loss 0.107861, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.743042: step 6200, loss 0.0749409, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.745061: step 6200, loss 0.0517933, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.747170: step 6200, loss 0.107044, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.749256: step 6200, loss 0.228308, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.751190: step 6200, loss 0.0723237, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.753248: step 6200, loss 0.113004, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.755165: step 6200, loss 0.0963877, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.757166: step 6200, loss 0.051394, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.759142: step 6200, loss 0.399828, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.761372: step 6200, loss 0.0949222, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.763516: step 6200, loss 0.0594046, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.765679: step 6200, loss 0.15354, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.769815: step 6200, loss 0.0909478, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.772172: step 6200, loss 0.112008, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.774117: step 6200, loss 0.0702512, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.775856: step 6200, loss 0.115893, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.777751: step 6200, loss 2.3733, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.779745: step 6200, loss 0.0795451, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.781510: step 6200, loss 2.43493, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.783424: step 6200, loss 0.0660583, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.785380: step 6200, loss 0.355844, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.787359: step 6200, loss 0.094941, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.789338: step 6200, loss 0.226231, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.791176: step 6200, loss 0.0553639, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.793132: step 6200, loss 0.0969654, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.795096: step 6200, loss 0.102842, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.797366: step 6200, loss 0.180617, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.799700: step 6200, loss 0.0967707, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.801693: step 6200, loss 0.0718109, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.803786: step 6200, loss 0.0786293, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.805612: step 6200, loss 0.0861734, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.807891: step 6200, loss 0.0987881, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.809974: step 6200, loss 0.104053, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.811852: step 6200, loss 0.0636442, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.813803: step 6200, loss 0.233534, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.815742: step 6200, loss 0.14982, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.817660: step 6200, loss 2.9907, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.821656: step 6200, loss 0.133148, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.823956: step 6200, loss 0.354824, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.825978: step 6200, loss 2.78897, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.827992: step 6200, loss 0.0705498, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.830133: step 6200, loss 0.135852, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.832125: step 6200, loss 0.172435, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.834189: step 6200, loss 0.0997135, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.836216: step 6200, loss 0.12934, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.838248: step 6200, loss 0.130055, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.840208: step 6200, loss 0.0903446, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.842243: step 6200, loss 0.10439, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.844400: step 6200, loss 0.200921, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.846391: step 6200, loss 0.167528, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.848001: step 6200, loss 0.0766984, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.849980: step 6200, loss 0.0718962, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.851764: step 6200, loss 0.0727128, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.853727: step 6200, loss 0.143634, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.855713: step 6200, loss 0.0951884, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.857785: step 6200, loss 0.332073, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.859768: step 6200, loss 0.109991, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.861902: step 6200, loss 0.0867301, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.863922: step 6200, loss 0.0532255, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.865562: step 6200, loss 0.243326, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.867462: step 6200, loss 0.114218, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.869713: step 6200, loss 0.128456, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.873735: step 6200, loss 0.0773734, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.875700: step 6200, loss 3.96759, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.877549: step 6200, loss 0.0845869, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.879489: step 6200, loss 0.111078, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.881467: step 6200, loss 0.0590705, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.883433: step 6200, loss 0.0792175, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.885600: step 6200, loss 0.342083, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.887569: step 6200, loss 0.0881348, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.889623: step 6200, loss 0.141199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.891755: step 6200, loss 0.082107, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.893252: step 6200, loss 0.0569636, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.895010: step 6200, loss 0.103354, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.896741: step 6200, loss 2.53141, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.898904: step 6200, loss 0.167752, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.900934: step 6200, loss 0.23032, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.902786: step 6200, loss 0.112326, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.904839: step 6200, loss 0.12838, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.906838: step 6200, loss 0.089679, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.908836: step 6200, loss 0.07301, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.911013: step 6200, loss 0.0912246, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.912887: step 6200, loss 0.0808431, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.914847: step 6200, loss 0.101136, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.916777: step 6200, loss 2.80979, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.918925: step 6200, loss 0.153531, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.920613: step 6200, loss 0.131839, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.924717: step 6200, loss 0.105282, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.926928: step 6200, loss 0.0550726, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.928765: step 6200, loss 0.178964, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.930712: step 6200, loss 0.192409, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.932908: step 6200, loss 0.107673, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.935163: step 6200, loss 3.65553, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.937412: step 6200, loss 0.0675976, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.939586: step 6200, loss 0.0694633, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.941568: step 6200, loss 0.141199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.943683: step 6200, loss 0.097117, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.945815: step 6200, loss 0.0850432, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.948001: step 6200, loss 0.105141, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.950235: step 6200, loss 0.166447, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.952377: step 6200, loss 0.124336, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.954516: step 6200, loss 0.0860295, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.956561: step 6200, loss 2.73084, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.958713: step 6200, loss 0.170452, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.960842: step 6200, loss 0.117221, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.962928: step 6200, loss 0.0979734, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.964879: step 6200, loss 0.0708028, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.966659: step 6200, loss 0.318327, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.968728: step 6200, loss 0.128701, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.971089: step 6200, loss 0.181555, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.973388: step 6200, loss 0.0774044, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.976834: step 6200, loss 0.154955, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.979045: step 6200, loss 0.11389, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.981381: step 6200, loss 2.89439, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:41.983494: step 6200, loss 0.149559, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.985524: step 6200, loss 0.0821878, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.987672: step 6200, loss 0.169506, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.989849: step 6200, loss 0.0776765, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.992082: step 6200, loss 0.118778, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.993862: step 6200, loss 0.109164, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.995831: step 6200, loss 0.0857434, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.997994: step 6200, loss 0.0923428, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:41.999826: step 6200, loss 0.167891, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.001942: step 6200, loss 0.116639, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.004438: step 6200, loss 0.1102, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.006237: step 6200, loss 0.11693, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.008449: step 6200, loss 0.219862, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.010697: step 6200, loss 0.0962111, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.012902: step 6200, loss 0.089213, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.014919: step 6200, loss 0.0855862, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.017037: step 6200, loss 0.215605, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.019125: step 6200, loss 0.0766666, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.020905: step 6200, loss 0.0955504, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.023100: step 6200, loss 0.0760075, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.024982: step 6200, loss 0.0682734, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.029077: step 6200, loss 0.101826, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.031071: step 6200, loss 2.50991, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.033352: step 6200, loss 0.0976391, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.035435: step 6200, loss 0.0841479, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.037489: step 6200, loss 0.0672452, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.039498: step 6200, loss 0.13783, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.041467: step 6200, loss 0.115676, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.043649: step 6200, loss 0.100141, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.045799: step 6200, loss 0.101523, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.047820: step 6200, loss 0.165706, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.049409: step 6200, loss 3.55104, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.051107: step 6200, loss 0.15184, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.053155: step 6200, loss 0.143264, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.054846: step 6200, loss 2.55011, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.056573: step 6200, loss 0.127836, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.058177: step 6200, loss 0.0798913, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.059834: step 6200, loss 0.0691884, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.061564: step 6200, loss 0.0973364, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.063179: step 6200, loss 0.249036, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.065217: step 6200, loss 0.0752615, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.067439: step 6200, loss 0.0830879, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.069707: step 6200, loss 0.138174, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.071756: step 6200, loss 0.152592, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.073607: step 6200, loss 0.183517, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.075711: step 6200, loss 0.105701, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.079953: step 6200, loss 0.0535666, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.082136: step 6200, loss 0.0857921, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.084271: step 6200, loss 0.0712703, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.086125: step 6200, loss 2.53784, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.088041: step 6200, loss 0.102484, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.090368: step 6200, loss 0.414985, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.092400: step 6200, loss 0.126909, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.094294: step 6200, loss 0.129384, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.095935: step 6200, loss 0.0521312, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.097849: step 6200, loss 0.0606128, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.099448: step 6200, loss 0.197933, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.101123: step 6200, loss 0.0733923, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.102842: step 6200, loss 0.136169, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.104511: step 6200, loss 0.10471, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.106166: step 6200, loss 0.230247, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.107863: step 6200, loss 0.106351, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.109550: step 6200, loss 0.101136, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.111312: step 6200, loss 0.100694, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.113030: step 6200, loss 0.159031, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.114587: step 6200, loss 0.0497965, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.116163: step 6200, loss 0.432978, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.117723: step 6200, loss 0.175473, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.119394: step 6200, loss 0.0667805, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.120949: step 6200, loss 0.147922, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.122726: step 6200, loss 3.06737, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.124388: step 6200, loss 0.155353, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.126174: step 6200, loss 0.197876, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.128107: step 6200, loss 0.096159, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.131009: step 6200, loss 0.17411, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.132983: step 6200, loss 0.125964, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.134892: step 6200, loss 0.073232, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.136956: step 6200, loss 0.0605302, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.139214: step 6200, loss 0.337949, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.141125: step 6200, loss 0.0708567, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.143240: step 6200, loss 0.105324, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.145258: step 6200, loss 0.317876, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.147295: step 6200, loss 0.134247, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.148931: step 6200, loss 2.18266, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.150544: step 6200, loss 2.02311, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.152206: step 6200, loss 3.14503, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.154074: step 6200, loss 0.0992031, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.156140: step 6200, loss 0.069516, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.158612: step 6200, loss 0.0665572, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.160988: step 6200, loss 0.0776226, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.163136: step 6200, loss 0.087773, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.165240: step 6200, loss 0.136051, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.167065: step 6200, loss 1.60047, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.168929: step 6200, loss 0.0677798, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.170873: step 6200, loss 0.176865, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.172683: step 6200, loss 0.122413, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.174804: step 6200, loss 0.0842245, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.176700: step 6200, loss 0.15167, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.178548: step 6200, loss 0.0791446, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.180466: step 6200, loss 0.087287, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.184628: step 6200, loss 4.16408, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.186503: step 6200, loss 0.14882, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.188413: step 6200, loss 0.0845334, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.190083: step 6200, loss 0.193529, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.192391: step 6200, loss 0.0979199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.194245: step 6200, loss 0.162133, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.196198: step 6200, loss 0.118389, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.197986: step 6200, loss 0.136458, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.199852: step 6200, loss 0.0998315, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.201605: step 6200, loss 0.154041, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.203420: step 6200, loss 0.230571, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.205196: step 6200, loss 2.85908, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.206756: step 6200, loss 0.113697, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.208415: step 6200, loss 2.21634, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.210015: step 6200, loss 0.0885766, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.211673: step 6200, loss 0.0993895, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.213355: step 6200, loss 0.137327, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.215167: step 6200, loss 0.134118, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.216655: step 6200, loss 0.122012, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.218326: step 6200, loss 0.100505, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.219916: step 6200, loss 0.132532, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.221654: step 6200, loss 0.149846, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.223192: step 6200, loss 0.0762658, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.224972: step 6200, loss 0.128723, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.226648: step 6200, loss 0.242154, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.228356: step 6200, loss 0.264153, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.229902: step 6200, loss 0.0789512, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.231513: step 6200, loss 0.121564, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.234694: step 6200, loss 0.0772128, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.236865: step 6200, loss 0.0795583, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.238853: step 6200, loss 0.064734, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.240608: step 6200, loss 0.106028, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.242713: step 6200, loss 0.0924641, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.244786: step 6200, loss 0.136861, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.246721: step 6200, loss 0.0867741, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.248636: step 6200, loss 0.11626, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.250532: step 6200, loss 0.235192, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.252535: step 6200, loss 0.156844, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.254753: step 6200, loss 0.0944696, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.256700: step 6200, loss 0.151569, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.258459: step 6200, loss 0.0937167, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.260181: step 6200, loss 0.113724, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.261793: step 6200, loss 0.0950436, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.263720: step 6200, loss 0.069366, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.265542: step 6200, loss 0.242543, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.267297: step 6200, loss 0.136433, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.269259: step 6200, loss 3.21875, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.271404: step 6200, loss 0.0763791, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.273741: step 6200, loss 0.208723, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.275517: step 6200, loss 0.181555, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.277028: step 6200, loss 0.0947172, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.278878: step 6200, loss 0.0905725, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.280524: step 6200, loss 0.115945, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.282380: step 6200, loss 0.108027, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.285540: step 6200, loss 0.138392, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.287441: step 6200, loss 0.108006, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.289339: step 6200, loss 0.105216, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.291388: step 6200, loss 0.0589765, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.293380: step 6200, loss 0.107545, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.295314: step 6200, loss 0.0939648, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.297027: step 6200, loss 0.319717, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.299108: step 6200, loss 0.123791, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.301110: step 6200, loss 0.134517, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.303255: step 6200, loss 0.0861135, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.305279: step 6200, loss 0.190357, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.307085: step 6200, loss 0.0755108, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.308906: step 6200, loss 0.113541, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.310623: step 6200, loss 0.0821802, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.312295: step 6200, loss 0.0887819, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.314061: step 6200, loss 0.064881, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.315732: step 6200, loss 0.316634, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.317206: step 6200, loss 0.193667, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.318955: step 6200, loss 2.23404, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.320383: step 6200, loss 0.0983775, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.322078: step 6200, loss 0.0722044, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.323628: step 6200, loss 0.0916563, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.325597: step 6200, loss 0.0734611, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.328032: step 6200, loss 3.02491, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.330232: step 6200, loss 0.0867018, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.332611: step 6200, loss 0.127015, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.334738: step 6200, loss 0.0452701, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.337814: step 6200, loss 0.138251, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.339655: step 6200, loss 0.147877, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.341680: step 6200, loss 0.119438, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.343855: step 6200, loss 0.0622881, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.345609: step 6200, loss 0.0942096, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.347235: step 6200, loss 0.135, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.348937: step 6200, loss 0.113596, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.350752: step 6200, loss 0.0691408, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.352389: step 6200, loss 2.60184, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.354066: step 6200, loss 0.0650039, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.355953: step 6200, loss 0.104355, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.357831: step 6200, loss 0.115953, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.359368: step 6200, loss 0.0978713, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.361103: step 6200, loss 0.17339, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.362774: step 6200, loss 0.123993, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.364667: step 6200, loss 0.0879186, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.366551: step 6200, loss 3.84989, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.368293: step 6200, loss 3.46857, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.370242: step 6200, loss 0.240142, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.372003: step 6200, loss 0.0730475, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.374212: step 6200, loss 0.0848095, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.375940: step 6200, loss 0.0972612, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.377700: step 6200, loss 0.123524, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.379952: step 6200, loss 0.0601698, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.382254: step 6200, loss 0.0697586, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.384185: step 6200, loss 0.175841, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.386161: step 6200, loss 0.0817823, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.389618: step 6200, loss 0.0456484, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.391546: step 6200, loss 0.0818881, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.393527: step 6200, loss 0.0746183, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.395139: step 6200, loss 0.233487, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.396756: step 6200, loss 0.0795616, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.398596: step 6200, loss 0.0779925, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.400240: step 6200, loss 0.0621816, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.401939: step 6200, loss 0.0668391, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.404003: step 6200, loss 0.180494, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.405914: step 6200, loss 0.11025, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.407497: step 6200, loss 0.109306, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.409468: step 6200, loss 0.0877882, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.411227: step 6200, loss 0.192103, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.412854: step 6200, loss 0.121254, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.414761: step 6200, loss 0.08788, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.416541: step 6200, loss 0.115424, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.418589: step 6200, loss 0.125079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.420559: step 6200, loss 0.133777, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.422452: step 6200, loss 0.0983542, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.424488: step 6200, loss 0.0817988, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.426152: step 6200, loss 0.183356, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.427802: step 6200, loss 0.0781756, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.429633: step 6200, loss 0.0575344, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.431710: step 6200, loss 0.173618, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.433805: step 6200, loss 0.128229, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.435713: step 6200, loss 0.161864, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.438024: step 6200, loss 0.079247, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.440926: step 6200, loss 0.10707, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.442768: step 6200, loss 0.0836636, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.445039: step 6200, loss 0.104124, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.446655: step 6200, loss 0.0961523, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.448310: step 6200, loss 0.0754006, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.449854: step 6200, loss 0.0847445, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.451880: step 6200, loss 0.11934, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.453498: step 6200, loss 0.200799, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.454975: step 6200, loss 0.110366, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.456644: step 6200, loss 0.115847, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.458257: step 6200, loss 0.0808779, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.460348: step 6200, loss 0.14734, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.462099: step 6200, loss 1.94721, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.463987: step 6200, loss 0.166313, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.465742: step 6200, loss 0.294482, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.467405: step 6200, loss 0.114553, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.469141: step 6200, loss 3.01855, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.471170: step 6200, loss 0.101008, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.473071: step 6200, loss 0.0900941, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.474931: step 6200, loss 2.12661, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.476999: step 6200, loss 0.0854719, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.479029: step 6200, loss 0.188845, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.481081: step 6200, loss 0.0826691, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.483145: step 6200, loss 0.144433, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.484868: step 6200, loss 0.291, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.486829: step 6200, loss 0.105931, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.488547: step 6200, loss 3.61922, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.490230: step 6200, loss 0.0933792, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.493673: step 6200, loss 0.0659428, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.495168: step 6200, loss 0.125079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.497244: step 6200, loss 0.0926647, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.499151: step 6200, loss 0.0827304, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.500734: step 6200, loss 0.106771, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.502470: step 6200, loss 0.308484, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.504437: step 6200, loss 0.118006, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.506594: step 6200, loss 0.0898543, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.508594: step 6200, loss 0.0823311, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.510561: step 6200, loss 2.84425, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.512402: step 6200, loss 0.108656, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.514165: step 6200, loss 0.151892, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.515941: step 6200, loss 0.126946, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.517767: step 6200, loss 0.0559358, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.519500: step 6200, loss 0.200612, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.521409: step 6200, loss 0.0865305, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.523452: step 6200, loss 0.101984, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.525649: step 6200, loss 1.95776, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.527410: step 6200, loss 2.82083, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.529405: step 6200, loss 1.86858, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.531669: step 6200, loss 3.76651, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.533585: step 6200, loss 0.0810719, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.535311: step 6200, loss 3.20651, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.537449: step 6200, loss 2.00907, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.539413: step 6200, loss 0.169032, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.541182: step 6200, loss 0.115367, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.544219: step 6200, loss 3.32601, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.546063: step 6200, loss 0.0604372, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.547716: step 6200, loss 0.122201, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.549531: step 6200, loss 0.100189, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.551488: step 6200, loss 0.0755727, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.553370: step 6200, loss 0.0561452, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.555331: step 6200, loss 0.260066, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.557133: step 6200, loss 0.0942225, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.558923: step 6200, loss 0.160685, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.560457: step 6200, loss 0.138142, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.562204: step 6200, loss 0.0709896, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.563836: step 6200, loss 0.0963199, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.565494: step 6200, loss 0.326295, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.567283: step 6200, loss 0.265192, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.569017: step 6200, loss 0.0849096, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.571046: step 6200, loss 2.60316, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.572934: step 6200, loss 0.134001, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.574747: step 6200, loss 0.0741171, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.576875: step 6200, loss 0.0747757, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.578815: step 6200, loss 0.063264, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.580651: step 6200, loss 0.156453, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.582882: step 6200, loss 0.0600669, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.585125: step 6200, loss 0.0883144, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.587360: step 6200, loss 0.0954393, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.589432: step 6200, loss 0.124626, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.591704: step 6200, loss 0.214343, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.593679: step 6200, loss 0.0994732, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.596253: step 6200, loss 0.107213, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.598647: step 6200, loss 0.0944223, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.600727: step 6200, loss 0.117079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.603019: step 6200, loss 0.0990062, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.605330: step 6200, loss 0.101861, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.607486: step 6200, loss 0.144159, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.609564: step 6200, loss 0.0898734, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.611540: step 6200, loss 0.0942592, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.613580: step 6200, loss 0.125079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.615598: step 6200, loss 0.093019, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.617725: step 6200, loss 2.12337, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.619891: step 6200, loss 0.111611, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.621939: step 6200, loss 0.072352, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.623904: step 6200, loss 0.083139, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.625930: step 6200, loss 0.108063, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.627944: step 6200, loss 0.238927, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.630099: step 6200, loss 0.19496, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.632256: step 6200, loss 4.28883, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.634714: step 6200, loss 0.115209, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.636581: step 6200, loss 0.0975845, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.638542: step 6200, loss 0.093223, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.640635: step 6200, loss 0.119779, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.642506: step 6200, loss 0.0870126, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.644571: step 6200, loss 0.0898839, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.648517: step 6200, loss 0.113082, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.650536: step 6200, loss 0.108619, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.652465: step 6200, loss 0.305751, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.654757: step 6200, loss 0.0875246, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.656854: step 6200, loss 0.114384, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.658807: step 6200, loss 2.14616, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.660779: step 6200, loss 0.074587, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.662823: step 6200, loss 0.0794363, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.664876: step 6200, loss 0.15056, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.666769: step 6200, loss 0.075946, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.668775: step 6200, loss 0.0860081, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.670781: step 6200, loss 0.105427, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.673030: step 6200, loss 0.115827, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.675056: step 6200, loss 0.137417, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.676952: step 6200, loss 0.116015, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.679250: step 6200, loss 0.40046, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.681478: step 6200, loss 1.90204, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.683508: step 6200, loss 0.128969, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.685645: step 6200, loss 0.262761, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.687794: step 6200, loss 0.0889488, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.689749: step 6200, loss 0.174818, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.691986: step 6200, loss 0.0999239, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.694131: step 6200, loss 0.118791, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.696284: step 6200, loss 0.0617228, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.701052: step 6200, loss 3.1668, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.705005: step 6200, loss 2.8499, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.707747: step 6200, loss 0.096146, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.710199: step 6200, loss 0.0939256, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.712384: step 6200, loss 0.128192, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.714732: step 6200, loss 0.150378, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.717268: step 6200, loss 3.42441, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.719612: step 6200, loss 0.129504, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.721859: step 6200, loss 0.169497, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.724136: step 6200, loss 0.109306, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.726431: step 6200, loss 0.120808, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.728679: step 6200, loss 0.0784002, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.730677: step 6200, loss 0.109488, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.732530: step 6200, loss 0.0801201, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.734489: step 6200, loss 0.201235, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.736316: step 6200, loss 0.13447, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.738140: step 6200, loss 0.0993246, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.740140: step 6200, loss 0.0534145, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.742230: step 6200, loss 0.272333, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.744319: step 6200, loss 0.142143, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.746216: step 6200, loss 3.53398, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.748405: step 6200, loss 0.0876224, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.751733: step 6200, loss 0.0693821, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.753928: step 6200, loss 0.11875, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.756097: step 6200, loss 0.0924641, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.758182: step 6200, loss 0.0657946, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.760326: step 6200, loss 0.259291, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.762393: step 6200, loss 2.53943, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.764423: step 6200, loss 0.0975587, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.766448: step 6200, loss 0.100605, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.768287: step 6200, loss 0.156628, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.770125: step 6200, loss 0.0843753, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.772135: step 6200, loss 0.115463, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.774445: step 6200, loss 0.12308, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.776541: step 6200, loss 0.122825, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.778761: step 6200, loss 0.0907236, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.780958: step 6200, loss 0.0716252, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.783222: step 6200, loss 0.0544967, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.785351: step 6200, loss 0.0561452, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.787248: step 6200, loss 0.255356, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.789266: step 6200, loss 0.122564, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.791458: step 6200, loss 0.107228, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.793493: step 6200, loss 0.0628175, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.795310: step 6200, loss 0.0965292, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.797367: step 6200, loss 0.139189, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.799306: step 6200, loss 2.73518, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.803571: step 6200, loss 2.71939, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.805700: step 6200, loss 0.267262, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.807896: step 6200, loss 0.0802541, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.810199: step 6200, loss 0.0632709, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.812296: step 6200, loss 0.0746865, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.814679: step 6200, loss 0.193073, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.816794: step 6200, loss 0.137054, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.818927: step 6200, loss 0.186622, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.821100: step 6200, loss 0.141673, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.823309: step 6200, loss 0.124433, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.825406: step 6200, loss 2.43493, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.827486: step 6200, loss 0.0887576, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.829625: step 6200, loss 0.101136, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.831700: step 6200, loss 2.2437, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.833885: step 6200, loss 0.0909603, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.836141: step 6200, loss 0.215987, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.838137: step 6200, loss 0.056429, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.840227: step 6200, loss 0.0929407, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.842191: step 6200, loss 0.0582877, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.844245: step 6200, loss 0.0981057, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.846439: step 6200, loss 2.35911, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.848452: step 6200, loss 0.124774, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.850667: step 6200, loss 0.0942821, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.854763: step 6200, loss 0.0809499, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.856910: step 6200, loss 0.136739, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.858870: step 6200, loss 2.14004, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.860881: step 6200, loss 2.74483, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.862835: step 6200, loss 0.104393, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.864947: step 6200, loss 0.0515616, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.867002: step 6200, loss 0.0658465, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.868898: step 6200, loss 0.0980207, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.870726: step 6200, loss 0.208723, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.872769: step 6200, loss 0.0769838, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.874776: step 6200, loss 0.0936698, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.876802: step 6200, loss 2.55842, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.878748: step 6200, loss 4.08486, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.880701: step 6200, loss 0.0601705, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.882482: step 6200, loss 0.125594, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.884446: step 6200, loss 0.144425, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.885946: step 6200, loss 0.153761, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.887642: step 6200, loss 0.120631, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.889594: step 6200, loss 0.0857228, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.891574: step 6200, loss 0.0733256, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.893616: step 6200, loss 3.20011, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.895670: step 6200, loss 1.89481, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.897593: step 6200, loss 0.167828, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.899525: step 6200, loss 0.153046, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.901581: step 6200, loss 3.62171, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.905692: step 6200, loss 3.87944, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.907655: step 6200, loss 0.123849, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.909625: step 6200, loss 0.114645, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.911537: step 6200, loss 0.230251, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.913498: step 6200, loss 0.0764376, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.915193: step 6200, loss 0.0800991, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.917064: step 6200, loss 0.348952, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.918732: step 6200, loss 2.87152, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.920536: step 6200, loss 2.70328, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.922460: step 6200, loss 0.0866084, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.924591: step 6200, loss 0.146818, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.926320: step 6200, loss 0.0822839, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.928359: step 6200, loss 0.0750506, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.930653: step 6200, loss 0.0938083, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.932609: step 6200, loss 0.073617, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.934472: step 6200, loss 0.0747112, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.936156: step 6200, loss 0.222367, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.938091: step 6200, loss 2.77879, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.940035: step 6200, loss 2.8378, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.941945: step 6200, loss 0.333152, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.943763: step 6200, loss 0.0875296, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.946003: step 6200, loss 0.204072, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.948373: step 6200, loss 0.104051, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.950040: step 6200, loss 2.92266, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.951802: step 6200, loss 0.143583, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.953401: step 6200, loss 2.2566, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.957204: step 6200, loss 0.182162, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.959129: step 6200, loss 0.122276, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.961043: step 6200, loss 0.0690565, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.962867: step 6200, loss 3.16768, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.964922: step 6200, loss 0.12279, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.966735: step 6200, loss 0.118439, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.968570: step 6200, loss 2.10663, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:42.970482: step 6200, loss 0.0658889, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.972502: step 6200, loss 0.0831877, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.974399: step 6200, loss 0.119842, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.976232: step 6200, loss 0.0826806, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.978197: step 6200, loss 0.138539, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.980292: step 6200, loss 0.0734325, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.982320: step 6200, loss 0.174698, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.984736: step 6200, loss 0.0849658, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.986646: step 6200, loss 0.0795877, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.988591: step 6200, loss 0.0956403, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.990522: step 6200, loss 0.0923685, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.992201: step 6200, loss 0.28149, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.993922: step 6200, loss 0.0996894, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.995484: step 6200, loss 0.0912465, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.997175: step 6200, loss 0.104628, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:42.999112: step 6200, loss 0.223281, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:43.001035: step 6200, loss 0.157263, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:43.002731: step 6200, loss 0.0845881, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:43.004529: step 6200, loss 0.0934362, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:43.008199: step 6200, loss 2.59655, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:43.010283: step 6200, loss 0.159488, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:43.012183: step 6200, loss 0.200079, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:43.014050: step 6200, loss 0.115306, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:43.015778: step 6200, loss 1.99261, acc 0\n",
      "False Negative\n",
      "2016-03-08T02:13:43.017526: step 6200, loss 0.0607921, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:43.019363: step 6200, loss 0.112086, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:43.021084: step 6200, loss 0.11435, acc 1\n",
      "False Negateive\n",
      "2016-03-08T02:13:43.023075: step 6200, loss 0.0711641, acc 1\n",
      "False Negateive\n",
      "True Positives 0\n",
      "True Negatives 1756\n",
      "False Positives 0\n",
      "False Negatives 219\n",
      "Sensitivity: 0.0\n",
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "    accuracies = []\n",
    "    for y, x in zip(even_y_dev, even_x_dev):\n",
    "        sent = []\n",
    "        for word in x:\n",
    "            sent.append(vocabulary_inv[word])\n",
    "        print(' '.join(sent))\n",
    "        print(\"example\" if y[0] == 0 and y[1] == 1 else \"nonexample\")\n",
    "        dev_step([x], [y])\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "\n",
    "        \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for y, x in zip(y_dev, x_dev):\n",
    "        a= dev_step([x], [y])\n",
    "        expected = \"example\" if y[0] == 0 and y[1] == 1  else \"nonexample\"\n",
    "        actual = None\n",
    "        if(y[0] == 0 and y[1] == 1):\n",
    "            # correct label is example\n",
    "            if(a == 1.0):\n",
    "                actual = \"example\"\n",
    "            else:\n",
    "                actual = \"nonexample\"\n",
    "        elif(y[0] == 1 and y[1] == 0):\n",
    "            if(a == 1.0):\n",
    "                actual = \"nonexample\"\n",
    "            else:\n",
    "                actual = \"example\"\n",
    "            \n",
    "            \n",
    "        if(expected == \"example\" and actual == \"example\"):\n",
    "            tp += 1\n",
    "            print(\"True Positive\")\n",
    "        elif(expected == \"example\" and actual == \"nonexample\"):\n",
    "            fn += 1\n",
    "            print(\"False Negative\")\n",
    "        elif(expected == \"nonexample\" and actual ==\"exaple\"):\n",
    "            fp += 1\n",
    "            print(\"False Positive\")\n",
    "        elif(expected == \"nonexample\" and actual == \"nonexample\"):\n",
    "            tn +=1 \n",
    "            print(\"False Negateive\")\n",
    "            \n",
    "            \n",
    "    print(\"True Positives %s\" % tp)\n",
    "    print(\"True Negatives %s\" % tn)\n",
    "    print(\"False Positives %s\" % fp)\n",
    "    print(\"False Negatives %s\" % fn)\n",
    "    sensitivity = (tp/(tp+float(fn)))\n",
    "    print(\"Sensitivity: %s\" % sensitivity)\n",
    "    specificity = (tn/(tn+float(fp)))\n",
    "    print(\"Specificity: %s\" % specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
